@article{chenCEBMapVisualLocalization2021a,
  title = {{{CEB-Map}}: {{Visual Localization Error Prediction}} for {{Safe Navigation}}},
  shorttitle = {{{CEB-Map}}},
  author = {Chen, Weinan and Zhu, Lei and Wang, Chaoqun and He, Li and Meng, Max Q.-H.},
  date = {2021-05-15},
  journaltitle = {IEEE Sensors Journal},
  shortjournal = {IEEE Sensors J.},
  volume = {21},
  number = {10},
  pages = {11769--11780},
  issn = {1530-437X, 1558-1748, 2379-9153},
  doi = {10.1109/JSEN.2020.2999641},
  url = {https://ieeexplore.ieee.org/document/9109263/},
  urldate = {2022-12-08},
  abstract = {For safe visual navigation, areas with high localization errors should be concentrated and could be further refined by additional mapping operations. Given an environment map, we propose to predict the visual localization error and hence to either improve the navigation performance or call an additional mapping to refine the built map. Previous work adopts the uncertainty of landmarks for the error prediction. In our work, we take into account both the spatial distribution of visual landmarks and the uncertainty of landmarks. Our main idea is that standing at one place, a good spatial distribution of landmarks means a sufficient enough visible landmarks from all views at that place, i.e., enough landmarks under arbitrary view-direction. Combining the spatial distribution and the uncertainty of landmarks, we propose a new framework to predict the error of visual localization. Furthermore, we show that additional mapping in the area with high predicted error can significantly improve the visual localization precision. Experimental results show that there is a strong relationship between the predicted error and the real error, of which the absolute value of correlation coefficient is between 0.707 to 0.915. We apply our method to conduct an optimal refining policy on the built map and the experimental results show the improved localization precision. Applications on navigation test verify the superiority of our proposed method.},
  langid = {english},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\999EDSQI\\Chen 等 - 2021 - CEB-Map Visual Localization Error Prediction for .pdf}
}

@article{chenDynamicStrategyKeyframe2022a,
  title = {Dynamic {{Strategy}} of {{Keyframe Selection With PD Controller}} for {{VSLAM Systems}}},
  author = {Chen, Weinan and Zhu, Lei and Lin, Xubin and He, Li and Guan, Yisheng and Zhang, Hong},
  date = {2022-02},
  journaltitle = {IEEE/ASME Transactions on Mechatronics},
  shortjournal = {IEEE/ASME Trans. Mechatron.},
  volume = {27},
  number = {1},
  pages = {115--125},
  issn = {1083-4435, 1941-014X},
  doi = {10.1109/TMECH.2021.3058617},
  url = {https://ieeexplore.ieee.org/document/9352537/},
  urldate = {2022-12-08},
  abstract = {Keyframe (KF) selection in a KF-based visual simultaneous localization and mapping (VSLAM) system is critical. In previous studies, static thresholds have been used for KF selection decision making; however, suboptimal performance can result from the use of such thresholds. To obtain a better KF setting than that obtained with the existing methods, in this article, we introduce a dynamic KF selection strategy. By considering both the view change between camera observation and KFs in the built map and the rate of this change, we propose to dynamically adjust the threshold for KF selection. A proportion and derivative (PD) controller is designed with the feedback of estimated view change, where the PD controller output is used for KF selection. According to the experimental results, compared with the existing studies, our method can improve the precision of visual tracking by 17.5\% and 16.7\% based on two popular VSLAM systems.},
  langid = {english},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\MS73WLCR\\Chen 等 - 2022 - Dynamic Strategy of Keyframe Selection With PD Con.pdf}
}

@inproceedings{chenKeyframeSelectionInformation2022,
  title = {Keyframe {{Selection}} with {{Information Occupancy Grid Model}} for {{Long-term Data Association}}},
  booktitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Chen, Weinan and Ye, Hanjing and Zhu, Lei and Tang, Chao and Fu, Changfei and Chen, Yonggang and Zhang, Hong},
  date = {2022},
  abstract = {As the basic of visual tracking and mapping in VSLAM systems, keyframe selection plays an essential role. In previous works, keyframes are selected according to a series of view change-based judgement that aim to provide the referenced tracking markers and enhance the current visual tracking. However, the enrichment of environment information is always ignored, which results to the lacking of complete environment representation, and the failure of closed-loop detection or submap alignment. In this paper, we propose a keyframe selection judgement based on the deep image global descriptor and information enrichment. The image is expressed by an illumination invariant descriptor for an appropriate statical abstraction; an information enrichment judgement is proposed based on the deep descriptor, by which the environment representation can be completed. The experimental results are conducted showing the advantage of our method in terms of image representation and information enrichment. The proposed information-based keyframe selection judgement can raise the tracking precision and the loop-closed detection rate.},
  eventtitle = {2022 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})}
}

@article{chenRobustnessImprovementUsing2021a,
  title = {Robustness {{Improvement}} of {{Using Pre-Trained Network}} in {{Visual Odometry}} for {{On-Road Driving}}},
  author = {Chen, Weinan and Zhu, Lei and Loo, Shing Yan and Wang, Jiankun and Wang, Chaoqun and Meng, Max Q.-H. and Zhang, Hong},
  date = {2021-12},
  journaltitle = {IEEE Transactions on Vehicular Technology},
  shortjournal = {IEEE Trans. Veh. Technol.},
  volume = {70},
  number = {12},
  pages = {12415--12426},
  issn = {0018-9545, 1939-9359},
  doi = {10.1109/TVT.2021.3120214},
  url = {https://ieeexplore.ieee.org/document/9573476/},
  urldate = {2022-12-08},
  abstract = {Robustness in on-road driving Visual Odometry (VO) systems is critical, as it determines the reliable performance in various scenarios and environments. Especially with the development of data-driven technology, the combination of data-driven VO and model-based VO has achieved accurate tracking performance. However, the lack of generalization of pre-trained deep neural networks (DNN) limits the robustness of such a combination in unseen environments. In this study, we introduce a novel framework with appropriate usage of DNN prediction and improve the robustness in the self-driving application. Based on the characteristic of on-road self-driving motion and the DNN output, we propose a two-step optimization strategy with a variable degree of freedom (DoF), i.e., the use of two types of DoF representations during pose estimation. Specifically, our two-step optimization operates according to the residual of the optimization with the motion label classification from the pre-trained DNN, as well as our proposed Motion Evaluation by essential matrix construction. Experimental results show that our framework obtains better tracking accuracy than the existing methods.},
  langid = {english},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\4RS9CTWH\\Chen 等 - 2021 - Robustness Improvement of Using Pre-Trained Networ.pdf}
}

@inproceedings{lvSemanticallyGuidedMultiView2021,
  title = {Semantically {{Guided Multi-View Stereo}} for {{Dense 3D Road Mapping}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lv, Mingzhe and Tu, Diantao and Tang, Xincheng and Liu, Yuqian and Shen, Shuhan},
  date = {2021-05-30},
  pages = {11189--11195},
  publisher = {{IEEE}},
  location = {{Xi'an, China}},
  doi = {10.1109/ICRA48506.2021.9561077},
  url = {https://ieeexplore.ieee.org/document/9561077/},
  urldate = {2022-04-03},
  abstract = {Compared to widely used LiDAR-based mapping in autonomous driving field, image-based mapping method has the advantages of low cost, high resolution, and no need for complex calibration. However, the image-based 3D mapping depends heavily on the texture richness and always leaves holes and outliers in low-textured areas, such as the road surface. To this end, this paper proposed a novel semantically guided Multi-View Stereo method for dense 3D road mapping, which integrates semantic information into PatchMatch-based MVS pipeline and uses image semantic segmentation as soft constraints in neighbor views selection, depth-map initialization, depth propagation, and depth-map completion. Experimental results on public and our own datasets show that, with the help of semantics, the proposed method achieves superior completeness with comparable accuracy for 3D road mapping compared to state-of-the-art MVS methods.},
  eventtitle = {2021 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  isbn = {978-1-72819-077-8},
  langid = {english},
  keywords = {师兄推荐},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\FW66NVLV\\Lv 等。 - 2021 - Semantically Guided Multi-View Stereo for Dense 3D.pdf}
}
