
@article{fu_unsupervised_nodate,
	series = {{SEDR}},
	title = {Unsupervised {Spatial} {Embedded} {Deep} {Representation} of {Spatial} {Transcriptomics}},
	language = {en},
	author = {Fu, Huazhu and Xu, Hang and Chong, Kelvin and Li, Mengwei and Ang, Kok Siong and Lee, Hong Kai and Ling, Jingjing and Chen, Ao and Shao, Ling and Liu, Longqi and Chen, Jinmiao},
	pages = {36},
	file = {Unsupervised Spatial Embedded Deep Representation of Spatial Transcriptomics.pdf:/home/red0orange/Zotero/storage/CZDNSVS3/Unsupervised Spatial Embedded Deep Representation of Spatial Transcriptomics.pdf:application/pdf},
}

@article{kim_unsupervised_2020,
	series = {superpixel优化},
	title = {Unsupervised {Learning} of {Image} {Segmentation} {Based} on {Differentiable} {Feature} {Clustering}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/9151332/},
	doi = {10.1109/TIP.2020.3011269},
	abstract = {The usage of convolutional neural networks (CNNs) for unsupervised image segmentation was investigated in this study. Similar to supervised image segmentation, the proposed CNN assigns labels to pixels that denote the cluster to which the pixel belongs. In unsupervised image segmentation, however, no training images or ground truth labels of pixels are speciﬁed beforehand. Therefore, once a target image is input, the pixel labels and feature representations are jointly optimized, and their parameters are updated by the gradient descent. In the proposed approach, label prediction and network parameter learning are alternately iterated to meet the following criteria: (a) pixels of similar features should be assigned the same label, (b) spatially continuous pixels should be assigned the same label, and (c) the number of unique labels should be large. Although these criteria are incompatible, the proposed approach minimizes the combination of similarity loss and spatial continuity loss to ﬁnd a plausible solution of label assignment that balances the aforementioned criteria well. The contributions of this study are four-fold. First, we propose a novel end-to-end network of unsupervised image segmentation that consists of normalization and an argmax function for differentiable clustering. Second, we introduce a spatial continuity loss function that mitigates the limitations of ﬁxed segment boundaries possessed by previous work. Third, we present an extension of the proposed method for segmentation with scribbles as user input, which showed better accuracy than existing methods while maintaining efﬁciency. Finally, we introduce another extension of the proposed method: unseen image segmentation by using networks pre-trained with a few reference images without re-training the networks. The effectiveness of the proposed approach was examined on several benchmark datasets of image segmentation.},
	language = {en},
	urldate = {2021-08-16},
	journal = {IEEE Transactions on Image Processing},
	author = {Kim, Wonjik and Kanezaki, Asako and Tanaka, Masayuki},
	year = {2020},
	pages = {8055--8068},
	file = {2020-Unsupervised Learning of Image Segmentation Based on Differentiable Feature.pdf:/home/red0orange/Zotero/storage/CNAHNWN5/2020-Unsupervised Learning of Image Segmentation Based on Differentiable Feature.pdf:application/pdf},
}

@inproceedings{kanezaki_unsupervised_2018,
	address = {Calgary, AB},
	series = {superpixel},
	title = {Unsupervised {Image} {Segmentation} by {Backpropagation}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462533/},
	doi = {10.1109/ICASSP.2018.8462533},
	abstract = {We investigate the use of convolutional neural networks (CNNs) for unsupervised image segmentation. As in the case of supervised image segmentation, the proposed CNN assigns labels to pixels that denote the cluster to which the pixel belongs. In the unsupervised scenario, however, no training images or ground truth labels of pixels are given beforehand. Therefore, once when a target image is input, we jointly optimize the pixel labels together with feature representations while their parameters are updated by gradient descent. In the proposed approach, we alternately iterate label prediction and network parameter learning to meet the following criteria: (a) pixels of similar features are desired to be assigned the same label, (b) spatially continuous pixels are desired to be assigned the same label, and (c) the number of unique labels is desired to be large. Although these criteria are incompatible, the proposed approach ﬁnds a plausible solution of label assignment that balances well the above criteria, which demonstrates good performance on a benchmark dataset of image segmentation.},
	language = {en},
	urldate = {2021-08-16},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Kanezaki, Asako},
	month = apr,
	year = {2018},
	pages = {1543--1547},
	file = {2018-Unsupervised Image Segmentation by Backpropagation.pdf:/home/red0orange/Zotero/storage/TWAJCJ8Q/2018-Unsupervised Image Segmentation by Backpropagation.pdf:application/pdf},
}

@article{fan_road_2020,
	title = {Road {Damage} {Detection} {Based} on {Unsupervised} {Disparity} {Map} {Segmentation}},
	volume = {21},
	issn = {1524-9050, 1558-0016},
	url = {http://arxiv.org/abs/1910.04988},
	doi = {10.1109/TITS.2019.2947206},
	abstract = {This paper presents a novel road damage detection algorithm based on unsupervised disparity map segmentation. Firstly, a disparity map is transformed by minimizing an energy function with respect to stereo rig roll angle and road disparity projection model. Instead of solving this energy minimization problem using non-linear optimization techniques, we directly ﬁnd its numerical solution. The transformed disparity map is then segmented using Otus’s thresholding method, and the damaged road areas can be extracted. The proposed algorithm requires no parameters when detecting road damage. The experimental results illustrate that our proposed algorithm performs both accurately and efﬁciently. The pixel-level road damage detection accuracy is approximately 97.56\%.},
	language = {en},
	number = {11},
	urldate = {2021-08-15},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Fan, Rui and Liu, Ming},
	month = nov,
	year = {2020},
	note = {Number: 11
arXiv: 1910.04988},
	pages = {4906--4911},
	file = {2020-Road Damage Detection Based on Unsupervised Disparity Map Segmentation.pdf:/home/red0orange/Zotero/storage/3GPI39NN/2020-Road Damage Detection Based on Unsupervised Disparity Map Segmentation.pdf:application/pdf},
}

@article{fan_real-time_nodate,
	series = {博士论文},
	title = {{REAL}-{TIME} {COMPUTER} {STEREO} {VISION} {FOR} {AUTOMOTIVE} {APPLICATIONS}},
	language = {en},
	author = {Fan, Rui},
	pages = {162},
	file = {REAL-TIME COMPUTER STEREO VISION FOR AUTOMOTIVE APPLICATIONS.pdf:/home/red0orange/Zotero/storage/M3HU2K72/REAL-TIME COMPUTER STEREO VISION FOR AUTOMOTIVE APPLICATIONS.pdf:application/pdf},
}

@article{wu_simultaneous_2021,
	title = {Simultaneous {Hand}-{Eye}/{Robot}-{World}/{Camera}-{IMU} {Calibration}},
	issn = {1083-4435, 1941-014X},
	url = {https://ieeexplore.ieee.org/document/9511803/},
	doi = {10.1109/TMECH.2021.3103995},
	abstract = {The problem of calibrating extrinsic parameter between camera and inertial measurement unit (IMU) using an industrial robotic manipulator has been studied. This generates a result of hand-eye/robot-world/camera-IMU calibration in a simultaneous fashion. The developed method is free of inertial integration over time and thus is robust to uncertain IMU biases. It is derived that the problem can be solved via a simultaneous optimization of hand-eye/robot-world/camera-IMU transformations. The resulted optimization is highly non-convex on the special Euclidean group and we give globally optimal solutions. Experiments verify that the proposed method is capable of estimating accurate calibration parameters. Comparative studies between representatives show the global optimality of the proposed method. The new simultaneous method is capable of conducting calibration of a robot/camera/IMU combination. The designed method guarantees the global optimality thus the accuracy is ensured. The developed globally optimal solutions will also be computationally efﬁcient on modern industrial computers. Finally, we show that the proposed method can give accurate calibration results for a stereo/IMU sensor combination.},
	language = {en},
	urldate = {2021-08-15},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Wu, Jin and Wang, Miaomiao and Jiang, Yi and Yi, Bowen and Fan, Rui and Liu, Ming},
	year = {2021},
	pages = {1--1},
	file = {2021-Simultaneous Hand-Eye-Robot-World-Camera-IMU Calibration.pdf:/home/red0orange/Zotero/storage/RAM3T3FF/2021-Simultaneous Hand-Eye-Robot-World-Camera-IMU Calibration.pdf:application/pdf},
}

@article{wang_sne-roadseg_2021,
	series = {freespace detection},
	title = {{SNE}-{RoadSeg}+: {Rethinking} {Depth}-{Normal} {Translation} and {Deep} {Supervision} for {Freespace} {Detection}},
	shorttitle = {{SNE}-{RoadSeg}+},
	url = {http://arxiv.org/abs/2107.14599},
	abstract = {Freespace detection is a fundamental component of autonomous driving perception. Recently, deep convolutional neural networks (DCNNs) have achieved impressive performance for this task. In particular, SNE-RoadSeg, our previously proposed method based on a surface normal estimator (SNE) and a data-fusion DCNN (RoadSeg), has achieved impressive performance in freespace detection. However, SNE-RoadSeg is computationally intensive, and it is difﬁcult to execute in real time. To address this problem, we introduce SNE-RoadSeg+, an upgraded version of SNE-RoadSeg. SNE-RoadSeg+ consists of 1) SNE+, a module for more accurate surface normal estimation, and 2) RoadSeg+, a data-fusion DCNN that can greatly minimize the trade-off between accuracy and efﬁciency with the use of deep supervision. Extensive experimental results have demonstrated the effectiveness of our SNE+ for surface normal estimation and the superior performance of our SNE-RoadSeg+ over all other freespace detection approaches. Speciﬁcally, our SNE-RoadSeg+ runs in real time, and meanwhile, achieves the state-of-the-art performance on the KITTI road benchmark. Our project page is at https://www.sne-roadseg.site/ sne-roadseg-plus.},
	language = {en},
	urldate = {2021-08-15},
	journal = {arXiv:2107.14599 [cs]},
	author = {Wang, Hengli and Fan, Rui and Cai, Peide and Liu, Ming},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.14599},
	file = {2021-SNE-RoadSeg+.pdf:/home/red0orange/Zotero/storage/NR6X2F7Y/2021-SNE-RoadSeg+.pdf:application/pdf},
}

@article{wang_co-teaching_2021,
	title = {Co-{Teaching}: {An} {Ark} to {Unsupervised} {Stereo} {Matching}},
	shorttitle = {Co-{Teaching}},
	url = {http://arxiv.org/abs/2107.08186},
	abstract = {Stereo matching is a key component of autonomous driving perception. Recent unsupervised stereo matching approaches have received adequate attention due to their advantage of not requiring disparity ground truth. These approaches, however, perform poorly near occlusions. To overcome this drawback, in this paper, we propose CoTStereo, a novel unsupervised stereo matching approach. Speciﬁcally, we adopt a co-teaching framework where two networks interactively teach each other about the occlusions in an unsupervised fashion, which greatly improves the robustness of unsupervised stereo matching. Extensive experiments on the KITTI Stereo benchmarks demonstrate the superior performance of CoT-Stereo over all other state-of-the-art unsupervised stereo matching approaches in terms of both accuracy and speed. Our project webpage is https://sites.google.com/view/cot-stereo. Index Terms— stereo matching, unsupervised learning, co-teaching strategy.},
	language = {en},
	urldate = {2021-08-15},
	journal = {arXiv:2107.08186 [cs]},
	author = {Wang, Hengli and Fan, Rui and Liu, Ming},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.08186},
	file = {2021-Co-Teaching.pdf:/home/red0orange/Zotero/storage/9UUELN9Y/2021-Co-Teaching.pdf:application/pdf},
}

@article{wang_scv-stereo_2021,
	title = {{SCV}-{Stereo}: {Learning} {Stereo} {Matching} from a {Sparse} {Cost} {Volume}},
	shorttitle = {{SCV}-{Stereo}},
	url = {http://arxiv.org/abs/2107.08187},
	abstract = {Convolutional neural network (CNN)-based stereo matching approaches generally require a dense cost volume (DCV) for disparity estimation. However, generating such cost volumes is computationally-intensive and memory-consuming, hindering CNN training and inference efﬁciency. To address this problem, we propose SCV-Stereo, a novel CNN architecture, capable of learning dense stereo matching from sparse cost volume (SCV) representations. Our inspiration is derived from the fact that DCV representations are somewhat redundant and can be replaced with SCV representations. Beneﬁting from these SCV representations, our SCV-Stereo can update disparity estimations in an iterative fashion for accurate and efﬁcient stereo matching. Extensive experiments carried out on the KITTI Stereo benchmarks demonstrate that our SCV-Stereo can significantly minimize the trade-off between accuracy and efﬁciency for stereo matching. Our project page is https: //sites.google.com/view/scv-stereo.},
	language = {en},
	urldate = {2021-08-15},
	journal = {arXiv:2107.08187 [cs]},
	author = {Wang, Hengli and Fan, Rui and Liu, Ming},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.08187},
	file = {2021-SCV-Stereo.pdf:/home/red0orange/Zotero/storage/GDS4VWNS/2021-SCV-Stereo.pdf:application/pdf},
}

@inproceedings{godard_digging_2019,
	address = {Seoul, Korea (South)},
	series = {无监督：{MonoDepth2}},
	title = {Digging {Into} {Self}-{Supervised} {Monocular} {Depth} {Estimation}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009796/},
	doi = {10.1109/ICCV.2019.00393},
	abstract = {Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
	month = oct,
	year = {2019},
	pages = {3827--3837},
	file = {2019-Digging Into Self-Supervised Monocular Depth Estimation.pdf:/home/red0orange/Zotero/storage/AFKLCBMQ/2019-Digging Into Self-Supervised Monocular Depth Estimation.pdf:application/pdf},
}

@inproceedings{poggi_uncertainty_2020,
	address = {Seattle, WA, USA},
	series = {无监督：{CVPR}，探索无监督目前方法的问题},
	title = {On the {Uncertainty} of {Self}-{Supervised} {Monocular} {Depth} {Estimation}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157619/},
	doi = {10.1109/CVPR42600.2020.00329},
	abstract = {Self-supervised paradigms for monocular depth estimation are very appealing since they do not require ground truth annotations at all. Despite the astonishing results yielded by such methodologies, learning to reason about the uncertainty of the estimated depth maps is of paramount importance for practical applications, yet uncharted in the literature. Purposely, we explore for the ﬁrst time how to estimate the uncertainty for this task and how this affects depth accuracy, proposing a novel peculiar technique speciﬁcally designed for self-supervised approaches. On the standard KITTI dataset, we exhaustively assess the performance of each method with different self-supervised paradigms. Such evaluation highlights that our proposal i) always improves depth accuracy signiﬁcantly and ii) yields state-of-the-art results concerning uncertainty estimation when training on sequences and competitive results uniquely deploying stereo pairs.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Poggi, Matteo and Aleotti, Filippo and Tosi, Fabio and Mattoccia, Stefano},
	month = jun,
	year = {2020},
	pages = {3224--3234},
	file = {2020-On the Uncertainty of Self-Supervised Monocular Depth Estimation.pdf:/home/red0orange/Zotero/storage/5TJ4Q93Z/2020-On the Uncertainty of Self-Supervised Monocular Depth Estimation.pdf:application/pdf},
}

@article{khan_deep_2020,
	series = {单目深度预测综述},
	title = {Deep {Learning}-{Based} {Monocular} {Depth} {Estimation} {Methods}—{A} {State}-of-the-{Art} {Review}},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/8/2272},
	doi = {10.3390/s20082272},
	abstract = {Monocular depth estimation from Red-Green-Blue (RGB) images is a well-studied ill-posed problem in computer vision which has been investigated intensively over the past decade using Deep Learning (DL) approaches. The recent approaches for monocular depth estimation mostly rely on Convolutional Neural Networks (CNN). Estimating depth from two-dimensional images plays an important role in various applications including scene reconstruction, 3D object-detection, robotics and autonomous driving. This survey provides a comprehensive overview of this research topic including the problem representation and a short description of traditional methods for depth estimation. Relevant datasets and 13 state-of-the-art deep learning-based approaches for monocular depth estimation are reviewed, evaluated and discussed. We conclude this paper with a perspective towards future research work requiring further investigation in monocular depth estimation challenges.},
	language = {en},
	number = {8},
	urldate = {2021-08-15},
	journal = {Sensors},
	author = {Khan, Faisal and Salahuddin, Saqib and Javidnia, Hossein},
	month = apr,
	year = {2020},
	note = {Number: 8},
	pages = {2272},
	file = {2020-Deep Learning-Based Monocular Depth Estimation Methods—A State-of-the-Art Review.pdf:/home/red0orange/Zotero/storage/3TBYUDX9/2020-Deep Learning-Based Monocular Depth Estimation Methods—A State-of-the-Art Review.pdf:application/pdf},
}

@article{ming_deep_2021,
	series = {单目深度预测综述（2区）},
	title = {Deep learning for monocular depth estimation: {A} review},
	volume = {438},
	issn = {09252312},
	shorttitle = {Deep learning for monocular depth estimation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220320014},
	doi = {10.1016/j.neucom.2020.12.089},
	abstract = {Depth estimation is a classic task in computer vision, which is of great signiﬁcance for many applications such as augmented reality, target tracking and autonomous driving. Traditional monocular depth estimation methods are based on depth cues for depth prediction with strict requirements, e.g. shape-fromfocus/ defocus methods require low depth of ﬁeld on the scenes and images. Recently, a large body of deep learning methods have been proposed and has shown great promise in handling the traditional ill-posed problem. This paper aims to review the state-of-the-art development in deep learning-based monocular depth estimation. We give an overview of published papers between 2014 and 2020 in terms of training manners and task types. We ﬁrstly summarize the deep learning models for monocular depth estimation. Secondly, we categorize various deep learning-based methods in monocular depth estimation. Thirdly, we introduce the publicly available dataset and the evaluation metrics. And we also analysis the properties of these methods and compare their performance. Finally, we highlight the challenges in order to inform the future research directions.},
	language = {en},
	urldate = {2021-08-15},
	journal = {Neurocomputing},
	author = {Ming, Yue and Meng, Xuyang and Fan, Chunxiao and Yu, Hui},
	month = may,
	year = {2021},
	pages = {14--33},
	file = {2021-Deep learning for monocular depth estimation.pdf:/home/red0orange/Zotero/storage/8MUI2YS7/2021-Deep learning for monocular depth estimation.pdf:application/pdf},
}

@inproceedings{yin_geonet_2018,
	address = {Salt Lake City, UT},
	series = {无监督：深度+ego-motion+光流},
	title = {{GeoNet}: {Unsupervised} {Learning} of {Dense} {Depth}, {Optical} {Flow} and {Camera} {Pose}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{GeoNet}},
	url = {https://ieeexplore.ieee.org/document/8578310/},
	doi = {10.1109/CVPR.2018.00212},
	abstract = {We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical ﬂow and egomotion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Speciﬁcally, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yin, Zhichao and Shi, Jianping},
	month = jun,
	year = {2018},
	pages = {1983--1992},
	file = {2018-GeoNet.pdf:/home/red0orange/Zotero/storage/NBPY32KN/2018-GeoNet.pdf:application/pdf},
}

@inproceedings{zhan_unsupervised_2018,
	address = {Salt Lake City, UT},
	series = {无监督：用双目系列数据训练},
	title = {Unsupervised {Learning} of {Monocular} {Depth} {Estimation} and {Visual} {Odometry} with {Deep} {Feature} {Reconstruction}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578141/},
	doi = {10.1109/CVPR.2018.00043},
	abstract = {Despite learning based methods showing promising results in single view depth estimation and visual odometry, most existing approaches treat the tasks in a supervised manner. Recent approaches to single view depth estimation explore the possibility of learning without full supervision via minimizing photometric error. In this paper, we explore the use of stereo sequences for learning depth and visual odometry. The use of stereo sequences enables the use of both spatial (between left-right pairs) and temporal (forward backward) photometric warp error, and constrains the scene depth and camera motion to be in a common, realworld scale. At test time our framework is able to estimate single view depth and two-view odometry from a monocular sequence. We also show how we can improve on a standard photometric warp loss by considering a warp of deep features. We show through extensive experiments that: (i) jointly training for single view depth and visual odometry improves depth prediction because of the additional constraint imposed on depths and achieves competitive results for visual odometry; (ii) deep feature-based warping loss improves upon simple photometric warp loss for both single view depth estimation and visual odometry. Our method outperforms existing learning based methods on the KITTI driving dataset in both tasks. The source code is available at https://github.com/Huangying-Zhan/ Depth-VO-Feat.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhan, Huangying and Garg, Ravi and Weerasekera, Chamara Saroj and Li, Kejie and Agarwal, Harsh and Reid, Ian M.},
	month = jun,
	year = {2018},
	pages = {340--349},
	file = {2018-Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with.pdf:/home/red0orange/Zotero/storage/5AMLKUQD/2018-Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with.pdf:application/pdf},
}

@inproceedings{wang_recurrent_2019,
	address = {Long Beach, CA, USA},
	series = {{RNN用于单目深度预测}+ego-motion},
	title = {Recurrent {Neural} {Network} for ({Un}-){Supervised} {Learning} of {Monocular} {Video} {Visual} {Odometry} and {Depth}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954360/},
	doi = {10.1109/CVPR.2019.00570},
	abstract = {Deep learning-based, single-view depth estimation methods have recently shown highly promising results. However, such methods ignore one of the most important features for determining depth in the human vision system, which is motion. We propose a learning-based, multiview dense depth map and odometry estimation method that uses Recurrent Neural Networks (RNN) and trains utilizing multi-view image reprojection and forward-backward ﬂowconsistency losses. Our model can be trained in a supervised or even unsupervised mode. It is designed for depth and visual odometry estimation from video where the input frames are temporally correlated. However, it also generalizes to single-view depth estimation. Our method produces superior results to the state-of-the-art approaches for single-view and multi-view learning-based depth estimation on the KITTI driving dataset.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Rui and Pizer, Stephen M. and Frahm, Jan-Michael},
	month = jun,
	year = {2019},
	pages = {5550--5559},
	file = {2019-Recurrent Neural Network for (Un-)Supervised Learning of Monocular Video Visual.pdf:/home/red0orange/Zotero/storage/VZSGU9NI/2019-Recurrent Neural Network for (Un-)Supervised Learning of Monocular Video Visual.pdf:application/pdf},
}

@incollection{aleotti_generative_2019,
	address = {Cham},
	series = {{GEN用于单目深度预测}},
	title = {Generative {Adversarial} {Networks} for {Unsupervised} {Monocular} {Depth} {Prediction}},
	volume = {11129},
	isbn = {978-3-030-11008-6 978-3-030-11009-3},
	url = {http://link.springer.com/10.1007/978-3-030-11009-3_20},
	abstract = {Estimating depth from a single image is a very challenging and exciting topic in computer vision with implications in several application domains. Recently proposed deep learning approaches achieve outstanding results by tackling it as an image reconstruction task and exploiting geometry constraints (e.g., epipolar geometry) to obtain supervisory signals for training. Inspired by these works and compelling results achieved by Generative Adversarial Network (GAN) on image reconstruction and generation tasks, in this paper we propose to cast unsupervised monocular depth estimation within a GAN paradigm. The generator network learns to infer depth from the reference image to generate a warped target image. At training time, the discriminator network learns to distinguish between fake images generated by the generator and target frames acquired with a stereo rig. To the best of our knowledge, our proposal is the ﬁrst successful attempt to tackle monocular depth estimation with a GAN paradigm and the extensive evaluation on CityScapes and KITTI datasets conﬁrm that it enables to improve traditional approaches. Additionally, we highlight a major issue with data deployed by a standard evaluation protocol widely used in this ﬁeld and ﬁx this problem using a more reliable dataset recently made available by the KITTI evaluation benchmark.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {Computer {Vision} – {ECCV} 2018 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Aleotti, Filippo and Tosi, Fabio and Poggi, Matteo and Mattoccia, Stefano},
	editor = {Leal-Taixé, Laura and Roth, Stefan},
	year = {2019},
	doi = {10.1007/978-3-030-11009-3_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {337--354},
	file = {2019-Generative Adversarial Networks for Unsupervised Monocular Depth Prediction.pdf:/home/red0orange/Zotero/storage/XFY7XVFU/2019-Generative Adversarial Networks for Unsupervised Monocular Depth Prediction.pdf:application/pdf},
}

@inproceedings{mahjourian_unsupervised_2018,
	address = {Salt Lake City, UT},
	series = {无监督：单目深度预测+{RGB}-{D} {SLAM}},
	title = {Unsupervised {Learning} of {Depth} and {Ego}-{Motion} from {Monocular} {Video} {Using} {3D} {Geometric} {Constraints}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578692/},
	doi = {10.1109/CVPR.2018.00594},
	abstract = {We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures.},
	language = {en},
	urldate = {2021-08-15},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Mahjourian, Reza and Wicke, Martin and Angelova, Anelia},
	month = jun,
	year = {2018},
	pages = {5667--5675},
	file = {2018-Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D.pdf:/home/red0orange/Zotero/storage/GXU43KGG/2018-Unsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D.pdf:application/pdf},
}

@article{chakravarty_gen-slam_2019,
	series = {{VAE用于单目深度预测}+{SLAM} {Pose预测}},
	title = {{GEN}-{SLAM}: {Generative} {Modeling} for {Monocular} {Simultaneous} {Localization} and {Mapping}},
	shorttitle = {{GEN}-{SLAM}},
	url = {http://arxiv.org/abs/1902.02086},
	abstract = {We present a Deep Learning based system for the twin tasks of localization and obstacle avoidance essential to any mobile robot. Our system learns from conventional geometric SLAM, and outputs, using a single camera, the topological pose of the camera in an environment, and the depth map of obstacles around it. We use a CNN to localize in a topological map, and a conditional VAE to output depth for a camera image, conditional on this topological location estimation. We demonstrate the effectiveness of our monocular localization and depth estimation system on simulated and real datasets.},
	language = {en},
	urldate = {2021-08-15},
	journal = {arXiv:1902.02086 [cs]},
	author = {Chakravarty, Punarjay and Narayanan, Praveen and Roussel, Tom},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.02086},
	file = {2019-GEN-SLAM.pdf:/home/red0orange/Zotero/storage/9JX9B92G/2019-GEN-SLAM.pdf:application/pdf},
}

@article{zhao_monocular_2020,
	series = {单目深度预测综述},
	title = {Monocular {Depth} {Estimation} {Based} {On} {Deep} {Learning}: {An} {Overview}},
	volume = {63},
	issn = {1674-7321, 1869-1900},
	shorttitle = {Monocular {Depth} {Estimation} {Based} {On} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2003.06620},
	doi = {10.1007/s11431-020-1582-8},
	abstract = {Depth information is important for autonomous systems to perceive environments and estimate their own state. Traditional depth estimation methods, like structure from motion and stereo vision matching, are built on feature correspondences of multiple viewpoints. Meanwhile, the predicted depth maps are sparse. Inferring depth information from a single image (monocular depth estimation) is an ill-posed problem. With the rapid development of deep neural networks, monocular depth estimation based on deep learning has been widely studied recently and achieved promising performance in accuracy. Meanwhile, dense depth maps are estimated from single images by deep neural networks in an end-to-end manner. In order to improve the accuracy of depth estimation, different kinds of network frameworks, loss functions and training strategies are proposed subsequently. Therefore, we survey the current monocular depth estimation methods based on deep learning in this review. Initially, we conclude several widely used datasets and evaluation indicators in deep learning-based depth estimation. Furthermore, we review some representative existing methods according to different training manners: supervised, unsupervised and semisupervised. Finally, we discuss the challenges and provide some ideas for future researches in monocular depth estimation.},
	language = {en},
	number = {9},
	urldate = {2021-08-15},
	journal = {Science China Technological Sciences},
	author = {Zhao, Chaoqiang and Sun, Qiyu and Zhang, Chongzhen and Tang, Yang and Qian, Feng},
	month = sep,
	year = {2020},
	note = {Number: 9
arXiv: 2003.06620},
	keywords = {在看},
	pages = {1612--1627},
	file = {2020-Monocular Depth Estimation Based On Deep Learning.pdf:/home/red0orange/Zotero/storage/YEG7LXQU/2020-Monocular Depth Estimation Based On Deep Learning.pdf:application/pdf},
}

@inproceedings{uhrig_sparsity_2017,
	address = {Qingdao},
	series = {{KITTI}：离散深度信息的稠密恢复},
	title = {Sparsity {Invariant} {CNNs}},
	isbn = {978-1-5386-2610-8},
	url = {https://ieeexplore.ieee.org/document/8374553/},
	doi = {10.1109/3DV.2017.00012},
	abstract = {In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth completion from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the beneﬁts of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising over 94k depth annotated RGB images. Our dataset allows for training and evaluating depth completion and depth prediction techniques in challenging real-world settings and is available online at: www.cvlibs.net/datasets/kitti.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2017 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Uhrig, Jonas and Schneider, Nick and Schneider, Lukas and Franke, Uwe and Brox, Thomas and Geiger, Andreas},
	month = oct,
	year = {2017},
	pages = {11--20},
	file = {2017-Sparsity Invariant CNNs.pdf:/home/red0orange/Zotero/storage/BT3QRPXV/2017-Sparsity Invariant CNNs.pdf:application/pdf},
}

@inproceedings{serafin_fast_2016,
	address = {Daejeon, South Korea},
	series = {{3D稀疏点云的特征快速提取算法}},
	title = {Fast and robust {3D} feature extraction from sparse point clouds},
	isbn = {978-1-5090-3762-9},
	url = {http://ieeexplore.ieee.org/document/7759604/},
	doi = {10.1109/IROS.2016.7759604},
	abstract = {Matching 3D point clouds, a critical operation in map building and localization, is difﬁcult with Velodyne-type sensors due to the sparse and non-uniform point clouds that they produce. Standard methods from dense 3D point clouds are generally not effective. In this paper, we describe a featurebased approach using Principal Components Analysis (PCA) of neighborhoods of points, which results in mathematically principled line and plane features. The key contribution in this work is to show how this type of feature extraction can be done efﬁciently and robustly even on non-uniformly sampled point clouds. The resulting detector runs in real-time and can be easily tuned to have a low false positive rate, simplifying data association. We evaluate the performance of our algorithm on an autonomous car at the MCity Test Facility using a Velodyne HDL-32E, and we compare our results against the state-of-theart NARF keypoint detector.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Serafin, Jacopo and Olson, Edwin and Grisetti, Giorgio},
	month = oct,
	year = {2016},
	pages = {4105--4112},
	file = {2016-Fast and robust 3D feature extraction from sparse point clouds.pdf:/home/red0orange/Zotero/storage/UNCNAUHC/2016-Fast and robust 3D feature extraction from sparse point clouds.pdf:application/pdf},
}

@inproceedings{li_lo-net_2019,
	address = {Long Beach, CA, USA},
	series = {引用量：60，用{DL实现端对端LiDAR里程计}},
	title = {{LO}-{Net}: {Deep} {Real}-{Time} {Lidar} {Odometry}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{LO}-{Net}},
	url = {https://ieeexplore.ieee.org/document/8954330/},
	doi = {10.1109/CVPR.2019.00867},
	abstract = {We present a novel deep convolutional network pipeline, LO-Net, for real-time lidar odometry estimation. Unlike most existing lidar odometry (LO) estimations that go through individually designed feature selection, feature matching, and pose estimation pipeline, LO-Net can be trained in an end-to-end manner. With a new maskweighted geometric constraint loss, LO-Net can effectively learn feature representation for LO estimation, and can implicitly exploit the sequential dependencies and dynamics in the data. We also design a scan-to-map module, which uses the geometric and semantic information learned in LO-Net, to improve the estimation accuracy. Experiments on benchmark datasets demonstrate that LO-Net outperforms existing learning based approaches and has similar accuracy with the state-of-the-art geometry-based approach, LOAM.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Qing and Chen, Shaoyang and Wang, Cheng and Li, Xin and Wen, Chenglu and Cheng, Ming and Li, Jonathan},
	month = jun,
	year = {2019},
	pages = {8465--8474},
	file = {2019-LO-Net.pdf:/home/red0orange/Zotero/storage/KH2VPI9C/2019-LO-Net.pdf:application/pdf},
}

@article{li_undeepvo_2018,
	series = {引用量：315},
	title = {{UnDeepVO}: {Monocular} {Visual} {Odometry} through {Unsupervised} {Deep} {Learning}},
	shorttitle = {{UnDeepVO}},
	url = {http://arxiv.org/abs/1709.06841},
	abstract = {We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Speciﬁcally, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function deﬁned for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:1709.06841 [cs]},
	author = {Li, Ruihao and Wang, Sen and Long, Zhiqiang and Gu, Dongbing},
	month = feb,
	year = {2018},
	note = {arXiv: 1709.06841},
	file = {2018-UnDeepVO.pdf:/home/red0orange/Zotero/storage/S3WKS9D2/2018-UnDeepVO.pdf:application/pdf},
}

@article{nubert_self-supervised_2021,
	title = {Self-supervised {Learning} of {LiDAR} {Odometry} for {Robotic} {Applications}},
	url = {http://arxiv.org/abs/2011.05418},
	abstract = {Reliable robot pose estimation is a key building block of many robot autonomy pipelines, with LiDAR localization being an active research domain. In this work, a versatile self-supervised LiDAR odometry estimation method is presented, in order to enable the efﬁcient utilization of all available LiDAR data while maintaining real-time performance. The proposed approach selectively applies geometric losses during training, being cognizant of the amount of information that can be extracted from scan points. In addition, no labeled or ground-truth data is required, hence making the presented approach suitable for pose estimation in applications where accurate ground-truth is difﬁcult to obtain. Furthermore, the presented network architecture is applicable to a wide range of environments and sensor modalities without requiring any network or loss function adjustments. The proposed approach is thoroughly tested for both indoor and outdoor real-world applications through a variety of experiments using legged, tracked and wheeled robots, demonstrating the suitability of learningbased LiDAR odometry for complex robotic applications.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:2011.05418 [cs]},
	author = {Nubert, Julian and Khattak, Shehryar and Hutter, Marco},
	month = jun,
	year = {2021},
	note = {arXiv: 2011.05418},
	keywords = {在看},
	file = {2021-Self-supervised Learning of LiDAR Odometry for Robotic Applications.pdf:/home/red0orange/Zotero/storage/FENCLPK5/2021-Self-supervised Learning of LiDAR Odometry for Robotic Applications.pdf:application/pdf},
}

@article{mur-artal_orb-slam_2015,
	title = {{ORB}-{SLAM}: a {Versatile} and {Accurate} {Monocular} {SLAM} {System}},
	volume = {31},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{ORB}-{SLAM}},
	url = {http://arxiv.org/abs/1502.00956},
	doi = {10.1109/TRO.2015.2463671},
	abstract = {This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the ﬁttest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORBSLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the beneﬁt of the community, we make the source code public.},
	language = {en},
	number = {5},
	urldate = {2021-08-14},
	journal = {IEEE Transactions on Robotics},
	author = {Mur-Artal, Raul and Montiel, J. M. M. and Tardos, Juan D.},
	month = oct,
	year = {2015},
	note = {Number: 5
arXiv: 1502.00956},
	pages = {1147--1163},
	file = {2015-ORB-SLAM.pdf:/home/red0orange/Zotero/storage/NXRX9R2X/2015-ORB-SLAM.pdf:application/pdf},
}

@article{servieres_visual_2021,
	title = {Visual and {Visual}-{Inertial} {SLAM}: {State} of the {Art}, {Classification}, and {Experimental} {Benchmarking}},
	volume = {2021},
	issn = {1687-7268, 1687-725X},
	shorttitle = {Visual and {Visual}-{Inertial} {SLAM}},
	url = {https://www.hindawi.com/journals/js/2021/2054828/},
	doi = {10.1155/2021/2054828},
	abstract = {Simultaneous Localization and Mapping is now widely adopted by many applications, and researchers have produced very dense literature on this topic. With the advent of smart devices, embedding cameras, inertial measurement units, visual SLAM (vSLAM), and visual-inertial SLAM (viSLAM) are enabling novel general public applications. In this context, this paper conducts a review of popular SLAM approaches with a focus on vSLAM/viSLAM, both at fundamental and experimental levels. It starts with a structured overview of existing vSLAM and viSLAM designs and continues with a new classification of a dozen main state-of-the-art methods. A chronological survey of viSLAM’s development highlights the historical milestones and presents more recent methods into a classification. Finally, the performance of vSLAM is experimentally assessed for the use case of pedestrian pose estimation with a handheld device in urban environments. The performance of five open-source methods Vins-Mono, ROVIO, ORB-SLAM2, DSO, and LSD-SLAM is compared using the EuRoC MAV dataset and a new visual-inertial dataset corresponding to urban pedestrian navigation. A detailed analysis of the computation results identifies the strengths and weaknesses for each method. Globally, ORB-SLAM2 appears to be the most promising algorithm to address the challenges of urban pedestrian navigation, tested with two datasets.},
	language = {en},
	urldate = {2021-08-14},
	journal = {Journal of Sensors},
	author = {Servières, Myriam and Renaudin, Valérie and Dupuis, Alexis and Antigny, Nicolas},
	editor = {Potirakis, Stelios M.},
	month = feb,
	year = {2021},
	pages = {1--26},
	file = {2021-Visual and Visual-Inertial SLAM.pdf:/home/red0orange/Zotero/storage/DEARVU8N/2021-Visual and Visual-Inertial SLAM.pdf:application/pdf},
}

@article{fan_rethinking_2020,
	title = {Rethinking {Road} {Surface} {3D} {Reconstruction} and {Pothole} {Detection}: {From} {Perspective} {Transformation} to {Disparity} {Map} {Segmentation}},
	shorttitle = {Rethinking {Road} {Surface} {3D} {Reconstruction} and {Pothole} {Detection}},
	url = {http://arxiv.org/abs/2012.10802},
	abstract = {Potholes are one of the most common forms of road damage, which can severely affect driving comfort, road safety and vehicle condition. Pothole detection is typically performed by either structural engineers or certiﬁed inspectors. This task is, however, not only hazardous for the personnel but also extremely time-consuming. This paper presents an efﬁcient pothole detection algorithm based on road disparity map estimation and segmentation. We ﬁrst generalize the perspective transformation by incorporating the stereo rig roll angle. The road disparities are then estimated using semi-global matching. A disparity map transformation algorithm is then performed to better distinguish the damaged road areas. Finally, we utilize simple linear iterative clustering to group the transformed disparities into a collection of superpixels. The potholes are then detected by ﬁnding the superpixels, whose values are lower than an adaptively determined threshold. The proposed algorithm is implemented on an NVIDIA RTX 2080 Ti GPU in CUDA. The experiments demonstrate the accuracy and efﬁciency of our proposed road pothole detection algorithm, where an accuracy of 99.6\% and an F-score of 89.4\% are achieved.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:2012.10802 [cs]},
	author = {Fan, Rui and Ozgunalp, Umar and Wang, Yuan and Liu, Ming and Pitas, Ioannis},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.10802},
	file = {2020-Rethinking Road Surface 3D Reconstruction and Pothole Detection.pdf:/home/red0orange/Zotero/storage/ANKSHR75/2020-Rethinking Road Surface 3D Reconstruction and Pothole Detection.pdf:application/pdf},
}

@inproceedings{van_dijk_how_2019,
	address = {Seoul, Korea (South)},
	title = {How {Do} {Neural} {Networks} {See} {Depth} in {Single} {Images}?},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009532/},
	doi = {10.1109/ICCV.2019.00227},
	abstract = {Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Van Dijk, Tom and De Croon, Guido},
	month = oct,
	year = {2019},
	pages = {2183--2191},
	file = {2019-How Do Neural Networks See Depth in Single Images.pdf:/home/red0orange/Zotero/storage/3XUKX4GD/2019-How Do Neural Networks See Depth in Single Images.pdf:application/pdf},
}

@article{wang_end--end_2021,
	title = {End-to-{End} {Interactive} {Prediction} and {Planning} {With} {Optical} {Flow} {Distillation} for {Autonomous} {Driving}},
	abstract = {With the recent advancement of deep learning technology, data-driven approaches for autonomous car prediction and planning have achieved extraordinary performance. Nevertheless, most of these approaches follow a non-interactive prediction and planning paradigm, hypothesizing that a vehicle’s behaviors do not affect others. The approaches based on such a non-interactive philosophy typically perform acceptably in sparse trafﬁc scenarios but can easily fail in dense trafﬁc scenarios. Therefore, we propose an end-to-end interactive neural motion planner (INMP) for autonomous driving in this paper. Given a set of past surrounding-view images and a high deﬁnition map, our INMP ﬁrst generates a feature map in bird’s-eyeview space, which is then processed to detect other agents and perform interactive prediction and planning jointly. Also, we adopt an optical ﬂow distillation paradigm, which can effectively improve the network performance while still maintaining its real-time inference speed. Extensive experiments on the nuScenes dataset and in the closed-loop Carla simulation environment demonstrate the effectiveness and efﬁciency of our INMP for the detection, prediction, and planning tasks. Our project page is at sites.google. com/view/inmp-ofd.},
	language = {en},
	author = {Wang, Hengli and Cai, Peide and Fan, Rui and Sun, Yuxiang and Liu, Ming},
	year = {2021},
	pages = {10},
	file = {2021-End-to-End Interactive Prediction and Planning With Optical Flow Distillation.pdf:/home/red0orange/Zotero/storage/WHNCKGUY/2021-End-to-End Interactive Prediction and Planning With Optical Flow Distillation.pdf:application/pdf},
}

@inproceedings{goldman_learn_2019,
	address = {Long Beach, CA, USA},
	title = {Learn {Stereo}, {Infer} {Mono}: {Siamese} {Networks} for {Self}-{Supervised}, {Monocular}, {Depth} {Estimation}},
	isbn = {978-1-72812-506-0},
	shorttitle = {Learn {Stereo}, {Infer} {Mono}},
	url = {https://ieeexplore.ieee.org/document/9025636/},
	doi = {10.1109/CVPRW.2019.00348},
	abstract = {The ﬁeld of self-supervised monocular depth estimation has seen huge advancements in recent years. Most methods assume stereo data is available during training but usually under-utilize it and only treat it as a reference signal. We propose a novel self-supervised approach which uses both left and right images equally during training, but can still be used with a single input image at test time, for monocular depth estimation. Our Siamese network architecture consists of two, twin networks, each learns to predict a disparity map from a single image. At test time, however, only one of these networks is used in order to infer depth. We show state-of-the-art results on the standard KITTI Eigen split benchmark as well as being the highest scoring selfsupervised method on the new KITTI single view benchmark. To demonstrate the ability of our method to generalize to new data sets, we further provide results on the Make3D benchmark, which was not used during training.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Goldman, Matan and Hassner, Tal and Avidan, Shai},
	month = jun,
	year = {2019},
	pages = {2886--2895},
	file = {2019-Learn Stereo, Infer Mono.pdf:/home/red0orange/Zotero/storage/9I73TWMZ/2019-Learn Stereo, Infer Mono.pdf:application/pdf},
}

@article{wang_dynamic_2021,
	title = {Dynamic {Fusion} {Module} {Evolves} {Drivable} {Area} and {Road} {Anomaly} {Detection}: {A} {Benchmark} and {Algorithms}},
	issn = {2168-2267, 2168-2275},
	shorttitle = {Dynamic {Fusion} {Module} {Evolves} {Drivable} {Area} and {Road} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2103.02433},
	doi = {10.1109/TCYB.2021.3064089},
	abstract = {Joint detection of drivable areas and road anomalies is very important for mobile robots. Recently, many semantic segmentation approaches based on convolutional neural networks (CNNs) have been proposed for pixel-wise drivable area and road anomaly detection. In addition, some benchmark datasets, such as KITTI and Cityscapes, have been widely used. However, the existing benchmarks are mostly designed for self-driving cars. There lacks a benchmark for ground mobile robots, such as robotic wheelchairs. Therefore, in this paper, we ﬁrst build a drivable area and road anomaly detection benchmark for ground mobile robots, evaluating the existing state-of-the-art single-modal and data-fusion semantic segmentation CNNs using six modalities of visual features. Furthermore, we propose a novel module, referred to as the dynamic fusion module (DFM), which can be easily deployed in existing data-fusion networks to fuse different types of visual features effectively and efﬁciently. The experimental results show that the transformed disparity image is the most informative visual feature and the proposed DFM-RTFNet outperforms the state-of-the-arts. Additionally, our DFM-RTFNet achieves competitive performance on the KITTI road benchmark. Our benchmark is publicly available at https://sites.google.com/view/gmrb.},
	language = {en},
	urldate = {2021-08-14},
	journal = {IEEE Transactions on Cybernetics},
	author = {Wang, Hengli and Fan, Rui and Sun, Yuxiang and Liu, Ming},
	year = {2021},
	note = {arXiv: 2103.02433},
	pages = {1--11},
	file = {2021-Dynamic Fusion Module Evolves Drivable Area and Road Anomaly Detection.pdf:/home/red0orange/Zotero/storage/ADRQGFDT/2021-Dynamic Fusion Module Evolves Drivable Area and Road Anomaly Detection.pdf:application/pdf},
}

@inproceedings{godard_digging_2019-1,
	address = {Seoul, Korea (South)},
	title = {Digging {Into} {Self}-{Supervised} {Monocular} {Depth} {Estimation}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9009796/},
	doi = {10.1109/ICCV.2019.00393},
	abstract = {Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
	month = oct,
	year = {2019},
	pages = {3827--3837},
	file = {2019-Digging Into Self-Supervised Monocular Depth Estimation.pdf:/home/red0orange/Zotero/storage/AMHCB89V/2019-Digging Into Self-Supervised Monocular Depth Estimation.pdf:application/pdf},
}

@article{casser_depth_2019,
	title = {Depth {Prediction} without the {Sensors}: {Leveraging} {Structure} for {Unsupervised} {Learning} from {Monocular} {Videos}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Depth {Prediction} without the {Sensors}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4801},
	doi = {10.1609/aaai.v33i01.33018001},
	abstract = {Learning to predict scene depth from RGB inputs is a challenging task both for indoor and outdoor robot navigation. In this work we address unsupervised learning of scene depth and robot ego-motion where supervision is provided by monocular videos, as cameras are the cheapest, least restrictive and most ubiquitous sensor for robotics.},
	language = {en},
	urldate = {2021-08-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Casser, Vincent and Pirk, Soeren and Mahjourian, Reza and Angelova, Anelia},
	month = jul,
	year = {2019},
	pages = {8001--8008},
	file = {2019-Depth Prediction without the Sensors.pdf:/home/red0orange/Zotero/storage/8U7GDDM6/2019-Depth Prediction without the Sensors.pdf:application/pdf},
}

@article{li_deep_2021,
	title = {Deep {Sensor} {Fusion} {Between} {2D} {Laser} {Scanner} and {IMU} for {Mobile} {Robot} {Localization}},
	volume = {21},
	issn = {1530-437X, 1558-1748, 2379-9153},
	url = {https://ieeexplore.ieee.org/document/8689068/},
	doi = {10.1109/JSEN.2019.2910826},
	abstract = {Multi-sensor fusion plays a key role in 2D laser-based robot location and navigation. Although it has achieved great success, there are still some challenges, e.g., being fragile when having a large angular rotation. In this paper, we present a deep learning-based approach to localizing a mobile robot using a 2D laser and an inertial measurement unit. A novel recurrent convolutional neural network (RCNN)-based architecture is developed to fuse laser and inertial data for scan-to-scan pose estimation. A scanto-submap optimization is also introduced to optimize the poses estimated by the RCNN for enhanced robustness and accuracy. Extensive experiments have been conducted in both simulation and practice with a real mobile robot, verifying the effectiveness of the proposed deep sensor fusion system.},
	language = {en},
	number = {6},
	urldate = {2021-08-14},
	journal = {IEEE Sensors Journal},
	author = {Li, Chi and Wang, Sen and Zhuang, Yan and Yan, Fei},
	month = mar,
	year = {2021},
	note = {Number: 6},
	pages = {8501--8509},
	file = {2021-Deep Sensor Fusion Between 2D Laser Scanner and IMU for Mobile Robot.pdf:/home/red0orange/Zotero/storage/HJ4YBRYI/2021-Deep Sensor Fusion Between 2D Laser Scanner and IMU for Mobile Robot.pdf:application/pdf},
}

@article{li_deep_2020,
	title = {Deep {Learning} based {Monocular} {Depth} {Prediction}: {Datasets}, {Methods} and {Applications}},
	shorttitle = {Deep {Learning} based {Monocular} {Depth} {Prediction}},
	url = {http://arxiv.org/abs/2011.04123},
	abstract = {Estimating depth from RGB images can facilitate many photometric computer vision tasks, such as indoor localization, height estimation, and simultaneous localization and mapping (SLAM). Recently, monocular depth estimation has obtained great progress owing to the rapid development of deep learning techniques. They surpass traditional machine learning-based methods by a large margin in terms of accuracy and speed. Despite the rapid progress in this topic, there are lacking of a comprehensive review, which is needed to summarize the current progress and provide the future directions. In this survey, we ﬁrst introduce the datasets for depth estimation, and then give a comprehensive introduction of the methods from three perspectives: supervised learning-based methods, unsupervised learning-based methods, and sparse samples guidance-based methods. In addition, downstream applications that beneﬁt from the progress have also been illustrated. Finally, we point out the future directions and conclude the paper.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:2011.04123 [cs]},
	author = {Li, Qing and Zhu, Jiasong and Liu, Jun and Cao, Rui and Li, Qingquan and Jia, Sen and Qiu, Guoping},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.04123},
	file = {2020-Deep Learning based Monocular Depth Prediction.pdf:/home/red0orange/Zotero/storage/AH3JWTQK/2020-Deep Learning based Monocular Depth Prediction.pdf:application/pdf},
}

@article{yi-ge_conditional_2021,
	title = {Conditional {Link} {Prediction} of {Category}-{Implicit} {Keypoint} {Detection}},
	abstract = {Keypoints of objects reﬂect their concise abstractions, while the corresponding connection links (CL) build the skeleton by detecting the intrinsic relations between keypoints. Existing approaches are typically computationallyintensive, inapplicable for instances belonging to multiple classes, and/or infeasible to simultaneously encode connection information. To address the aforementioned issues, we propose an end-to-end category-implicit Keypoint and Link Prediction Network (KLPNet), which is the ﬁrst approach for simultaneous semantic keypoint detection (for multi-class instances) and CL rejuvenation. In our KLPNet, a novel Conditional Link Prediction Graph is proposed for link prediction among keypoints that are contingent on a predeﬁned category. Furthermore, a Cross-stage Keypoint Localization Module (CKLM) is introduced to explore feature aggregation for coarse-to-ﬁne keypoint localization. Comprehensive experiments conducted on three publicly available benchmarks demonstrate that our KLPNet consistently outperforms all other state-of-the-art approaches. Furthermore, the experimental results of CL prediction also show the effectiveness of our KLPNet with respect to occlusion problems.},
	language = {en},
	author = {Yi-Ge, Ellen and Fan, Rui and Liu, Zechun and Shen, Zhiqiang},
	year = {2021},
	pages = {10},
	file = {2021-Conditional Link Prediction of Category-Implicit Keypoint Detection.pdf:/home/red0orange/Zotero/storage/GQSNEM7V/2021-Conditional Link Prediction of Category-Implicit Keypoint Detection.pdf:application/pdf},
}

@article{nubert_self-supervised_2021-1,
	title = {Self-supervised {Learning} of {LiDAR} {Odometry} for {Robotic} {Applications}},
	url = {http://arxiv.org/abs/2011.05418},
	abstract = {Reliable robot pose estimation is a key building block of many robot autonomy pipelines, with LiDAR localization being an active research domain. In this work, a versatile self-supervised LiDAR odometry estimation method is presented, in order to enable the efﬁcient utilization of all available LiDAR data while maintaining real-time performance. The proposed approach selectively applies geometric losses during training, being cognizant of the amount of information that can be extracted from scan points. In addition, no labeled or ground-truth data is required, hence making the presented approach suitable for pose estimation in applications where accurate ground-truth is difﬁcult to obtain. Furthermore, the presented network architecture is applicable to a wide range of environments and sensor modalities without requiring any network or loss function adjustments. The proposed approach is thoroughly tested for both indoor and outdoor real-world applications through a variety of experiments using legged, tracked and wheeled robots, demonstrating the suitability of learningbased LiDAR odometry for complex robotic applications.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:2011.05418 [cs]},
	author = {Nubert, Julian and Khattak, Shehryar and Hutter, Marco},
	month = jun,
	year = {2021},
	note = {arXiv: 2011.05418},
	file = {2021-Self-supervised Learning of LiDAR Odometry for Robotic Applications.pdf:/home/red0orange/Zotero/storage/FK4WVHAI/2021-Self-supervised Learning of LiDAR Odometry for Robotic Applications.pdf:application/pdf},
}

@article{cho_unsupervised_2020,
	title = {Unsupervised {Geometry}-{Aware} {Deep} {LiDAR} {Odometry}},
	abstract = {Learning-based ego-motion estimation approaches have recently drawn strong interest from researchers, mostly focusing on visual perception. A few learning-based approaches using Light Detection and Ranging (LiDAR) have been reported; however, they heavily rely on a supervised learning manner. Despite the meaningful performance of these approaches, supervised training requires ground-truth pose labels, which is the bottleneck for real-world applications. Differing from these approaches, we focus on unsupervised learning for LiDAR odometry (LO) without trainable labels. Achieving trainable LO in an unsupervised manner, we introduce the uncertainty-aware loss with geometric conﬁdence, thereby allowing the reliability of the proposed pipeline. Evaluation on the KITTI, Complex Urban, and Oxford RobotCar datasets demonstrate the prominent performance of the proposed method compared to conventional model-based methods. The proposed method shows a comparable result against SuMa (in KITTI), LeGO-LOAM (in Complex Urban), and Stereo-VO (in Oxford RobotCar). The video and extra-information of the paper are described in https://sites.google.com/view/deeplo.},
	language = {en},
	author = {Cho, Younggun and Kim, Giseop and Kim, Ayoung},
	year = {2020},
	pages = {8},
	file = {2020-Unsupervised Geometry-Aware Deep LiDAR Odometry.pdf:/home/red0orange/Zotero/storage/NGHI4D5U/2020-Unsupervised Geometry-Aware Deep LiDAR Odometry.pdf:application/pdf},
}

@article{qin_vins-mono_2018,
	title = {{VINS}-{Mono}: {A} {Robust} and {Versatile} {Monocular} {Visual}-{Inertial} {State} {Estimator}},
	volume = {34},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{VINS}-{Mono}},
	url = {http://arxiv.org/abs/1708.03852},
	doi = {10.1109/TRO.2018.2853729},
	abstract = {A monocular visual-inertial system (VINS), consisting of a camera and a low-cost inertial measurement unit (IMU), forms the minimum sensor suite for metric six degreesof-freedom (DOF) state estimation. However, the lack of direct distance measurement poses signiﬁcant challenges in terms of IMU processing, estimator initialization, extrinsic calibration, and nonlinear optimization. In this work, we present VINSMono: a robust and versatile monocular visual-inertial state estimator. Our approach starts with a robust procedure for estimator initialization and failure recovery. A tightly-coupled, nonlinear optimization-based method is used to obtain high accuracy visual-inertial odometry by fusing pre-integrated IMU measurements and feature observations. A loop detection module, in combination with our tightly-coupled formulation, enables relocalization with minimum computation overhead. We additionally perform four degrees-of-freedom pose graph optimization to enforce global consistency. We validate the performance of our system on public datasets and real-world experiments and compare against other state-of-the-art algorithms. We also perform onboard closed-loop autonomous ﬂight on the MAV platform and port the algorithm to an iOS-based demonstration. We highlight that the proposed work is a reliable, complete, and versatile system that is applicable for different applications that require high accuracy localization. We open source our implementations for both PCs1 and iOS mobile devices2.},
	language = {en},
	number = {4},
	urldate = {2021-08-14},
	journal = {IEEE Transactions on Robotics},
	author = {Qin, Tong and Li, Peiliang and Shen, Shaojie},
	month = aug,
	year = {2018},
	note = {Number: 4
arXiv: 1708.03852},
	pages = {1004--1020},
	file = {2018-VINS-Mono.pdf:/home/red0orange/Zotero/storage/BUBFQZVW/2018-VINS-Mono.pdf:application/pdf},
}

@article{godard_unsupervised_2017,
	title = {Unsupervised {Monocular} {Depth} {Estimation} with {Left}-{Right} {Consistency}},
	url = {http://arxiv.org/abs/1609.03677},
	abstract = {Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:1609.03677 [cs, stat]},
	author = {Godard, Clément and Mac Aodha, Oisin and Brostow, Gabriel J.},
	month = apr,
	year = {2017},
	note = {arXiv: 1609.03677},
	keywords = {Statistics - Machine Learning},
	file = {2017-Unsupervised Monocular Depth Estimation with Left-Right Consistency.pdf:/home/red0orange/Zotero/storage/AG6T4BYP/2017-Unsupervised Monocular Depth Estimation with Left-Right Consistency.pdf:application/pdf},
}

@article{fan_three-filters--normal_2021,
	title = {Three-{Filters}-to-{Normal}: {An} {Accurate} and {Ultrafast} {Surface} {Normal} {Estimator}},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Three-{Filters}-to-{Normal}},
	url = {http://arxiv.org/abs/2005.08165},
	doi = {10.1109/LRA.2021.3067308},
	abstract = {This paper proposes three-ﬁlters-to-normal (3F2N), an accurate and ultrafast surface normal estimator (SNE), which is designed for structured range sensor data, e.g., depth/disparity images. 3F2N SNE computes surface normals by simply performing three ﬁltering operations (two image gradient ﬁlters in horizontal and vertical directions, respectively, and a mean/median ﬁlter) on an inverse depth image or a disparity image. Despite the simplicity of 3F2N SNE, no similar method already exists in the literature. To evaluate the performance of our proposed SNE, we created three large-scale synthetic datasets (easy, medium and hard) using 24 3D mesh models, each of which is used to generate 1800–2500 pairs of depth images (resolution: 480×640 pixels) and the corresponding ground-truth surface normal maps from different views. 3F2N SNE demonstrates the state-of-the-art performance, outperforming all other existing geometry-based SNEs, where the average angular errors with respect to the easy, medium and hard datasets are 1.66◦, 5.69◦ and 15.31◦, respectively. Furthermore, our C++ and CUDA implementations achieve a processing speed of over 260 Hz and 21 kHz, respectively. Our datasets and source code are publicly available at sites.google.com/view/3f2n.},
	language = {en},
	urldate = {2021-08-14},
	journal = {IEEE Robotics and Automation Letters},
	author = {Fan, Rui and Wang, Hengli and Xue, Bohuan and Huang, Huaiyang and Wang, Yuan and Liu, Ming and Pitas, Ioannis},
	year = {2021},
	note = {arXiv: 2005.08165},
	pages = {1--1},
	file = {2021-Three-Filters-to-Normal.pdf:/home/red0orange/Zotero/storage/KW9SSRA6/2021-Three-Filters-to-Normal.pdf:application/pdf},
}

@inproceedings{uhrig_sparsity_2017-1,
	address = {Qingdao},
	title = {Sparsity {Invariant} {CNNs}},
	isbn = {978-1-5386-2610-8},
	url = {https://ieeexplore.ieee.org/document/8374553/},
	doi = {10.1109/3DV.2017.00012},
	abstract = {In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth completion from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the beneﬁts of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising over 94k depth annotated RGB images. Our dataset allows for training and evaluating depth completion and depth prediction techniques in challenging real-world settings and is available online at: www.cvlibs.net/datasets/kitti.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2017 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Uhrig, Jonas and Schneider, Nick and Schneider, Lukas and Franke, Uwe and Brox, Thomas and Geiger, Andreas},
	month = oct,
	year = {2017},
	pages = {11--20},
	file = {2017-Sparsity Invariant CNNs.pdf:/home/red0orange/Zotero/storage/L3PIG5DT/2017-Sparsity Invariant CNNs.pdf:application/pdf},
}

@article{fan_sne-roadseg_2020,
	series = {freespace detection},
	title = {{SNE}-{RoadSeg}: {Incorporating} {Surface} {Normal} {Information} into {Semantic} {Segmentation} for {Accurate} {Freespace} {Detection}},
	volume = {12375},
	shorttitle = {{SNE}-{RoadSeg}},
	url = {http://arxiv.org/abs/2008.11351},
	doi = {10.1007/978-3-030-58577-8_21},
	abstract = {Freespace detection is an essential component of visual perception for self-driving cars. The recent eﬀorts made in data-fusion convolutional neural networks (CNNs) have signiﬁcantly improved semantic driving scene segmentation. Freespace can be hypothesized as a ground plane, on which the points have similar surface normals. Hence, in this paper, we ﬁrst introduce a novel module, named surface normal estimator (SNE), which can infer surface normal information from dense depth/disparity images with high accuracy and eﬃciency. Furthermore, we propose a data-fusion CNN architecture, referred to as RoadSeg, which can extract and fuse features from both RGB images and the inferred surface normal information for accurate freespace detection. For research purposes, we publish a large-scale synthetic freespace detection dataset, named Ready-to-Drive (R2D) road dataset, collected under diﬀerent illumination and weather conditions. The experimental results demonstrate that our proposed SNE module can beneﬁt all the stateof-the-art CNNs for freespace detection, and our SNE-RoadSeg achieves the best overall performance among diﬀerent datasets.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:2008.11351 [cs, eess]},
	author = {Fan, Rui and Wang, Hengli and Cai, Peide and Liu, Ming},
	year = {2020},
	note = {arXiv: 2008.11351},
	pages = {340--356},
	file = {2020-SNE-RoadSeg.pdf:/home/red0orange/Zotero/storage/LEGSW6HE/2020-SNE-RoadSeg.pdf:application/pdf},
}

@article{fan_multi-scale_2021,
	title = {{MULTI}-{SCALE} {FEATURE} {FUSION}: {LEARNING} {BETTER} {SEMANTIC} {SEGMENTATION} {FOR} {ROAD} {POTHOLE} {DETECTION}},
	abstract = {This paper presents a novel pothole detection approach based on single-modal semantic segmentation. It ﬁrst extracts visual features from input images using a convolutional neural network. A channel attention module then reweighs the channel features to enhance the consistency of different feature maps. Subsequently, we employ an atrous spatial pyramid pooling module (comprising of atrous convolutions in series, with progressive rates of dilation) to integrate the spatial context information. This helps better distinguish between potholes and undamaged road areas. Finally, the feature maps in the adjacent layers are fused using our proposed multiscale feature fusion module. This further reduces the semantic gap between different feature channel layers. Extensive experiments were carried out on the Pothole-600 dataset to demonstrate the effectiveness of our proposed method. The quantitative comparisons suggest that our method achieves the state-of-the-art (SoTA) performance on both RGB images and transformed disparity images, outperforming three SoTA single-modal semantic segmentation networks.},
	language = {en},
	author = {Fan, Jiahe and Bocus, Mohammud J and Hosking, Brett and Wu, Rigen and Liu, Yanan and Vityazev, Sergey and Fan, Rui},
	year = {2021},
	pages = {6},
	file = {2021-MULTI-SCALE FEATURE FUSION.pdf:/home/red0orange/Zotero/storage/I3K5Y5J8/2021-MULTI-SCALE FEATURE FUSION.pdf:application/pdf},
}

@article{wang_s2p2_2021,
	title = {{S2P2}: {Self}-{Supervised} {Goal}-{Directed} {Path} {Planning} {Using} {RGB}-{D} {Data} for {Robotic} {Wheelchairs}},
	shorttitle = {{S2P2}},
	url = {http://arxiv.org/abs/2103.10210},
	abstract = {Path planning is a fundamental capability for autonomous navigation of robotic wheelchairs. With the impressive development of deep-learning technologies, imitation learning-based path planning approaches have achieved effective results in recent years. However, the disadvantages of these approaches are twofold: 1) they may need extensive time and labor to record expert demonstrations as training data; and 2) existing approaches could only receive high-level commands, such as turning left/right. These commands could be less sufﬁcient for the navigation of mobile robots (e.g., robotic wheelchairs), which usually require exact poses of goals. We contribute a solution to this problem by proposing S2P2, a selfsupervised goal-directed path planning approach. Speciﬁcally, we develop a pipeline to automatically generate planned path labels given as input RGB-D images and poses of goals. Then, we present a best-ﬁt regression plane loss to train our datadriven path planning model based on the generated labels. Our S2P2 does not need pre-built maps, but it can be integrated into existing map-based navigation systems through our framework. Experimental results show that our S2P2 outperforms traditional path planning algorithms, and increases the robustness of existing map-based navigation systems. Our project page is available at https://sites.google.com/view/s2p2.},
	language = {en},
	urldate = {2021-08-14},
	journal = {arXiv:2103.10210 [cs]},
	author = {Wang, Hengli and Sun, Yuxiang and Fan, Rui and Liu, Ming},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.10210},
	file = {2021-S2P2.pdf:/home/red0orange/Zotero/storage/6VAWZNRU/2021-S2P2.pdf:application/pdf},
}

@article{wang_pvstereo_2021,
	title = {{PVStereo}: {Pyramid} {Voting} {Module} for {End}-to-{End} {Self}-{Supervised} {Stereo} {Matching}},
	volume = {6},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{PVStereo}},
	url = {http://arxiv.org/abs/2103.07094},
	doi = {10.1109/LRA.2021.3068108},
	abstract = {Supervised learning with deep convolutional neural networks (DCNNs) has seen huge adoption in stereo matching. However, the acquisition of large-scale datasets with well-labeled ground truth is cumbersome and labor-intensive, making supervised learning-based approaches often hard to implement in practice. To overcome this drawback, we propose a robust and effective self-supervised stereo matching approach, consisting of a pyramid voting module (PVM) and a novel DCNN architecture, referred to as OptStereo. Speciﬁcally, our OptStereo ﬁrst builds multi-scale cost volumes, and then adopts a recurrent unit to iteratively update disparity estimations at high resolution; while our PVM can generate reliable semi-dense disparity images, which can be employed to supervise OptStereo training. Furthermore, we publish the HKUST-Drive dataset, a large-scale synthetic stereo dataset, collected under different illumination and weather conditions for research purposes. Extensive experimental results demonstrate the effectiveness and efﬁciency of our self-supervised stereo matching approach on the KITTI Stereo benchmarks and our HKUST-Drive dataset. PVStereo, our best-performing implementation, greatly outperforms all other state-of-the-art self-supervised stereo matching approaches. Our project page is available at sites.google.com/view/pvstereo.},
	language = {en},
	number = {3},
	urldate = {2021-08-14},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wang, Hengli and Fan, Rui and Cai, Peide and Liu, Ming},
	month = jul,
	year = {2021},
	note = {Number: 3
arXiv: 2103.07094},
	pages = {4353--4360},
	file = {2021-PVStereo.pdf:/home/red0orange/Zotero/storage/4C8UCDKI/2021-PVStereo.pdf:application/pdf},
}

@article{fan_learning_2021,
	title = {Learning {Collision}-{Free} {Space} {Detection} from {Stereo} {Images}: {Homography} {Matrix} {Brings} {Better} {Data} {Augmentation}},
	issn = {1083-4435, 1941-014X},
	shorttitle = {Learning {Collision}-{Free} {Space} {Detection} from {Stereo} {Images}},
	url = {http://arxiv.org/abs/2012.07890},
	doi = {10.1109/TMECH.2021.3061077},
	abstract = {Collision-free space detection is a critical component of autonomous vehicle perception. The stateof-the-art algorithms are typically based on supervised deep learning. Their performance is dependent on the quality and amount of labeled training data. It remains an open challenge to train deep convolutional neural networks (DCNNs) using only a small quantity of training samples. Therefore, in this paper, we mainly explore an effective training data augmentation approach that can be employed to improve the overall DCNN performance, when additional images captured from different views are available. Due to the fact that the pixels in collision-free space (generally regarded as a planar surface) between two images, captured from different views, can be associated using a homography matrix, the target image can be transformed into the reference view. This provides a simple but effective way to generate training data from additional multi-view images. Extensive experimental results, conducted with six state-of-the-art semantic segmentation DCNNs on three datasets, validate the effectiveness of the proposed method for enhancing collision-free space detection performance. When validated on the KITTI road benchmark, our approach provides the best results, compared with other state-ofthe-art stereo vision-based collision-free space detection approaches.},
	language = {en},
	urldate = {2021-08-14},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Fan, Rui and Wang, Hengli and Cai, Peide and Wu, Jin and Bocus, Mohammud Junaid and Qiao, Lei and Liu, Ming},
	year = {2021},
	note = {arXiv: 2012.07890},
	pages = {1--1},
	file = {2021-Learning Collision-Free Space Detection from Stereo Images.pdf:/home/red0orange/Zotero/storage/U8R9LCLZ/2021-Learning Collision-Free Space Detection from Stereo Images.pdf:application/pdf},
}

@article{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	url = {http://arxiv.org/abs/1703.04977},
	abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model – uncertainty which can be explained away given enough data. Traditionally it has been difﬁcult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the beneﬁts of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
	language = {en},
	urldate = {2021-08-13},
	journal = {arXiv:1703.04977 [cs]},
	author = {Kendall, Alex and Gal, Yarin},
	month = oct,
	year = {2017},
	note = {arXiv: 1703.04977},
	file = {2017-What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:/home/red0orange/Zotero/storage/BMW3UF82/2017-What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:application/pdf;2017-What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:/home/red0orange/Zotero/storage/KI9KXVVY/2017-What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:application/pdf},
}

@inproceedings{kendall_geometric_2017,
	address = {Honolulu, HI},
	title = {Geometric {Loss} {Functions} for {Camera} {Pose} {Regression} with {Deep} {Learning}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100177/},
	doi = {10.1109/CVPR.2017.694},
	abstract = {Deep learning has shown to be effective for robust and real-time monocular image relocalisation. In particular, PoseNet [22] is a deep convolutional neural network which learns to regress the 6-DOF camera pose from a single image. It learns to localize using high level features and is robust to difﬁcult lighting, motion blur and unknown camera intrinsics, where point based SIFT registration fails. However, it was trained using a naive loss function, with hyper-parameters which require expensive tuning. In this paper, we give the problem a more fundamental theoretical treatment. We explore a number of novel loss functions for learning camera pose which are based on geometry and scene reprojection error. Additionally we show how to automatically learn an optimal weighting to simultaneously regress position and orientation. By leveraging geometry, we demonstrate that our technique signiﬁcantly improves PoseNet’s performance across datasets ranging from indoor rooms to a small city.},
	language = {en},
	urldate = {2021-08-13},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kendall, Alex and Cipolla, Roberto},
	month = jul,
	year = {2017},
	pages = {6555--6564},
	file = {2017-Geometric Loss Functions for Camera Pose Regression with Deep Learning.pdf:/home/red0orange/Zotero/storage/FCL9KFFV/2017-Geometric Loss Functions for Camera Pose Regression with Deep Learning.pdf:application/pdf;2017-Geometric Loss Functions for Camera Pose Regression with Deep Learning.pdf:/home/red0orange/Zotero/storage/IMRPAUV3/2017-Geometric Loss Functions for Camera Pose Regression with Deep Learning.pdf:application/pdf},
}

@article{wang_deeppco_2020,
	title = {{DeepPCO}: {End}-to-{End} {Point} {Cloud} {Odometry} through {Deep} {Parallel} {Neural} {Network}},
	shorttitle = {{DeepPCO}},
	url = {http://arxiv.org/abs/1910.11088},
	abstract = {Odometry is of key importance for localization in the absence of a map. There is considerable work in the area of visual odometry (VO), and recent advances in deep learning have brought novel approaches to VO, which directly learn salient features from raw images. These learning-based approaches have led to more accurate and robust VO systems. However, they have not been well applied to point cloud data yet. In this work, we investigate how to exploit deep learning to estimate point cloud odometry (PCO), which may serve as a critical component in point cloud-based downstream tasks or learning-based systems. Speciﬁcally, we propose a novel end-to-end deep parallel neural network called DeepPCO, which can estimate the 6-DOF poses using consecutive point clouds. It consists of two parallel sub-networks to estimate 3D translation and orientation respectively rather than a single neural network. We validate our approach on KITTI Visual Odometry/SLAM benchmark dataset with different baselines. Experiments demonstrate that the proposed approach achieves good performance in terms of pose accuracy.},
	language = {en},
	urldate = {2021-08-13},
	journal = {arXiv:1910.11088 [cs]},
	author = {Wang, Wei and Saputra, Muhamad Risqi U. and Zhao, Peijun and Gusmao, Pedro and Yang, Bo and Chen, Changhao and Markham, Andrew and Trigoni, Niki},
	month = mar,
	year = {2020},
	note = {arXiv: 1910.11088},
	file = {2020-DeepPCO.pdf:/home/red0orange/Zotero/storage/BIV9XGA6/2020-DeepPCO.pdf:application/pdf;2020-DeepPCO.pdf:/home/red0orange/Zotero/storage/ZR9AUYPT/2020-DeepPCO.pdf:application/pdf},
}

@article{cho_deeplo_2019,
	series = {{3D} {LiDAR球面映射到2D}},
	title = {{DeepLO}: {Geometry}-{Aware} {Deep} {LiDAR} {Odometry}},
	shorttitle = {{DeepLO}},
	url = {http://arxiv.org/abs/1902.10562},
	abstract = {Recently, learning-based ego-motion estimation approaches have drawn strong interest from studies mostly focusing on visual perception. These groundbreaking works focus on unsupervised learning for odometry estimation but mostly for visual sensors. Compared to images, a learning-based approach using Light Detection and Ranging (LiDAR) has been reported in a few studies where, most often, a supervised learning framework is proposed. In this paper, we propose a novel approach to geometry-aware deep LiDAR odometry trainable via both supervised and unsupervised frameworks. We incorporate the Iterated Closest Point (ICP) algorithm into a deep-learning framework and show the reliability of the proposed pipeline. We provide two loss functions that allow switching between supervised and unsupervised learning depending on the ground-truth validity in the training phase. An evaluation using the KITTI and Oxford RobotCar dataset demonstrates the prominent performance and efﬁciency of the proposed method when achieving pose accuracy. The overall algorithm is presented in https://youtu.be/Y2s08dv-Mq0.},
	language = {en},
	urldate = {2021-08-13},
	journal = {arXiv:1902.10562 [cs]},
	author = {Cho, Younggun and Kim, Giseop and Kim, Ayoung},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.10562},
	file = {2019-DeepLO.pdf:/home/red0orange/Zotero/storage/RPXP9588/2019-DeepLO.pdf:application/pdf;2019-DeepLO.pdf:/home/red0orange/Zotero/storage/9IH5T9P9/2019-DeepLO.pdf:application/pdf},
}

@article{tiwari_pseudo_2020,
	series = {无监督：单目深度预测+{RGB}-{D} {SLAM}},
	title = {Pseudo {RGB}-{D} for {Self}-{Improving} {Monocular} {SLAM} and {Depth} {Prediction}},
	url = {http://arxiv.org/abs/2004.10681},
	abstract = {Classical monocular Simultaneous Localization And Mapping (SLAM) and the recently emerging convolutional neural networks (CNNs) for monocular depth prediction represent two largely disjoint approaches towards building a 3D map of the surrounding environment. In this paper, we demonstrate that the coupling of these two by leveraging the strengths of each mitigates the others shortcomings. Speciﬁcally, we propose a joint narrow and wide baseline based self-improving framework, where on the one hand the CNN-predicted depth is leveraged to perform pseudo RGB-D feature-based SLAM, leading to better accuracy and robustness than the monocular RGB SLAM baseline. On the other hand, the bundleadjusted 3D scene structures and camera poses from the more principled geometric SLAM are injected back into the depth network through novel wide baseline losses proposed for improving the depth prediction network, which then continues to contribute towards better pose and 3D structure estimation in the next iteration. We emphasize that our framework only requires unlabeled monocular videos in both training and inference stages, and yet is able to outperform state-of-the-art self-supervised monocular and stereo depth prediction networks (e.g., Monodepth2) and feature-based monocular SLAM system (i.e., ORB-SLAM). Extensive experiments on KITTI and TUM RGB-D datasets verify the superiority of our self-improving geometry-CNN framework.},
	language = {en},
	urldate = {2021-08-13},
	journal = {arXiv:2004.10681 [cs]},
	author = {Tiwari, Lokender and Ji, Pan and Tran, Quoc-Huy and Zhuang, Bingbing and Anand, Saket and Chandraker, Manmohan},
	month = aug,
	year = {2020},
	note = {arXiv: 2004.10681},
	keywords = {在看},
	file = {2020-Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction.pdf:/home/red0orange/Zotero/storage/ISEKCZJC/2020-Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction.pdf:application/pdf;2020-Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction.pdf:/home/red0orange/Zotero/storage/CXCLKAQ8/Tiwari 等。 - 2020 - Pseudo RGB-D for Self-Improving Monocular SLAM and.pdf:application/pdf},
}

@article{javanmard-gh_deeplio_2021,
	title = {{DEEPLIO}: {DEEP} {LIDAR} {INERTIAL} {SENSOR} {FUSION} {FOR} {ODOMETRY} {ESTIMATION}},
	volume = {V-1-2021},
	issn = {2194-9050},
	shorttitle = {{DEEPLIO}},
	url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/V-1-2021/47/2021/},
	doi = {10.5194/isprs-annals-V-1-2021-47-2021},
	abstract = {Having a good estimate of the position and orientation of a mobile agent is essential for many application domains such as robotics, autonomous driving, and virtual and augmented reality. In particular, when using LiDAR and IMU sensors as the inputs, most existing methods still use classical ﬁlter-based fusion methods to achieve this task. In this work, we propose DeepLIO, a modular, end-to-end learning-based fusion framework for odometry estimation using LiDAR and IMU sensors. For this task, our network learns an appropriate fusion function by considering different modalities of its input latent feature vectors. We also formulate a loss function, where we combine both global and local pose information over an input sequence to improve the accuracy of the network predictions. Furthermore, we design three sub-networks with different modules and architectures derived from DeepLIO to analyze the effect of each sensory input on the task of odometry estimation. Experiments on the benchmark dataset demonstrate that DeepLIO outperforms existing learning-based and model-based methods regarding orientation estimation and shows a marginal position accuracy difference.},
	language = {en},
	urldate = {2021-08-13},
	journal = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Javanmard-Gh., A. and Iwaszczuk, D. and Roth, S.},
	month = jun,
	year = {2021},
	pages = {47--54},
	file = {2021-DEEPLIO.pdf:/home/red0orange/Zotero/storage/TE29XRB3/2021-DEEPLIO.pdf:application/pdf;2021-DEEPLIO.pdf:/home/red0orange/Zotero/storage/LP73RHIF/2021-DEEPLIO.pdf:application/pdf},
}

@article{teed_raft_2020,
	series = {看笔记，{RAFT}，光流模型的重要里程碑},
	title = {{RAFT}: {Recurrent} {All}-{Pairs} {Field} {Transforms} for {Optical} {Flow}},
	shorttitle = {{RAFT}},
	url = {http://arxiv.org/abs/2003.12039},
	abstract = {We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical ﬂow. RAFT extracts perpixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a ﬂow ﬁeld through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves stateof-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10\%, a 16\% error reduction from the best published result (6.10\%). On Sintel (ﬁnal pass), RAFT obtains an end-point-error of 2.855 pixels, a 30\% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high eﬃciency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.},
	language = {en},
	urldate = {2022-03-31},
	journal = {arXiv:2003.12039 [cs]},
	author = {Teed, Zachary and Deng, Jia},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.12039},
	file = {Teed 和 Deng - 2020 - RAFT Recurrent All-Pairs Field Transforms for Opt.pdf:/home/red0orange/Zotero/storage/K5289DUF/Teed 和 Deng - 2020 - RAFT Recurrent All-Pairs Field Transforms for Opt.pdf:application/pdf},
}

@article{davison_futuremapping_2018,
	title = {{FutureMapping}: {The} {Computational} {Structure} of {Spatial} {AI} {Systems}},
	shorttitle = {{FutureMapping}},
	url = {http://arxiv.org/abs/1803.11288},
	abstract = {We discuss and predict the evolution of Simultaneous Localisation and Mapping (SLAM) into a general geometric and semantic ‘Spatial AI’ perception capability for intelligent embodied devices. A big gap remains between the visual perception performance that devices such as augmented reality eyewear or comsumer robots will require and what is possible within the constraints imposed by real products. Co-design of algorithms, processors and sensors will be needed. We explore the computational structure of current and future Spatial AI algorithms and consider this within the landscape of ongoing hardware developments.},
	language = {en},
	urldate = {2022-04-01},
	journal = {arXiv:1803.11288 [cs]},
	author = {Davison, Andrew J.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.11288},
	keywords = {未阅读, 综述},
	file = {Davison - 2018 - FutureMapping The Computational Structure of Spat.pdf:/home/red0orange/Zotero/storage/97NI9DHG/Davison - 2018 - FutureMapping The Computational Structure of Spat.pdf:application/pdf},
}

@article{xia_survey_2020,
	series = {看笔记，非嵌入方式的语义{SLAM}},
	title = {A survey of image semantics-based visual simultaneous localization and mapping: {Application}-oriented solutions to autonomous navigation of mobile robots},
	volume = {17},
	issn = {1729-8814, 1729-8814},
	shorttitle = {A survey of image semantics-based visual simultaneous localization and mapping},
	url = {http://journals.sagepub.com/doi/10.1177/1729881420919185},
	doi = {10.1177/1729881420919185},
	abstract = {As one of the typical application-oriented solutions to robot autonomous navigation, visual simultaneous localization and mapping is essentially restricted to simplex environmental understanding based on geometric features of images. By contrast, the semantic simultaneous localization and mapping that is characterized by high-level environmental perception has apparently opened the door to apply image semantics to efficiently estimate poses, detect loop closures, build 3D maps, and so on. This article presents a detailed review of recent advances in semantic simultaneous localization and mapping, which mainly covers the treatments in terms of perception, robustness, and accuracy. Specifically, the concept of “semantic extractor” and the framework of “modern visual simultaneous localization and mapping” are initially presented. As the challenges associated with perception, robustness, and accuracy are being stated, we further discuss some open problems from a macroscopic view and attempt to find answers. We argue that multiscaled map representation, object simultaneous localization and mapping system, and deep neural network-based simultaneous localization and mapping pipeline design could be effective solutions to image semantics-fused visual simultaneous localization and mapping.},
	language = {en},
	number = {3},
	urldate = {2022-03-22},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Xia, Linlin and Cui, Jiashuo and Shen, Ran and Xu, Xun and Gao, Yiping and Li, Xinying},
	month = may,
	year = {2020},
	keywords = {综述},
	pages = {172988142091918},
	file = {Xia 等。 - 2020 - A survey of image semantics-based visual simultane.pdf:/home/red0orange/Zotero/storage/RPB4FXQA/Xia 等。 - 2020 - A survey of image semantics-based visual simultane.pdf:application/pdf},
}

@article{runz_maskfusion_2018,
	series = {看笔记，{MaskFusion}},
	title = {{MaskFusion}: {Real}-{Time} {Recognition}, {Tracking} and {Reconstruction} of {Multiple} {Moving} {Objects}},
	shorttitle = {{MaskFusion}},
	url = {http://arxiv.org/abs/1804.09194},
	abstract = {We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera. As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable real-time object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic.},
	language = {en},
	urldate = {2022-03-30},
	journal = {arXiv:1804.09194 [cs]},
	author = {Rünz, Martin and Buffier, Maud and Agapito, Lourdes},
	month = oct,
	year = {2018},
	note = {arXiv: 1804.09194},
	keywords = {未阅读},
	file = {Rünz 等。 - 2018 - MaskFusion Real-Time Recognition, Tracking and Re.pdf:/home/red0orange/Zotero/storage/4SXWJEPI/Rünz 等。 - 2018 - MaskFusion Real-Time Recognition, Tracking and Re.pdf:application/pdf},
}

@misc{noauthor__nodate,
	title = {中文的一些期刊论文、硕士论文},
	keywords = {未阅读},
	file = {变结构的鲁棒语义SLAM算法_张铮.pdf:/home/red0orange/Zotero/storage/ZX6AXC9C/变结构的鲁棒语义SLAM算法_张铮.pdf:application/pdf;动态场景下基于深度学习的语义视觉SLAM_阮晓钢.pdf:/home/red0orange/Zotero/storage/IIAKNNUE/动态场景下基于深度学习的语义视觉SLAM_阮晓钢.pdf:application/pdf;动态环境下的语义SLAM算法_张诚.pdf:/home/red0orange/Zotero/storage/LZ4ZJ4WW/动态环境下的语义SLAM算法_张诚.pdf:application/pdf;基于环境语义信息的同步定位与地图构建方法综述.pdf:/home/red0orange/Zotero/storage/E9VR4Z68/基于环境语义信息的同步定位与地图构建方法综述.pdf:application/pdf;基于深度学习的语义SLAM关键帧图像处理_邓晨.pdf:/home/red0orange/Zotero/storage/4QTE89IW/基于深度学习的语义SLAM关键帧图像处理_邓晨.pdf:application/pdf;基于深度学习的语义SLAM研究_李晓.caj:/home/red0orange/Zotero/storage/BLCVVMK9/基于深度学习的语义SLAM研究_李晓.caj:application/caj},
}

@article{qian_semantic_2020,
	title = {Semantic {SLAM} with {Autonomous} {Object}-{Level} {Data} {Association}},
	url = {http://arxiv.org/abs/2011.10625},
	abstract = {It is often desirable to capture and map semantic information of an environment during simultaneous localization and mapping (SLAM). Such semantic information can enable a robot to better distinguish places with similar low-level geometric and visual features and perform high-level tasks that use semantic information about objects to be manipulated and environments to be navigated. While semantic SLAM has gained increasing attention, there is little research on semanticlevel data association based on semantic objects, i.e., object-level data association. In this paper, we propose a novel object-level data association algorithm based on bag of words algorithm [1], formulated as a maximum weighted bipartite matching problem. With object-level data association solved, we develop a quadratic-programming-based semantic object initialization scheme using dual quadric and introduce additional constraints to improve the success rate of object initialization. The integrated semantic-level SLAM system can achieve high-accuracy object-level data association and real-time semantic mapping as demonstrated in the experiments. The online semantic map building and semantic-level localization capabilities facilitate semantic-level mapping and task planning in a priori unknown environment.},
	language = {en},
	urldate = {2022-03-22},
	journal = {arXiv:2011.10625 [cs]},
	author = {Qian, Zhentian and Patath, Kartik and Fu, Jie and Xiao, Jing},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.10625},
	file = {Qian 等。 - 2020 - Semantic SLAM with Autonomous Object-Level Data As.pdf:/home/red0orange/Zotero/storage/MF7898YF/Qian 等。 - 2020 - Semantic SLAM with Autonomous Object-Level Data As.pdf:application/pdf},
}

@article{outahar_direct_2021,
	series = {看{Zotero笔记}},
	title = {Direct and {Indirect} {vSLAM} {Fusion} for {Augmented} {Reality}},
	volume = {7},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/7/8/141},
	doi = {10.3390/jimaging7080141},
	abstract = {Augmented reality (AR) is an emerging technology that is applied in many ﬁelds. One of the limitations that still prevents AR to be even more widely used relates to the accessibility of devices. Indeed, the devices currently used are usually high end, expensive glasses or mobile devices. vSLAM (visual simultaneous localization and mapping) algorithms circumvent this problem by requiring relatively cheap cameras for AR. vSLAM algorithms can be classiﬁed as direct or indirect methods based on the type of data used. Each class of algorithms works optimally on a type of scene (e.g., textured or untextured) but unfortunately with little overlap. In this work, a method is proposed to fuse a direct and an indirect methods in order to have a higher robustness and to offer the possibility for AR to move seamlessly between different types of scenes. Our method is tested on three datasets against state-of-the-art direct (LSD-SLAM), semi-direct (LCSD) and indirect (ORBSLAM2) algorithms in two different scenarios: a trajectory planning and an AR scenario where a virtual object is displayed on top of the video feed; furthermore, a similar method (LCSD SLAM) is also compared to our proposal. Results show that our fusion algorithm is generally as efﬁcient as the best algorithm both in terms of trajectory (mean errors with respect to ground truth trajectory measurements) as well as in terms of quality of the augmentation (robustness and stability). In short, we can propose a fusion algorithm that, in our tests, takes the best of both the direct and indirect methods.},
	language = {en},
	number = {8},
	urldate = {2022-03-22},
	journal = {Journal of Imaging},
	author = {Outahar, Mohamed and Moreau, Guillaume and Normand, Jean-Marie},
	month = aug,
	year = {2021},
	keywords = {AR},
	pages = {141},
	file = {Outahar 等。 - 2021 - Direct and Indirect vSLAM Fusion for Augmented Rea.pdf:/home/red0orange/Zotero/storage/2GYJX96I/Outahar 等。 - 2021 - Direct and Indirect vSLAM Fusion for Augmented Rea.pdf:application/pdf},
}

@inproceedings{bolya_tide_2020,
	address = {Cham},
	series = {{TIDE}：目标检测错误分析的工具},
	title = {{TIDE}: {A} {General} {Toolbox} for {Identifying} {Object} {Detection} {Errors}},
	isbn = {978-3-030-58580-8},
	shorttitle = {{TIDE}},
	doi = {10.1007/978-3-030-58580-8_33},
	abstract = {We introduce TIDE, a framework and associated toolbox (https://dbolya.github.io/tide/) for analyzing the sources of error in object detection and instance segmentation algorithms. Importantly, our framework is applicable across datasets and can be applied directly to output prediction files without required knowledge of the underlying prediction system. Thus, our framework can be used as a drop-in replacement for the standard mAP computation while providing a comprehensive analysis of each model’s strengths and weaknesses. We segment errors into six types and, crucially, are the first to introduce a technique for measuring the contribution of each error in a way that isolates its effect on overall performance. We show that such a representation is critical for drawing accurate, comprehensive conclusions through in-depth analysis across 4 datasets and 7 recognition models.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Bolya, Daniel and Foley, Sean and Hays, James and Hoffman, Judy},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {558--573},
	file = {已提交版本:/home/red0orange/Zotero/storage/JHQ2RIIW/Bolya 等。 - 2020 - TIDE A General Toolbox for Identifying Object Det.pdf:application/pdf},
}

@article{guo_classification_2022,
	series = {师兄不平衡的论文},
	title = {A classification method to classify bone marrow cells with class imbalance problem},
	volume = {72},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809421008934},
	doi = {https://doi.org/10.1016/j.bspc.2021.103296},
	abstract = {Bone marrow cell morphology has long been used to diagnose blood diseases. However, it requires long-term experience from a suitable person. Furthermore, the outcomes of their recognition are subjective and no quantitative standard has been established yet. Consequently, developing a deep learning automatic system for classifying bone marrow cells is extremely important. However, real-life data sets, such as bone marrow cell data, constantly suffer from a long-tail distribution problem, owing to which the final trained classifier is biased toward a large number of categories. Thus, addressing this issue is crucial. The current research presents a class balance classification method (CBCM) for classifying 15 types of bone marrow cell data sets with a class imbalance problem. CBCM outperforms other balance approaches such as random over-sampling, synthetic minority over-sampling technique (SMOTE), random under-sampling, weighted random forest and weighted cross-entropy function, achieving precision, sensitivity, and specificity values of 84.53\%, 84.44\% and 99.29\% respectively. A more extensive comparison between the baseline and CBCM, as well as the Grad-CAM and Guided Grad-CAM of CBCM, reveals that CBCM is a reliable and effective solution to address the long-tail distribution problem of the bone marrow cell data sets.},
	journal = {Biomedical Signal Processing and Control},
	author = {Guo, Liang and Huang, Peiduo and Huang, Dehao and Li, Zilan and She, Chenglong and Guo, Qianhang and Zhang, Qingmao and Li, Jiaming and Ma, Qiongxiong and Li, Jie},
	year = {2022},
	pages = {103296},
}

@article{zhang_deep_2021,
	series = {长尾型数据综述},
	title = {Deep {Long}-{Tailed} {Learning}: {A} {Survey}},
	shorttitle = {Deep {Long}-{Tailed} {Learning}},
	url = {http://arxiv.org/abs/2110.04596},
	abstract = {Deep long-tailed learning, one of the most challenging problems in visual recognition, aims to train well-performing deep models from a large number of images that follow a long-tailed class distribution. In the last decade, deep learning has emerged as a powerful recognition model for learning high-quality image representations and has led to remarkable breakthroughs in generic visual recognition. However, long-tailed class imbalance, a common problem in practical visual recognition tasks, often limits the practicality of deep network based recognition models in real-world applications, since they can be easily biased towards dominant classes and perform poorly on tail classes. To address this problem, a large number of studies have been conducted in recent years, making promising progress in the ﬁeld of deep long-tailed learning. Considering the rapid evolution of this ﬁeld, this paper aims to provide a comprehensive survey on recent advances in deep long-tailed learning. To be speciﬁc, we group existing deep long-tailed learning studies into three main categories (i.e., class re-balancing, information augmentation and module improvement), and review these methods following this taxonomy in detail. Afterward, we empirically analyze several state-of-the-art methods by evaluating to what extent they address the issue of class imbalance via a newly proposed evaluation metric, i.e., relative accuracy. We conclude the survey by highlighting important applications of deep long-tailed learning and identifying several promising directions for future research.},
	language = {en},
	urldate = {2022-03-19},
	journal = {arXiv:2110.04596 [cs]},
	author = {Zhang, Yifan and Kang, Bingyi and Hooi, Bryan and Yan, Shuicheng and Feng, Jiashi},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.04596},
	file = {Zhang 等。 - 2021 - Deep Long-Tailed Learning A Survey.pdf:/home/red0orange/Zotero/storage/58Z937EU/Zhang 等。 - 2021 - Deep Long-Tailed Learning A Survey.pdf:application/pdf},
}

@article{qin_roadmap_2021,
	series = {和{AVP}-{SLAM同类型}},
	title = {{RoadMap}: {A} {Light}-{Weight} {Semantic} {Map} for {Visual} {Localization} towards {Autonomous} {Driving}},
	shorttitle = {{RoadMap}},
	url = {http://arxiv.org/abs/2106.02527},
	abstract = {Accurate localization is of crucial importance for autonomous driving tasks. Nowadays, we have seen a lot of sensor-rich vehicles (e.g. Robo-taxi) driving on the street autonomously, which rely on high-accurate sensors (e.g. Lidar and RTK GPS) and high-resolution map. However, low-cost production cars cannot afford such high expenses on sensors and maps. How to reduce costs? How do sensor-rich vehicles beneﬁt low-cost cars? In this paper, we proposed a light-weight localization solution, which relies on low-cost cameras and compact visual semantic maps. The map is easily produced and updated by sensor-rich vehicles in a crowd-sourced way. Speciﬁcally, the map consists of several semantic elements, such as lane line, crosswalk, ground sign, and stop line on the road surface. We introduce the whole framework of on-vehicle mapping, on-cloud maintenance, and user-end localization. The map data is collected and preprocessed on vehicles. Then, the crowd-sourced data is uploaded to a cloud server. The mass data from multiple vehicles are merged on the cloud so that the semantic map is updated in time. Finally, the semantic map is compressed and distributed to production cars, which use this map for localization. We validate the performance of the proposed map in real-world experiments and compare it against other algorithms. The average size of the semantic map is 36 kb/km. We highlight that this framework is a reliable and practical localization solution for autonomous driving.},
	language = {en},
	urldate = {2022-03-22},
	journal = {arXiv:2106.02527 [cs]},
	author = {Qin, Tong and Zheng, Yuxin and Chen, Tongqing and Chen, Yilun and Su, Qing},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.02527},
	file = {Qin 等。 - 2021 - RoadMap A Light-Weight Semantic Map for Visual Lo.pdf:/home/red0orange/Zotero/storage/X9X7H78F/Qin 等。 - 2021 - RoadMap A Light-Weight Semantic Map for Visual Lo.pdf:application/pdf},
}

@article{makhataeva_augmented_2020,
	series = {{AR在机器人上的应用}},
	title = {Augmented {Reality} for {Robotics}: {A} {Review}},
	volume = {9},
	issn = {2218-6581},
	shorttitle = {Augmented {Reality} for {Robotics}},
	url = {https://www.mdpi.com/2218-6581/9/2/21},
	doi = {10.3390/robotics9020021},
	abstract = {Augmented reality (AR) is used to enhance the perception of the real world by integrating virtual objects to an image sequence acquired from various camera technologies. Numerous AR applications in robotics have been developed in recent years. The aim of this paper is to provide an overview of AR research in robotics during the ﬁve year period from 2015 to 2019. We classiﬁed these works in terms of application areas into four categories: (1) Medical robotics: Robot-Assisted surgery (RAS), prosthetics, rehabilitation, and training systems; (2) Motion planning and control: trajectory generation, robot programming, simulation, and manipulation; (3) Human-robot interaction (HRI): teleoperation, collaborative interfaces, wearable robots, haptic interfaces, brain-computer interfaces (BCIs), and gaming; (4) Multi-agent systems: use of visual feedback to remotely control drones, robot swarms, and robots with shared workspace. Recent developments in AR technology are discussed followed by the challenges met in AR due to issues of camera localization, environment mapping, and registration. We explore AR applications in terms of how AR was integrated and which improvements it introduced to corresponding ﬁelds of robotics. In addition, we summarize the major limitations of the presented applications in each category. Finally, we conclude our review with future directions of AR research in robotics. The survey covers over 100 research works published over the last ﬁve years.},
	language = {en},
	number = {2},
	urldate = {2022-03-21},
	journal = {Robotics},
	author = {Makhataeva, Zhanat and Varol, Huseyin},
	month = apr,
	year = {2020},
	pages = {21},
	file = {Makhataeva 和 Varol - 2020 - Augmented Reality for Robotics A Review.pdf:/home/red0orange/Zotero/storage/GS7E2F2B/Makhataeva 和 Varol - 2020 - Augmented Reality for Robotics A Review.pdf:application/pdf},
}

@inproceedings{yang_reppoints_2019,
	series = {{RepPoints}},
	title = {{RepPoints}: {Point} {Set} {Representation} for {Object} {Detection}},
	booktitle = {The {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Yang, Ze and Liu, Shaohui and Hu, Han and Wang, Liwei and Lin, Stephen},
	year = {2019},
}

@inproceedings{duan_corner_2020,
	series = {{CPNDet}},
	title = {Corner proposal network for anchor-free, two-stage object detection},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Duan, Kaiwen and Xie, Lingxi and Qi, Honggang and Bai, Song and Huang, Qingming and Tian, Qi},
	year = {2020},
	pages = {399--416},
}

@article{huang_densebox_2015,
	series = {{DenseBox}},
	title = {Densebox: {Unifying} landmark localization with end to end object detection},
	journal = {arXiv preprint arXiv:1509.04874},
	author = {Huang, Lichao and Yang, Yi and Deng, Yafeng and Yu, Yinan},
	year = {2015},
}

@article{he_spatial_2015,
	series = {{SPP}},
	title = {Spatial pyramid pooling in deep convolutional networks for visual recognition},
	volume = {37},
	issn = {0162-8828},
	number = {9},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	pages = {1904--1916},
}

@inproceedings{lin_focal_2017,
	series = {{RetinaNet}},
	title = {Focal loss for dense object detection},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	year = {2017},
}

@article{liu_ssd_2016,
	series = {{SSD}},
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	journal = {ECCV},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
}

@article{ren_faster_2017,
	series = {Faster {RCNN}},
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2017},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
}

@inproceedings{law_cornernet_2018,
	series = {Cornernet},
	title = {Cornernet: {Detecting} objects as paired keypoints},
	booktitle = {15th {European} {Conference} on {Computer} {Vision}, {ECCV} 2018},
	publisher = {Springer Verlag},
	author = {Law, Hei and Deng, Jia},
	year = {2018},
	pages = {765--781},
}

@article{zhou_objects_2019,
	series = {{CenterNet}},
	title = {Objects as {Points}},
	journal = {arXiv preprint arXiv:1904.07850},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	year = {2019},
}

@article{cai_cascade_2019,
	series = {Cascade {R}-{CNN}},
	title = {Cascade {R}-{CNN}: {High} {Quality} {Object} {Detection} and {Instance} {Segmentation}},
	issn = {1939-3539},
	url = {http://dx.doi.org/10.1109/tpami.2019.2956516},
	doi = {10.1109/tpami.2019.2956516},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {1--1},
}

@inproceedings{szegedy_inception-v4_2017,
	series = {Inceptionv4},
	title = {Inception-v4, inception-resnet and the impact of residual connections on learning},
	booktitle = {Thirty-first {AAAI} conference on artificial intelligence},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
	year = {2017},
}

@article{simonyan_very_2014,
	series = {vgg},
	title = {Very deep convolutional networks for large-scale image recognition},
	journal = {arXiv preprint arXiv:1409.1556},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
}

@inproceedings{tan_efficientnet_2019,
	series = {Efficientnet},
	title = {Efficientnet: {Rethinking} model scaling for convolutional neural networks},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	year = {2019},
	pages = {6105--6114},
}

@inproceedings{dosovitskiy_image_2021,
	series = {{ViT}},
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	url = {https://openreview.net/forum?id=YicbFdNTTy},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2021},
}

@article{liu_swin_2021,
	series = {Swin {Transformer}},
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting selfattention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the ﬂexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classiﬁcation (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The code and models will be made publicly available at https:// github.com/microsoft/Swin-Transformer.},
	language = {en},
	urldate = {2021-12-15},
	journal = {arXiv:2103.14030 [cs]},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {00000 
arXiv: 2103.14030},
	file = {Liu 等。 - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:/home/red0orange/Zotero/storage/XK65EMM2/Liu 等。 - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf},
}

@inproceedings{xie_aggregated_2017,
	series = {{ResNext}},
	title = {Aggregated residual transformations for deep neural networks},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	year = {2017},
	pages = {1492--1500},
}

@inproceedings{he_deep_2016,
	series = {{ResNet}},
	title = {Deep residual learning for image recognition},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@article{krizhevsky_imagenet_2012,
	series = {{AlexNet}},
	title = {Imagenet classification with deep convolutional neural networks},
	volume = {25},
	journal = {Advances in neural information processing systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@article{teed_droid-slam_2022,
	series = {Droid-{SLAM} 深度学习端对端{SLAM}},
	title = {{DROID}-{SLAM}: {Deep} {Visual} {SLAM} for {Monocular}, {Stereo}, and {RGB}-{D} {Cameras}},
	shorttitle = {{DROID}-{SLAM}},
	url = {http://arxiv.org/abs/2108.10869},
	abstract = {We introduce DROID-SLAM, a new deep learning based SLAM system. DROIDSLAM consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. DROID-SLAM is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or RGB-D video to achieve improved performance at test time. The URL to our open source code is https://github.com/princeton-vl/DROID-SLAM.},
	language = {en},
	urldate = {2022-03-11},
	journal = {arXiv:2108.10869 [cs]},
	author = {Teed, Zachary and Deng, Jia},
	month = feb,
	year = {2022},
	note = {arXiv: 2108.10869},
	file = {Teed 和 Deng - 2022 - DROID-SLAM Deep Visual SLAM for Monocular, Stereo.pdf:/home/red0orange/Zotero/storage/QNRDCVEC/Teed 和 Deng - 2022 - DROID-SLAM Deep Visual SLAM for Monocular, Stereo.pdf:application/pdf},
}

@misc{jocher_ultralyticsyolov5_2021,
	title = {ultralytics/yolov5: v6.0 - {YOLOv5n} '{Nano}' models, {Roboflow} integration, {TensorFlow} export, {OpenCV} {DNN} support},
	url = {https://doi.org/10.5281/zenodo.5563715},
	publisher = {Zenodo},
	author = {Jocher, Glenn and Stoken, Alex and Chaurasia, Ayush and Borovec, Jirka and {NanoCode012} and {TaoXie} and Kwon, Yonghye and Michael, Kalen and Changyu, Liu and Fang, Jiacong and V, Abhiram and {Laughing} and {tkianai} and {yxNONG} and Skalski, Piotr and Hogan, Adam and Nadar, Jebastin and {imyhxy} and Mammana, Lorenzo and {AlexWang1900} and Fati, Cristi and Montes, Diego and Hajek, Jan and Diaconu, Laurentiu and Minh, Mai Thanh and {Marc} and {albinxavi} and {fatih} and {oleg} and {wanghaoyang0106}},
	year = {2021},
	doi = {10.5281/zenodo.5563715},
}

@article{chen_mmdetection_2019,
	series = {{MMdetection}},
	title = {{MMDetection}: {Open} {MMLab} {Detection} {Toolbox} and {Benchmark}},
	journal = {arXiv preprint arXiv:1906.07155},
	author = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
	year = {2019},
}

@inproceedings{zhou_convnets_2021,
	address = {Montreal, BC, Canada},
	series = {Transformer在细粒度任务上有更好的表现},
	title = {{ConvNets} vs. {Transformers}: {Whose} {Visual} {Representations} are {More} {Transferable}?},
	isbn = {978-1-66540-191-3},
	shorttitle = {{ConvNets} vs. {Transformers}},
	url = {https://ieeexplore.ieee.org/document/9607461/},
	doi = {10.1109/ICCVW54120.2021.00252},
	abstract = {Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets’ features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), opendomain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	publisher = {IEEE},
	author = {Zhou, Hong-Yu and Lu, Chixiang and Yang, Sibei and Yu, Yizhou},
	month = oct,
	year = {2021},
	pages = {2230--2238},
	file = {Zhou 等。 - 2021 - ConvNets vs. Transformers Whose Visual Representa.pdf:/home/red0orange/Zotero/storage/KJ9KJBX9/Zhou 等。 - 2021 - ConvNets vs. Transformers Whose Visual Representa.pdf:application/pdf},
}

@misc{noauthor_home_nodate,
	title = {Home},
	url = {https://raabindata.com/},
	abstract = {Raabin Health Database Free Access To Health Data Free Download Data Request Latest Events New Algorithm for Classification of White Blood Cells Download the “Generate Your Business Idea” ebook for free Raabin Health Database in Nature/SRP Few Steps To Free Download Raabin Website You are here! Step One In the first step, register to the … Home Read More »},
	language = {en-US},
	urldate = {2022-03-06},
	journal = {Raabin Health Database},
}

@inproceedings{labati_all-idb_2011,
	series = {{ALL}-{IDB数据集}},
	title = {All-{IDB}: {The} acute lymphoblastic leukemia image database for image processing},
	booktitle = {2011 18th {IEEE} international conference on image processing},
	publisher = {IEEE},
	author = {Labati, Ruggero Donida and Piuri, Vincenzo and Scotti, Fabio},
	year = {2011},
	keywords = {数据集},
	pages = {2045--2048},
}

@article{almezhghwi_improved_2020,
	title = {Improved {Classification} of {White} {Blood} {Cells} with the {Generative} {Adversarial} {Network} and {Deep} {Convolutional} {Neural} {Network}},
	volume = {2020},
	issn = {1687-5265, 1687-5273},
	url = {https://www.hindawi.com/journals/cin/2020/6490479/},
	doi = {10.1155/2020/6490479},
	abstract = {White blood cells (leukocytes) are a very important component of the blood that forms the immune system, which is responsible for fighting foreign elements. The five types of white blood cells include
              neutrophils
              ,
              eosinophils
              ,
              lymphocytes
              ,
              monocytes
              , and
              basophils
              , where each type constitutes a different proportion and performs specific functions. Being able to classify and, therefore, count these different constituents is critical for assessing the health of patients and infection risks. Generally, laboratory experiments are used for determining the type of a white blood cell. The staining process and manual evaluation of acquired images under the microscope are tedious and subject to human errors. Moreover, a major challenge is the unavailability of training data that cover the morphological variations of white blood cells so that trained classifiers can generalize well. As such, this paper investigates image transformation operations and generative adversarial networks (GAN) for data augmentation and state-of-the-art deep neural networks (i.e., VGG-16, ResNet, and DenseNet) for the classification of white blood cells into the five types. Furthermore, we explore initializing the DNNs’ weights randomly or using weights pretrained on the CIFAR-100 dataset. In contrast to other works that require advanced image preprocessing and manual feature extraction before classification, our method works directly with the acquired images. The results of extensive experiments show that the proposed method can successfully classify white blood cells. The best DNN model, DenseNet-169, yields a validation accuracy of 98.8\%. Particularly, we find that the proposed approach outperforms other methods that rely on sophisticated image processing and manual feature engineering.},
	language = {en},
	urldate = {2022-03-06},
	journal = {Computational Intelligence and Neuroscience},
	author = {Almezhghwi, Khaled and Serte, Sertan},
	month = jul,
	year = {2020},
	pages = {1--12},
	file = {Almezhghwi 和 Serte - 2020 - Improved Classification of White Blood Cells with .pdf:/home/red0orange/Zotero/storage/YAHCXJUQ/Almezhghwi 和 Serte - 2020 - Improved Classification of White Blood Cells with .pdf:application/pdf},
}

@misc{__2009,
	title = {血液病诊断与鉴别诊断图谱},
	publisher = {人民卫生出版社},
	author = {{吴晓芝} and {others}},
	year = {2009},
}

@article{matek_human-level_2019,
	series = {Nature {Resnext50}，开源数据集},
	title = {Human-level recognition of blast cells in acute myeloid leukemia with convolutional neural networks},
	url = {http://biorxiv.org/lookup/doi/10.1101/564039},
	doi = {10.1101/564039},
	abstract = {Reliable recognition of malignant white blood cells is a key step in the diagnosis of hematologic malignancies such as Acute Myeloid Leukemia. Microscopic morphological examination of blood cells is usually performed by trained human examiners, making the process tedious, time-consuming and hard to standardise.},
	language = {en},
	urldate = {2021-12-29},
	author = {Matek, Christian and Schwarz, Simone and Spiekermann, Karsten and Marr, Carsten},
	month = feb,
	year = {2019},
	keywords = {数据集, 表格参考, 分类},
	file = {Matek 等。 - 2019 - Human-level recognition of blast cells in acute my.pdf:/home/red0orange/Zotero/storage/FG2G87A4/Matek 等。 - 2019 - Human-level recognition of blast cells in acute my.pdf:application/pdf},
}

@article{matek_highly_2021,
	series = {resnext 骨髓细胞21分类},
	title = {Highly accurate differentiation of bone marrow cell morphologies using deep neural networks on a large image data set},
	volume = {138},
	issn = {0006-4971, 1528-0020},
	url = {https://ashpublications.org/blood/article/138/20/1917/477932/Highly-accurate-differentiation-of-bone-marrow},
	doi = {10.1182/blood.2020010568},
	abstract = {Abstract
            Biomedical applications of deep learning algorithms rely on large expert annotated data sets. The classification of bone marrow (BM) cell cytomorphology, an important cornerstone of hematological diagnosis, is still done manually thousands of times every day because of a lack of data sets and trained models. We applied convolutional neural networks (CNNs) to a large data set of 171 374 microscopic cytological images taken from BM smears from 945 patients diagnosed with a variety of hematological diseases. The data set is the largest expert-annotated pool of BM cytology images available in the literature. It allows us to train high-quality classifiers of leukocyte cytomorphology that identify a wide range of diagnostically relevant cell species with high precision and recall. Our CNNs outcompete previous feature-based approaches and provide a proof-of-concept for the classification problem of single BM cells. This study is a step toward automated evaluation of BM cell morphology using state-of-the-art image-classification algorithms. The underlying data set represents an educational resource, as well as a reference for future artificial intelligence–based approaches to BM cytomorphology.},
	language = {en},
	number = {20},
	urldate = {2021-12-10},
	journal = {Blood},
	author = {Matek, Christian and Krappe, Sebastian and Münzenmayer, Christian and Haferlach, Torsten and Marr, Carsten},
	month = nov,
	year = {2021},
	note = {00000},
	keywords = {分类},
	pages = {1917--1927},
	file = {Matek 等。 - 2021 - Highly accurate differentiation of bone marrow cel.pdf:/home/red0orange/Zotero/storage/3BYY88RM/Matek 等。 - 2021 - Highly accurate differentiation of bone marrow cel.pdf:application/pdf},
}

@incollection{ishikawa_attention-based_2021,
	address = {Cham},
	series = {骨髓细胞分类，注意力网络，视觉会议},
	title = {Attention-{Based} {Fine}-{Grained} {Classification} of {Bone} {Marrow} {Cells}},
	volume = {12626},
	isbn = {978-3-030-69540-8 978-3-030-69541-5},
	url = {http://link.springer.com/10.1007/978-3-030-69541-5_39},
	abstract = {Computer aided ﬁne-grained classiﬁcation of bone marrow cells is a signiﬁcant task because manual morphological examination is time-consuming and highly dependent on the expert knowledge. Limited methods are proposed for the ﬁne-grained classiﬁcation of bone marrow cells. This can be partially attributed to challenges of insuﬃcient data, high intra-class and low inter-class variances.},
	language = {en},
	urldate = {2021-11-21},
	booktitle = {Computer {Vision} – {ACCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Weining and Guo, Peirong and Li, Lemin and Tan, Yan and Shi, Hongxia and Wei, Yan and Xu, Xiangmin},
	editor = {Ishikawa, Hiroshi and Liu, Cheng-Lin and Pajdla, Tomas and Shi, Jianbo},
	year = {2021},
	doi = {10.1007/978-3-030-69541-5_39},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {分类, 顶会},
	pages = {652--668},
	file = {Wang 等。 - 2021 - Attention-Based Fine-Grained Classification of Bon.pdf:/home/red0orange/Zotero/storage/WZITE62M/Wang 等。 - 2021 - Attention-Based Fine-Grained Classification of Bon.pdf:application/pdf},
}

@article{jiang_white_2018,
	series = {{AlexNet改进} 40类},
	title = {White {Blood} {Cells} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {32},
	issn = {0218-0014, 1793-6381},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218001418570069},
	doi = {10.1142/S0218001418570069},
	abstract = {The necessary step in the diagnosis of leukemia by the attending physician is to classify the white blood cells in the bone marrow, which requires the attending physician to have a wealth of clinical experience. Now the deep learning is very suitable for the study of image recognition classification, and the effect is not good enough todirectly use some famous convolution neural network models, such as AlexNet model, GoogleNet model, and VGGFace model. In this paper, we construct a new convolution neural network model called WBCNet model that can fully extract features of the microscopic white blood cell image by combining batch normalization algorithm, residual convolution architecture and improved activation function. WBCNet model has 33 layers of network architecture, whose speed has greatly been improved compared with the traditional CNN model in training period, and it can quickly identify the category of white blood cell images. The accuracy rate is 77.65\% for Top-1 and 98.65\% for Top-5 on the training set, while 83\% for Top-1 on the test set. This study can help doctors to diagnosis on leukemia, and reduce misdiagnosis rate.},
	language = {en},
	number = {09},
	urldate = {2022-03-04},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	author = {Jiang, Ming and Cheng, Liu and Qin, Feiwei and Du, Lian and Zhang, Min},
	month = sep,
	year = {2018},
	keywords = {分类},
	pages = {1857006},
	file = {Jiang 等。 - 2018 - White Blood Cells Classification with Deep Convolu.pdf:/home/red0orange/Zotero/storage/V5JP2JJL/Jiang 等。 - 2018 - White Blood Cells Classification with Deep Convolu.pdf:application/pdf},
}

@inproceedings{tran_blood_2018,
	address = {Xi'an, China},
	series = {{SegNet语义分割} 红白细胞两类},
	title = {Blood {Cell} {Images} {Segmentation} using {Deep} {Learning} {Semantic} {Segmentation}},
	isbn = {978-1-72811-304-3},
	url = {https://ieeexplore.ieee.org/document/8644754/},
	doi = {10.1109/ICECOME.2018.8644754},
	abstract = {S egmentation of red blood cells (RBCs) and white blood cells (WBCs) in peripheral blood smear images plays an important role in the evaluation and diagnosis a vast of disorders, including infection, leukemia, and some types of cancer. Generally, various image processing techniques are used to enhance the quality of images before the segmentation step. Therefore, the segmentation of blood cells is still a challenge. However, in this research, deep learning semantic segmentation - cutting-edge technology is applied for segmentation red blood cells and white blood cells in blood smear images. The experiment result shows that the global accuracy of our model yielded 89.45\%. Besides, the accuracy of the segmentation of white blood cells, red blood cells, and the background of blood smear image reached 94.93\%, 91.11\%, and 87.32\%, respectively.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {2018 {IEEE} {International} {Conference} on {Electronics} and {Communication} {Engineering} ({ICECE})},
	publisher = {IEEE},
	author = {Tran, Thanh and Kwon, Oh-Heum and Kwon, Ki-Ryong and Lee, Suk-Hwan and Kang, Kyung-Won},
	month = dec,
	year = {2018},
	pages = {13--16},
	file = {Tran 等。 - 2018 - Blood Cell Images Segmentation using Deep Learning.pdf:/home/red0orange/Zotero/storage/BWXV5PSW/Tran 等。 - 2018 - Blood Cell Images Segmentation using Deep Learning.pdf:application/pdf},
}

@article{baydilli_classification_2020,
	series = {{CNN改进模型} 白细胞5类},
	title = {Classification of white blood cells using capsule networks},
	volume = {80},
	issn = {08956111},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611120300021},
	doi = {10.1016/j.compmedimag.2020.101699},
	abstract = {Background: While the number and structural features of white blood cells (WBC) can provide important information about the health status of human beings, the ratio of sub-types of these cells and the deformations that can be observed serve as a good indicator in the diagnosis process of some diseases. Hence, correct identiﬁcation and classiﬁcation of the WBC types is of great importance. In addition, the fact that the diagnostic process that is carried out manually is slow, and the success is directly proportional to the expert’s skills makes this problem an excellent ﬁeld of application for computer-aided diagnostic systems. Unfortunately, both the ethical reasons and the cost of image acquisition process is one of the biggest obstacles to the fact that researchers working with medical images are able to collect enough data to produce a stable model. For that reasons, researchers who want to perform a successful analysis with small data sets using classical machine learning methods need to undergo their data a long and error-prone pre-process, while those using deep learning methods need to increase the data size using augmentation techniques. As a result, there is a need for a model that does not need pre-processing and can perform a successful classiﬁcation in small data sets.
Methods: WBCs were classiﬁed under ﬁve categories using a small data set via capsule networks, a new deep learning method. We improved the model using many techniques and compared the results with the most known deep learning methods.
Results: Both the above-mentioned problems were overcame and higher success rates were obtained compared to other deep learning models. While, convolutional neural networks (CNN) and transfer learning (TL) models suffered from over-ﬁtting, capsule networks learned well training data and achieved a high accuracy on test data (96.86\%).
Conclusion: In this study, we brieﬂy discussed the abilities of capsule networks in a case study. We showed that capsule networks are a quite successful alternative for deep learning and medical data analysis when the sample size is limited.},
	language = {en},
	urldate = {2022-03-04},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Baydilli, Yusuf Yargı and Atila, Ümit},
	month = mar,
	year = {2020},
	pages = {101699},
	file = {Baydilli 和 Atila - 2020 - Classification of white blood cells using capsule .pdf:/home/red0orange/Zotero/storage/BCI9VHI4/Baydilli 和 Atila - 2020 - Classification of white blood cells using capsule .pdf:application/pdf},
}

@article{tiwari_detection_2018,
	series = {极简模型 白细胞4分类},
	title = {Detection of subtype blood cells using deep learning},
	volume = {52},
	issn = {13890417},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389041718303760},
	doi = {10.1016/j.cogsys.2018.08.022},
	language = {en},
	urldate = {2022-03-04},
	journal = {Cognitive Systems Research},
	author = {Tiwari, Prayag and Qian, Jia and Li, Qiuchi and Wang, Benyou and Gupta, Deepak and Khanna, Ashish and Rodrigues, Joel J.P.C. and de Albuquerque, Victor Hugo C.},
	month = dec,
	year = {2018},
	keywords = {分类},
	pages = {1036--1044},
	file = {Tiwari 等。 - 2018 - Detection of subtype blood cells using deep learni.pdf:/home/red0orange/Zotero/storage/JLEBYJ43/Tiwari 等。 - 2018 - Detection of subtype blood cells using deep learni.pdf:application/pdf},
}

@phdthesis{dekhil_computational_2020,
	title = {Computational techniques in medical image analysis application for white blood cells classification.},
	url = {https://ir.library.louisville.edu/etd/3424},
	language = {en},
	urldate = {2022-03-04},
	school = {University of Louisville},
	author = {Dekhil, Omar},
	year = {2020},
	doi = {10.18297/etd/3424},
	keywords = {双阶段},
	file = {Dekhil - 2020 - Computational techniques in medical image analysis.pdf:/home/red0orange/Zotero/storage/KRCXVLLH/Dekhil - 2020 - Computational techniques in medical image analysis.pdf:application/pdf},
}

@incollection{mahrishi_obtaining_2020,
	series = {{DenseNet}、{Resnet}、{Xception}，白细胞4类},
	title = {Obtaining {Deep} {Learning} {Models} for {Automatic} {Classification} of {Leukocytes}:},
	isbn = {978-1-79983-095-5 978-1-79983-097-9},
	shorttitle = {Obtaining {Deep} {Learning} {Models} for {Automatic} {Classification} of {Leukocytes}},
	url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-7998-3095-5.ch001},
	abstract = {In this work, the authors classify leukocyte images using the neural network architectures that won the annual ILSVRC competition. The classification of leukocytes is made using pretrained networks and the same networks trained from scratch in order to select the ones that achieve the best performance for the intended task. The categories used are eosinophils, lymphocytes, monocytes, and neutrophils. The analysis of the results takes into account the amount of training required, the regularization techniques used, the training time, and the accuracy in image classification. The best classification results, on the order of 98\%, suggest that it is possible, considering a competent preprocessing, to train a network like the DenseNet with 169 or 201 layers, in about 100 epochs, to classify leukocytes in microscopy images.},
	language = {en},
	urldate = {2022-03-04},
	booktitle = {Advances in {Computer} and {Electrical} {Engineering}},
	publisher = {IGI Global},
	author = {Rodrigues, Pedro João and Igrejas, Getúlio Peixoto and Beato, Romeu Ferreira},
	editor = {Mahrishi, Mehul and Hiran, Kamal Kant and Meena, Gaurav and Sharma, Paawan},
	year = {2020},
	doi = {10.4018/978-1-7998-3095-5.ch001},
	keywords = {分类},
	pages = {1--32},
	file = {Rodrigues 等。 - 2020 - Obtaining Deep Learning Models for Automatic Class.pdf:/home/red0orange/Zotero/storage/AE6S3BCM/Rodrigues 等。 - 2020 - Obtaining Deep Learning Models for Automatic Class.pdf:application/pdf},
}

@article{yildirim_classification_2019,
	series = {{AlexNet}、{Resnet50}、{DenseNet}、{GooLeNet} 白细胞4分类},
	title = {Classification of {White} {Blood} {Cells} by {Deep} {Learning} {Methods} for {Diagnosing} {Disease}},
	volume = {33},
	doi = {10.18280/ria.330502},
	journal = {Revue d'Intelligence Artificielle},
	author = {Yildirim, Muhammed and Çinar, Ahmet},
	month = nov,
	year = {2019},
	keywords = {分类},
	pages = {335--340},
	file = {2019-Classification of White Blood Cells by Deep Learning Methods for Diagnosing.pdf:/home/red0orange/Zotero/storage/GJBEZ9K7/2019-Classification of White Blood Cells by Deep Learning Methods for Diagnosing.pdf:application/pdf},
}

@article{espino_high-efficiency_2021,
	series = {4区水文，{BCCD数据集}，套用{YOLOv4}、{Faster} {RCNN用于BCCD数据集}},
	title = {High-{Efficiency} {Classification} of {White} {Blood} {Cells} {Based} on {Object} {Detection}},
	volume = {2021},
	issn = {2040-2295},
	url = {https://doi.org/10.1155/2021/1615192},
	doi = {10.1155/2021/1615192},
	abstract = {White blood cells (WBCs) play a significant role in the human immune system, and the content of various subtypes of WBCs is usually maintained within a certain range in the human body, while deviant levels are important warning signs for diseases. Hence, the detection and classification of WBCs is an essential diagnostic technique. However, traditional WBC classification technologies based on image processing usually need to segment the collected target cell images from the background. This preprocessing operation not only increases the workload but also heavily affects the classification quality and efficiency. Therefore, we proposed one high-efficiency object detection technology that combines the segmentation and recognition of targets into one step to realize the detection and classification of WBCs in an image at the same time. Two state-of-the-art object detection models, Faster RCNN and Yolov4, were employed and comparatively studied to classify neutrophils, eosinophils, monocytes, and lymphocytes on a balanced and enhanced Blood Cell Count Dataset (BCCD). Our experimental results showed that the Faster RCNN and Yolov4 based deep transfer learning models achieved classification accuracy rates of 96.25\&\#x0025; and 95.75\&\#x0025;, respectively. For the one-stage model, Yolov4, while ensuring more than 95\&\#x0025; accuracy, its detection speed could reach 60 FPS, which showed better performance compared with the two-stage model, Faster RCNN. The high-efficiency object detection network that does not require cell presegmentation can remove the difficulty of image preprocessing and greatly improve the efficiency of the entire classification task, which provides a potential solution for future real-time point-of-care diagnostic systems.},
	journal = {Journal of Healthcare Engineering},
	author = {Yao, Jiangfan and Huang, Xiwei and Wei, Maoyu and Han, Wentao and Xu, Xuefeng and Wang, Renjie and Chen, Jin and Sun, Lingling},
	editor = {Espino, Daniel},
	month = sep,
	year = {2021},
	note = {Publisher: Hindawi},
	keywords = {数据集, 单阶段, 四区},
	pages = {1615192},
	file = {Yao 等。 - 2021 - High-Efficiency Classification of White Blood Cell.pdf:/home/red0orange/Zotero/storage/NDLVWGTQ/Yao 等。 - 2021 - High-Efficiency Classification of White Blood Cell.pdf:application/pdf},
}

@article{radosavovic_designing_2020,
	series = {主流分类模型：{RegNet}},
	title = {Designing {Network} {Design} {Spaces}},
	url = {http://arxiv.org/abs/2003.13678},
	abstract = {In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting ﬁndings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of ﬂop regimes. Under comparable training settings and ﬂops, the RegNet models outperform the popular EfﬁcientNet models while being up to 5× faster on GPUs.},
	language = {en},
	urldate = {2022-03-02},
	journal = {arXiv:2003.13678 [cs]},
	author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.13678},
	file = {Radosavovic 等。 - 2020 - Designing Network Design Spaces.pdf:/home/red0orange/Zotero/storage/BPZHXIHM/Radosavovic 等。 - 2020 - Designing Network Design Spaces.pdf:application/pdf},
}

@article{choi_white_2017,
	series = {{VGG小改进} 10分类(4红+6粒)},
	title = {White blood cell differential count of maturation stages in bone marrow smear using dual-stage convolutional neural networks},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0189259},
	doi = {10.1371/journal.pone.0189259},
	abstract = {The white blood cell differential count of the bone marrow provides information concerning the distribution of immature and mature cells within maturation stages. The results of such examinations are important for the diagnosis of various diseases and for follow-up care after chemotherapy. However, manual, labor-intensive methods to determine the differential count lead to inter- and intra-variations among the results obtained by hematologists. Therefore, an automated system to conduct the white blood cell differential count is highly desirable, but several difficulties hinder progress. There are variations in the white blood cells of each maturation stage, small inter-class differences within each stage, and variations in images because of the different acquisition and staining processes. Moreover, a large number of classes need to be classified for bone marrow smear analysis, and the high density of touching cells in bone marrow smears renders difficult the segmentation of single cells, which is crucial to traditional image processing and machine learning. Few studies have attempted to discriminate bone marrow cells, and even these have either discriminated only a few classes or yielded insufficient performance. In this study, we propose an automated white blood cell differential counting system from bone marrow smear images using a dual-stage convolutional neural network (CNN). A total of 2,174 patch images were collected for training and testing. The dual-stage CNN classified images into 10 classes of the myeloid and erythroid maturation series, and achieved an accuracy of 97.06\%, a precision of 97.13\%, a recall of 97.06\%, and an F-1 score of 97.1\%. The proposed method not only showed high classification performance, but also successfully classified raw images without single cell segmentation and manual feature extraction by implementing CNN. Moreover, it demonstrated rotation and location invariance. These results highlight the promise of the proposed method as an automated white blood cell differential count system.},
	language = {en},
	number = {12},
	urldate = {2022-02-27},
	journal = {PLOS ONE},
	author = {Choi, Jin Woo and Ku, Yunseo and Yoo, Byeong Wook and Kim, Jung-Ah and Lee, Dong Soon and Chai, Young Jun and Kong, Hyoun-Joong and Kim, Hee Chan},
	month = dec,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {分类, 骨髓涂片},
	pages = {e0189259},
	file = {2017-White blood cell differential count of maturation stages in bone marrow smear.pdf:/home/red0orange/Zotero/storage/WWCW7RVW/2017-White blood cell differential count of maturation stages in bone marrow smear.pdf:application/pdf;Snapshot:/home/red0orange/Zotero/storage/NMLU3HZY/article.html:text/html},
}

@article{hung_applying_2017,
	series = {Faster {RCNN} + {AlexNet} 红细胞、白细胞、感染细胞分类},
	title = {Applying {Faster} {R}-{CNN} for {Object} {Detection} on {Malaria} {Images}},
	abstract = {Deep learning based models have had great success in object detection, but the state of the art models have not yet been widely applied to biological image data. We apply for the first time an object detection model previously used on natural images to identify cells and recognize their stages in brightfield microscopy images of malaria-infected blood. Many micro-organisms like malaria parasites are still studied by expert manual inspection and hand counting. This type of object detection task is challenging due to factors like variations in cell shape, density, and color, and uncertainty of some cell classes. In addition, annotated data useful for training is scarce, and the class distribution is inherently highly imbalanced due to the dominance of uninfected red blood cells. We use Faster Region-based Convolutional Neural Network (Faster R-CNN), one of the top performing object detection models in recent years, pre-trained on ImageNet but fine tuned with our data, and compare it to a baseline, which is based on a traditional approach consisting of cell segmentation, extraction of several single-cell features, and classification using random forests. To conduct our initial study, we collect and label a dataset of 1300 fields of view consisting of around 100,000 individual cells. We demonstrate that Faster R-CNN outperforms our baseline and put the results in context of human performance.},
	language = {en},
	author = {Hung, Jane and Carpenter, Anne},
	year = {2017},
	keywords = {双阶段},
	pages = {6},
	file = {Hung 和 Carpenter - Applying Faster R-CNN for Object Detection on Mala.pdf:/home/red0orange/Zotero/storage/W8CL6E7G/Hung 和 Carpenter - Applying Faster R-CNN for Object Detection on Mala.pdf:application/pdf},
}

@article{lin_feature_2017,
	series = {{FPN}},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2022-03-03},
	journal = {arXiv:1612.03144 [cs]},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = apr,
	year = {2017},
	note = {arXiv: 1612.03144},
	file = {Lin 等。 - 2017 - Feature Pyramid Networks for Object Detection.pdf:/home/red0orange/Zotero/storage/N5UZ76R5/Lin 等。 - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@misc{contributors_openmmlabs_2020,
	title = {{OpenMMLab}'s {Image} {Classification} {Toolbox} and {Benchmark}},
	url = {https://github.com/open-mmlab/mmclassification},
	author = {Contributors, MMClassification},
	year = {2020},
}

@misc{wightman_pytorch_2019,
	title = {{PyTorch} {Image} {Models}},
	url = {https://github.com/rwightman/pytorch-image-models},
	publisher = {GitHub},
	author = {Wightman, Ross},
	year = {2019},
	doi = {10.5281/zenodo.4414861},
	note = {Publication Title: GitHub repository},
}

@article{jiao_survey_2019,
	series = {目标检测综述 至2019年 细化},
	title = {A {Survey} of {Deep} {Learning}-{Based} {Object} {Detection}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8825470/},
	doi = {10.1109/ACCESS.2019.2939201},
	abstract = {Object detection is one of the most important and challenging branches of computer vision, which has been widely applied in people’s life, such as monitoring security, autonomous driving and so on, with the purpose of locating instances of semantic objects of a certain class. With the rapid development of deep learning algorithms for detection tasks, the performance of object detectors has been greatly improved. In order to understand the main development status of object detection pipeline thoroughly and deeply, in this survey, we analyze the methods of existing typical detection models and describe the benchmark datasets at ﬁrst. Afterwards and primarily, we provide a comprehensive overview of a variety of object detection methods in a systematic manner, covering the one-stage and two-stage detectors. Moreover, we list the traditional and new applications. Some representative branches of object detection are analyzed as well. Finally, we discuss the architecture of exploiting these object detection methods to build an effective and efﬁcient system and point out a set of development trends to better follow the state-of-the-art algorithms and further research.},
	language = {en},
	urldate = {2022-02-28},
	journal = {IEEE Access},
	author = {Jiao, Licheng and Zhang, Fan and Liu, Fang and Yang, Shuyuan and Li, Lingling and Feng, Zhixi and Qu, Rong},
	year = {2019},
	keywords = {综述},
	pages = {128837--128868},
	file = {Jiao 等。 - 2019 - A Survey of Deep Learning-Based Object Detection.pdf:/home/red0orange/Zotero/storage/SDVFPVFH/Jiao 等。 - 2019 - A Survey of Deep Learning-Based Object Detection.pdf:application/pdf},
}

@article{zou_object_2019,
	series = {目标检测综述 至2019年},
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {http://arxiv.org/abs/1905.05055},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today’s object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century’s time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
	language = {en},
	urldate = {2022-02-28},
	journal = {arXiv:1905.05055 [cs]},
	author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05055},
	keywords = {综述},
	file = {Zou 等。 - 2019 - Object Detection in 20 Years A Survey.pdf:/home/red0orange/Zotero/storage/4SJBTNK3/Zou 等。 - 2019 - Object Detection in 20 Years A Survey.pdf:application/pdf},
}

@inproceedings{loddo_computer-aided_2016,
	address = {Naples},
	series = {图像处理+{SVM} 全流程},
	title = {A {Computer}-{Aided} {System} for {Differential} {Count} from {Peripheral} {Blood} {Cell} {Images}},
	isbn = {978-1-5090-5698-9},
	url = {https://ieeexplore.ieee.org/document/7907453/},
	doi = {10.1109/SITIS.2016.26},
	abstract = {The differential count and analysis of blood cells in microscope images can provide useful information concerning the health of patients. There are three major blood cell types, namely, erythrocytes (RBCs), leukocytes (WBCs), and platelets. Automated blood cell analysers can provide RBCs, WBCs and platelets count but the presence of abnormal cells could affect the cells counting, that should be checked manually. This is why today the conventional practice for such procedure is executed manually by pathologists under light microscope. However, the manual visual inspection is tedious, time consuming, repetitive and it is strongly inﬂuenced by the operator’s capabilities and tiredness. Therefore, a good clinical decision support system for cells counting and classiﬁcation has always become a necessity. Few examples of automated systems that can analyse and classify blood cells have been reported in the literature. This research proposes a computer-aided systems that simulates a human visual inspection to automate the process of detection and identiﬁcation of WBCs and RBCs from blood smear images. The proposed method has been tested on public datasets of blood cell images and demonstrates a reliable and effective system for differential counting, obtaining an average accuracy value of 99.2\% for WBCs and 98\% for RBCs, outperforming the state-of-the-art.},
	language = {en},
	urldate = {2022-02-23},
	booktitle = {2016 12th {International} {Conference} on {Signal}-{Image} {Technology} \& {Internet}-{Based} {Systems} ({SITIS})},
	publisher = {IEEE},
	author = {Loddo, Andrea and Putzu, Lorenzo and Di Ruberto, Cecilia and Fenu, Gianni},
	year = {2016},
	keywords = {外周血涂片, 效果好},
	pages = {112--118},
	file = {Loddo 等。 - 2016 - A Computer-Aided System for Differential Count fro.pdf:/home/red0orange/Zotero/storage/7JL229J3/Loddo 等。 - 2016 - A Computer-Aided System for Differential Count fro.pdf:application/pdf},
}

@article{ruberto_leukocytes_nodate,
	series = {图像处理+{SVM} 全流程},
	title = {A {Leukocytes} {Count} {System} from {Blood} {Smear} {Images}},
	language = {en},
	author = {Ruberto, Cecilia Di and Loddo, Andrea and Putzu, Lorenzo},
	keywords = {外周血涂片, 效果好},
	pages = {10},
}

@article{khan_review_2021,
	series = {2021年综述白细胞检测和分类的文章，小二区{IEEE} {ACCESS}},
	title = {A {Review} on {Traditional} {Machine} {Learning} and {Deep} {Learning} {Models} for {WBCs} {Classification} in {Blood} {Smear} {Images}},
	volume = {9},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9311202/},
	doi = {10.1109/ACCESS.2020.3048172},
	abstract = {In computer vision, traditional machine learning (TML) and deep learning (DL) methods have signiﬁcantly contributed to the advancements of medical image analysis (MIA) by enhancing prediction accuracy, leading to appropriate planning and diagnosis. These methods substantially improved the diagnoses of automatic brain tumor and leukemia/blood cancer detection and can assist the hematologist and doctors by providing a second opinion. This review provides an in-depth analysis of available TML and DL techniques for MIA with a signiﬁcant focus on leukocytes classiﬁcation in blood smear images and other medical imaging domains, i.e., magnetic resonance imaging (MRI), CT images, X-ray, and ultrasounds. The proposed review’s main impact is to ﬁnd the most suitable TML and DL techniques in MIA, especially for leukocyte classiﬁcation in blood smear images. The advanced DL techniques, particularly the evolving convolutional neural networks-based models in the MIA domain, are deeply investigated in this review article. The related literature study reveals that mainstream TML methods are vastly applied to microscopic blood smear images for white blood cells (WBC) analysis. They provide valuable information to medical specialists and help diagnose various hematic diseases such as AIDS and blood cancer (Leukaemia). Based on WBC related literature study and its extensive analysis presented in this study, we derive future research directions for scientists and practitioners working in the MIA domain.},
	language = {en},
	urldate = {2021-10-28},
	journal = {IEEE Access},
	author = {Khan, Siraj and Sajjad, Muhammad and Hussain, Tanveer and Ullah, Amin and Imran, Ali Shariq},
	year = {2021},
	keywords = {综述, 二区},
	pages = {10657--10673},
	file = {Khan 等。 - 2021 - A Review on Traditional Machine Learning and Deep .pdf:/home/red0orange/Zotero/storage/DRL3FPF8/Khan 等。 - 2021 - A Review on Traditional Machine Learning and Deep .pdf:application/pdf},
}

@article{choe_attention-based_2021,
	series = {{类似与GGAM}-{Dropout的思想}，只是用激活层},
	title = {Attention-{Based} {Dropout} {Layer} for {Weakly} {Supervised} {Single} {Object} {Localization} and {Semantic} {Segmentation}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/9105077/},
	doi = {10.1109/TPAMI.2020.2999099},
	abstract = {Both weakly supervised single object localization and semantic segmentation techniques learn an object’s location using only image-level labels. However, these techniques are limited to cover only the most discriminative part of the object and not the entire object. To address this problem, we propose an attention-based dropout layer, which utilizes the attention mechanism to locate the entire object efﬁciently. To achieve this, we devise two key components, 1) hiding the most discriminative part from the model to capture the entire object, and 2) highlighting the informative region to improve the classiﬁcation power of the model. These allow the classiﬁer to be maintained with a reasonable accuracy while the entire object is covered. Through extensive experiments, we demonstrate that the proposed method effectively improves the weakly supervised single object localization accuracy, thereby achieving a new state-of-the-art localization accuracy on the CUB-200-2011 and a comparable accuracy existing state-of-the-arts on the ImageNet-1k. The proposed method is also effective in improving the weakly supervised semantic segmentation performance on the Pascal VOC and MS COCO. Furthermore, the proposed method is more efﬁcient than existing techniques in terms of parameter and computation overheads. Additionally, the proposed method can be easily applied in various backbone networks.},
	language = {en},
	number = {12},
	urldate = {2022-02-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Choe, Junsuk and Lee, Seungho and Shim, Hyunjung},
	month = dec,
	year = {2021},
	pages = {4256--4271},
	file = {Choe 等。 - 2021 - Attention-Based Dropout Layer for Weakly Supervise.pdf:/home/red0orange/Zotero/storage/56F7CQJQ/Choe 等。 - 2021 - Attention-Based Dropout Layer for Weakly Supervise.pdf:application/pdf},
}

@article{wu_hematologist-level_2020,
	series = {{基于YOLOv3的端对端检测}},
	title = {A {Hematologist}-{Level} {Deep} {Learning} {Algorithm} ({BMSNet}) for {Assessing} the {Morphologies} of {Single} {Nuclear} {Balls} in {Bone} {Marrow} {Smears}: {Algorithm} {Development}},
	volume = {8},
	copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
	shorttitle = {A {Hematologist}-{Level} {Deep} {Learning} {Algorithm} ({BMSNet}) for {Assessing} the {Morphologies} of {Single} {Nuclear} {Balls} in {Bone} {Marrow} {Smears}},
	url = {https://medinform.jmir.org/2020/4/e15963},
	doi = {10.2196/15963},
	abstract = {Background: Bone marrow aspiration and biopsy remain the gold standard for the diagnosis of hematological diseases despite the development of flow cytometry (FCM) and molecular and gene analyses. However, the interpretation of the results is laborious and operator dependent. Furthermore, the obtained results exhibit inter- and intravariations among specialists. Therefore, it is important to develop a more objective and automated analysis system. Several deep learning models have been developed and applied in medical image analysis but not in the field of hematological histology, especially for bone marrow smear applications. Objective: The aim of this study was to develop a deep learning model (BMSNet) for assisting hematologists in the interpretation of bone marrow smears for faster diagnosis and disease monitoring. Methods: From January 1, 2016, to December 31, 2018, 122 bone marrow smears were photographed and divided into a development cohort (N=42), a validation cohort (N=70), and a competition cohort (N=10). The development cohort included 17,319 annotated cells from 291 high-resolution photos. In total, 20 photos were taken for each patient in the validation cohort and the competition cohort. This study included eight annotation categories: erythroid, blasts, myeloid, lymphoid, plasma cells, monocyte, megakaryocyte, and unable to identify. BMSNet is a convolutional neural network with the YOLO v3 architecture, which detects and classifies single cells in a single model. Six visiting staff members participated in a human-machine competition, and the results from the FCM were regarded as the ground truth. Results: In the development cohort, according to 6-fold cross-validation, the average precision of the bounding box prediction without consideration of the classification is 67.4\%. After removing the bounding box prediction error, the precision and recall of BMSNet were similar to those of the hematologists in most categories. In detecting more than 5\% of blasts in the validation cohort, the area under the curve (AUC) of BMSNet (0.948) was higher than the AUC of the hematologists (0.929) but lower than the AUC of the pathologists (0.985). In detecting more than 20\% of blasts, the AUCs of the hematologists (0.981) and pathologists (0.980) were similar and were higher than the AUC of BMSNet (0.942). Further analysis showed that the performance difference could be attributed to the myelodysplastic syndrome cases. In the competition cohort, the mean value of the correlations between BMSNet and FCM was 0.960, and the mean values of the correlations between the visiting staff and FCM ranged between 0.952 and 0.990. Conclusions: Our deep learning model can assist hematologists in interpreting bone marrow smears by facilitating and accelerating the detection of hematopoietic cells. However, a detailed morphological interpretation still requires trained hematologists.},
	language = {EN},
	number = {4},
	urldate = {2022-02-24},
	journal = {JMIR Medical Informatics},
	author = {Wu, Yi-Ying and Huang, Tzu-Chuan and Ye, Ren-Hua and Fang, Wen-Hui and Lai, Shiue-Wei and Chang, Ping-Ying and Liu, Wei-Nung and Kuo, Tai-Yu and Lee, Cho-Hao and Tsai, Wen-Chiuan and Lin, Chin},
	month = apr,
	year = {2020},
	note = {Company: JMIR Medical Informatics
Distributor: JMIR Medical Informatics
Institution: JMIR Medical Informatics
Label: JMIR Medical Informatics
Publisher: JMIR Publications Inc., Toronto, Canada},
	pages = {e15963},
	file = {全文:/home/red0orange/Zotero/storage/FCMI2HZV/Wu 等。 - 2020 - A Hematologist-Level Deep Learning Algorithm (BMSN.pdf:application/pdf;Snapshot:/home/red0orange/Zotero/storage/4AVRDLTG/e15963.html:text/html},
}

@article{di_ruberto_detection_2020,
	series = {传统检测方法 开源数据集{ALL}-{IDB} {MP}-{IDB}},
	title = {Detection of red and white blood cells from microscopic blood images using a region proposal approach},
	volume = {116},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482519303890},
	doi = {10.1016/j.compbiomed.2019.103530},
	abstract = {In this paper, we propose a novel and efficient method for detecting and quantifying red and white blood cells from microscopic blood images. Laboratory tests that use a cell counter or a flow cytometer can perform a complete blood count (CBC) rapidly. Nonetheless, a manual blood smear inspection is still needed, both to have a human check on the counter results and to monitor patients under therapy. Moreover, it allows for describing the cells’ appearance as well as any abnormalities. However, manual analysis is lengthy and repetitive, and its result can be subjective and error-prone. In contrast, by using image processing techniques, the proposed system is entirely automated. The main effort is devoted to both achieving high accuracy and finding a way to overcome the typical differences in the condition of blood smear images that computer-aided methods encounter. It is based on the Edge Boxes method, which is considered a state-of-art region proposal approach. By incorporating knowledge-based constraints into the detection process using Edge Boxes, we can find cell proposals rapidly and efficiently. We tested the proposed approach on the Acute Lymphoblastic Leukaemia Image Database (ALL-IDB), a well-known public dataset proposed for leukaemia detection, and the Malaria Parasite Image Database (MP-IDB), a recently proposed dataset for malaria detection. Experimental results were excellent in both cases, outperforming the state-of-the-art on ALL-IDB and creating a strong baseline on MP-IDB, demonstrating that the proposed method can work well on different datasets and different types of images.},
	language = {en},
	urldate = {2022-02-24},
	journal = {Computers in Biology and Medicine},
	author = {Di Ruberto, Cecilia and Loddo, Andrea and Putzu, Lorenzo},
	month = jan,
	year = {2020},
	keywords = {数据集},
	pages = {103530},
	file = {Di Ruberto 等。 - 2020 - Detection of red and white blood cells from micros.pdf:/home/red0orange/Zotero/storage/6AJUZETF/Di Ruberto 等。 - 2020 - Detection of red and white blood cells from micros.pdf:application/pdf},
}

@article{di_ruberto_leucocytes_2016,
	series = {传统方法+{SVM} 三个开源数据集},
	title = {A leucocytes count system from blood smear images: {Segmentation} and counting of white blood cells based on learning by sampling},
	volume = {27},
	issn = {0932-8092, 1432-1769},
	shorttitle = {A leucocytes count system from blood smear images},
	url = {http://link.springer.com/10.1007/s00138-016-0812-4},
	doi = {10.1007/s00138-016-0812-4},
	language = {en},
	number = {8},
	urldate = {2022-02-24},
	journal = {Machine Vision and Applications},
	author = {Di Ruberto, Cecilia and Loddo, Andrea and Putzu, Lorenzo},
	month = nov,
	year = {2016},
	keywords = {数据集},
	pages = {1151--1160},
	file = {Di Ruberto 等。 - 2016 - A leucocytes count system from blood smear images.pdf:/home/red0orange/Zotero/storage/Y9W2D4F8/Di Ruberto 等。 - 2016 - A leucocytes count system from blood smear images.pdf:application/pdf},
}

@article{el_achi_artificial_2020,
	series = {2020年综述人工智能方法诊断血液病},
	title = {Artificial {Intelligence} and {Digital} {Microscopy} {Applications} in {Diagnostic} {Hematopathology}},
	volume = {12},
	issn = {2072-6694},
	url = {https://www.mdpi.com/2072-6694/12/4/797},
	doi = {10.3390/cancers12040797},
	abstract = {Digital Pathology is the process of converting histology glass slides to digital images using sophisticated computerized technology to facilitate acquisition, evaluation, storage, and portability of histologic information. By its nature, digitization of analog histology data renders it amenable to analysis using deep learning/artiﬁcial intelligence (DL/AI) techniques. The application of DL/AI to digital pathology data holds promise, even if the scope of use cases and regulatory framework for deploying such applications in the clinical environment remains in the early stages. Recent studies using whole-slide images and DL/AI to detect histologic abnormalities in general and cancer in particular have shown encouraging results. In this review, we focus on these emerging technologies intended for use in diagnostic hematology and the evaluation of lymphoproliferative diseases.},
	language = {en},
	number = {4},
	urldate = {2022-02-24},
	journal = {Cancers},
	author = {El Achi, Hanadi El and Khoury, Joseph D.},
	month = mar,
	year = {2020},
	pages = {797},
	file = {El Achi 和 Khoury - 2020 - Artificial Intelligence and Digital Microscopy App.pdf:/home/red0orange/Zotero/storage/W2N4PB4F/El Achi 和 Khoury - 2020 - Artificial Intelligence and Digital Microscopy App.pdf:application/pdf},
}

@article{zhao_automatic_2017,
	series = {传统方法检测+{CNN}，传统方法检测属性特征},
	title = {Automatic detection and classification of leukocytes using convolutional neural networks},
	volume = {55},
	issn = {0140-0118, 1741-0444},
	url = {http://link.springer.com/10.1007/s11517-016-1590-x},
	doi = {10.1007/s11517-016-1590-x},
	language = {en},
	number = {8},
	urldate = {2022-02-24},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Zhao, Jianwei and Zhang, Minshu and Zhou, Zhenghua and Chu, Jianjun and Cao, Feilong},
	month = aug,
	year = {2017},
	pages = {1287--1301},
	file = {Zhao 等。 - 2017 - Automatic detection and classification of leukocyt.pdf:/home/red0orange/Zotero/storage/DFJLEI7I/Zhao 等。 - 2017 - Automatic detection and classification of leukocyt.pdf:application/pdf},
}

@article{jin_developing_2020,
	series = {诊断系统 传统方法+{ANN}},
	title = {Developing and {Preliminary} {Validating} an {Automatic} {Cell} {Classification} {System} for {Bone} {Marrow} {Smears}: a {Pilot} {Study}},
	volume = {44},
	issn = {0148-5598, 1573-689X},
	shorttitle = {Developing and {Preliminary} {Validating} an {Automatic} {Cell} {Classification} {System} for {Bone} {Marrow} {Smears}},
	url = {https://link.springer.com/10.1007/s10916-020-01654-y},
	doi = {10.1007/s10916-020-01654-y},
	abstract = {Bone marrow smear examination is an indispensable diagnostic tool in the evaluation of hematological diseases, but the process of manual differential count is labor extensive. In this study, we developed an automatic system with integrated scanning hardware and machine learning-based software to perform differential cell count on bone marrow smears to assist diagnosis. The initial development of the artificial neural network was based on 3000 marrow smear samples retrospectively archived from Sir Run Run Shaw Hospital affiliated to Zhejiang University School of Medicine between June 2016 and December 2018. The preliminary field validating test of the system was based on 124 marrow smears newly collected from the Second Affiliated Hospital of Harbin Medical University between April 2019 and November 2019. The study was performed in parallel of machine automatic recognition with conventional manual differential count by pathologists using the microscope. We selected representative 600,000 marrow cell images as training set of the algorithm, followed by random captured 30,867 cell images for validation. In validation, the overall accuracy of automatic cell classification was 90.1\% (95\% CI, 89.8–90.5\%). In a preliminary field validating test, the reliability coefficient (ICC) of cell series proportion between the two analysis methods were high (ICC ≥ 0.883, P {\textless} 0.0001) and the results by the two analysis methods were consistent for granulocytes and erythrocytes. The system was effective in cell classification and differential cell count on marrow smears. It provides a useful digital tool in the screening and evaluation of various hematological disorders.},
	language = {en},
	number = {10},
	urldate = {2022-02-24},
	journal = {Journal of Medical Systems},
	author = {Jin, Hong and Fu, Xinyan and Cao, Xinyi and Sun, Mingxia and Wang, Xiaofen and Zhong, Yuhong and Yang, Suwen and Qi, Chao and Peng, Bo and He, Xin and He, Fei and Jiang, Yongfang and Gao, Haiyan and Li, Shun and Huang, Zhen and Li, Qiang and Fang, Fengqi and Zhang, Jun},
	month = oct,
	year = {2020},
	pages = {184},
	file = {Jin 等。 - 2020 - Developing and Preliminary Validating an Automatic.pdf:/home/red0orange/Zotero/storage/A73IXCMP/Jin 等。 - 2020 - Developing and Preliminary Validating an Automatic.pdf:application/pdf},
}

@inproceedings{tobias_faster_2020,
	address = {Kyoto, Japan},
	series = {Faster-{RCNN端对端}},
	title = {Faster {R}-{CNN} {Model} {With} {Momentum} {Optimizer} for {RBC} and {WBC} {Variants} {Classification}},
	isbn = {978-1-72817-063-3},
	url = {https://ieeexplore.ieee.org/document/9081283/},
	doi = {10.1109/LifeTech48969.2020.1570619208},
	abstract = {Since many diseases and infections are dependent on the count and type of Red Blood Cells (RBCs) and White Blood Cells (WBCs) present in the blood stream, detection and classification pertaining to them is necessary and relevant. Based from existing related literature, ordinary Neural Networks are usually employed. Also, in existing researches, RBC types are the main focus. Hence, after observing research gaps, a Faster Region-based Convolutional Neural Network (Faster R-CNN) was utilized for this study, focusing not only on RBCs but also on the variants of WBCs. The aim is to have a fast and reliable system in order to achieve the goal of aiding the medical field in the classification of RBCs and WBCs.},
	language = {en},
	urldate = {2022-02-24},
	booktitle = {2020 {IEEE} 2nd {Global} {Conference} on {Life} {Sciences} and {Technologies} ({LifeTech})},
	publisher = {IEEE},
	author = {Tobias, Rogelio Ruzcko and Carlo De Jesus, Luigi and Mital, Matt Ervin and Lauguico, Sandy and Guillermo, Marielet and Vicerra, Ryan Rhay and Bandala, Argel and Dadios, Elmer},
	month = mar,
	year = {2020},
	pages = {235--239},
	file = {Tobias 等。 - 2020 - Faster R-CNN Model With Momentum Optimizer for RBC.pdf:/home/red0orange/Zotero/storage/4HASPHJU/Tobias 等。 - 2020 - Faster R-CNN Model With Momentum Optimizer for RBC.pdf:application/pdf},
}

@article{mohd_safuan_white_2018,
	series = {传统 细胞分割},
	title = {White blood cell ({WBC}) counting analysis in blood smear images using various color segmentation methods},
	volume = {116},
	issn = {02632241},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263224117307042},
	doi = {10.1016/j.measurement.2017.11.002},
	abstract = {White blood cells (WBCs) is closely related to human immunity system which is useful to fight viruses and bacteria. Cancer therapy effectiveness and some of the blood-related diseases can be determined from the count of cell region. Traditionally, WBC count is done manually which yields inaccurate results as the blood sample increases. Moreover, even though hematology counter device is fast and accurate, it is very costly. These problems has led to the invention of cost effective Computer Aided System (CAS) which analyzes the blood smeared images obtained from microscope. In CAS, the most important step is segmentation and any failure at this stage will cause inaccuracy in the subsequent stages. Realizing the importance of segmentation, this paper investigated various segmentation methods for the purpose of WBC counting based on color band thresholding procedure. Initially, color space correction based on L*a*b* color space was applied to standardize the image color intensity. Next, segmentation process was conducted to prune out the WBC region from the background by combining color analysis of RGB, CMYK and HSV with Otsu thresholding. Morphological filter was employed as the segmented image contained noises would affect the system performance. Henceforth, Connected Component Labelling (CCL) was done to distinguish the small particles that still existed in the image. Eventually, Circle Hough Transform (CHT) was applied to identify and count the WBC including the one in the clump region. Overall system performance was accessed and it was found that by using S color component of HSV, color space provided the highest WBC segmentation accuracy which was 96.92\%. Meanwhile, for the combination of two color bands; S-C produced 96.56\% of WBC counting accuracy. Another interesting finding was that the usage of nucleus based detection was superior compared to cytoplasm-based detection for WBC counting purpose.},
	language = {en},
	urldate = {2022-02-24},
	journal = {Measurement},
	author = {Mohd Safuan, Syadia Nabilah and Md Tomari, Mohd Razali and Wan Zakaria, Wan Nurshazwani},
	month = feb,
	year = {2018},
	pages = {543--555},
	file = {Mohd Safuan 等。 - 2018 - White blood cell (WBC) counting analysis in blood .pdf:/home/red0orange/Zotero/storage/MCLKWVY6/Mohd Safuan 等。 - 2018 - White blood cell (WBC) counting analysis in blood .pdf:application/pdf},
}

@article{naugler_peripheral_2014,
	series = {2014年传统方法},
	title = {Peripheral blood smear image analysis: {A} comprehensive review},
	volume = {5},
	issn = {2153-3539},
	shorttitle = {Peripheral blood smear image analysis},
	url = {http://www.jpathinformatics.org/text.asp?2014/5/1/9/129442},
	doi = {10.4103/2153-3539.129442},
	abstract = {Peripheral blood smear image examination is a part of the routine work of every laboratory. The manual examination of these images is tedious, time‑consuming and suffers from interobserver variation. This has motivated researchers to develop different algorithms and methods to automate peripheral blood smear image analysis. Image analysis itself consists of a sequence of steps consisting of image segmentation, features extraction and selection and pattern classification. The image segmentation step addresses the problem of extraction of the object or region of interest from the complicated peripheral blood smear image. Support vector machine (SVM) and artificial neural networks (ANNs) are two common approaches to image segmentation. Features extraction and selection aims to derive descriptive characteristics of the extracted object, which are similar within the same object class and different between different objects. This will facilitate the last step of the image analysis process: pattern classification. The goal of pattern classification is to assign a class to the selected features from a group of known classes. There are two types of classifier learning algorithms: supervised and unsupervised. Supervised learning algorithms predict the class of the object under test using training data of known classes. The training data have a predefined label for every class and the learning algorithm can utilize this data to predict the class of a test object. Unsupervised learning algorithms use unlabeled training data and divide them into groups using similarity measurements. Unsupervised learning algorithms predict the group to which a new test object belong to, based on the training data without giving an explicit class to that object.ANN, SVM, decision tree and K‑nearest neighbor are possible approaches to classification algorithms. Increased discrimination may be obtained by combining several classifiers together.},
	language = {en},
	number = {1},
	urldate = {2022-02-24},
	journal = {Journal of Pathology Informatics},
	author = {Naugler, Christopher and Mohammed, EmadA and Mohamed, MostafaM. A. and Far, BehrouzH},
	year = {2014},
	pages = {9},
	file = {Naugler 等。 - 2014 - Peripheral blood smear image analysis A comprehen.pdf:/home/red0orange/Zotero/storage/Q76YC2H3/Naugler 等。 - 2014 - Peripheral blood smear image analysis A comprehen.pdf:application/pdf},
}

@article{rezatofighi_automatic_2011,
	series = {{传统ANN分类}},
	title = {Automatic recognition of five types of white blood cells in peripheral blood},
	volume = {35},
	issn = {08956111},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611111000048},
	doi = {10.1016/j.compmedimag.2011.01.003},
	abstract = {This paper proposes image processing algorithms to recognize ﬁve types of white blood cells in peripheral blood automatically. First, a method based on Gram–Schmidt orthogonalization is proposed along with a snake algorithm to segment nucleus and cytoplasm of the cells. Then, a variety of features are extracted from the segmented regions. Next, most discriminative features are selected using a Sequential Forward Selection (SFS) algorithm and performances of two classiﬁers, Artiﬁcial Neural Network (ANN) and Support Vector Machine (SVM), are compared. The results demonstrate that the proposed methods are accurate and sufﬁciently fast to be used in hematological laboratories.},
	language = {en},
	number = {4},
	urldate = {2021-10-27},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Rezatofighi, Seyed Hamid and Soltanian-Zadeh, Hamid},
	month = jun,
	year = {2011},
	pages = {333--343},
	file = {Rezatofighi 和 Soltanian-Zadeh - 2011 - Automatic recognition of five types of white blood.pdf:/home/red0orange/Zotero/storage/EI66FT94/Rezatofighi 和 Soltanian-Zadeh - 2011 - Automatic recognition of five types of white blood.pdf:application/pdf},
}

@inproceedings{ben-suliman_computerized_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Computerized {Counting}-{Based} {System} for {Acute} {Lymphoblastic} {Leukemia} {Detection} in {Microscopic} {Blood} {Images}},
	isbn = {978-3-030-01421-6},
	doi = {10.1007/978-3-030-01421-6_17},
	abstract = {Counting of white blood cells (WBCs) and detecting the morphological abnormality of these cells allow for diagnosis some blood diseases such as leukemia. This can be accomplished by automatic quantification analysis of microscope images of blood smear. This paper is oriented towards presenting a novel framework that consists of two sub-systems as indicators for detection Acute Lymphoblastic Leukemia (ALL). The first sub-system aims at counting WBCs by adapting a deep learning based approach to separate agglomerates of WBCs. After separation of WBCs, we propose the second sub-system to detect and count abnormal WBCs (lymphoblasts) required to diagnose ALL. The performance of the proposed framework is evaluated using ALL-IDB dataset. The first presented sub-system is able to count WBCs with an accuracy up to 97.38\%. Furthermore, an approach using ensemble classifiers based on handcrafted features is able to detect and count the lymphoblasts with an average accuracy of 98.67\%.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Ben-Suliman, Karima and Krzyżak, Adam},
	editor = {Kůrková, Věra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2018},
	pages = {167--178},
	file = {Springer Full Text PDF:/home/red0orange/Zotero/storage/UZULHV4E/Ben-Suliman 和 Krzyżak - 2018 - Computerized Counting-Based System for Acute Lymph.pdf:application/pdf},
}

@article{akula_cx-tom_2021,
	title = {{CX}-{ToM}: {Counterfactual} {Explanations} with {Theory}-of-{Mind} for {Enhancing} {Human} {Trust} in {Image} {Recognition} {Models}},
	shorttitle = {{CX}-{ToM}},
	url = {http://arxiv.org/abs/2109.01401},
	abstract = {We propose CX-ToM, short for counterfactual explanations with theory-of-mind, a new explainable AI (XAI) framework for explaining decisions made by a deep convolutional neural network (CNN). In contrast to the current methods in XAI that generate explanations as a single shot response, we pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, our CX-ToM framework generates a sequence of explanations in a dialog by mediating the diﬀerences between the minds of the machine and human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling human’s intention, machine’s mind as inferred by the human as well as human’s mind as inferred by the machine. Moreover, most state-of-the-art XAI frameworks provide attention (or heat map) based explanations. In our work, we show that these attention-based explanations are not suﬃcient for increasing human trust in the underlying CNN model. In CX-ToM, we instead use counterfactual explanations called fault-lines which we deﬁne as follows: given an input image I for which a CNN classiﬁcation model M predicts class cpred, a fault-line identiﬁes the minimal semantic-level features (e.g., stripes on zebra), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classiﬁcation category of I by M to another speciﬁed class calt. Extensive experiments verify our hypotheses, demonstrating that our CX-ToM signiﬁcantly outperforms the state-of-the-art XAI models.},
	language = {en},
	urldate = {2022-02-23},
	journal = {arXiv:2109.01401 [cs]},
	author = {Akula, Arjun R. and Wang, Keze and Liu, Changsong and Saba-Sadiya, Sari and Lu, Hongjing and Todorovic, Sinisa and Chai, Joyce and Zhu, Song-Chun},
	month = dec,
	year = {2021},
	note = {arXiv: 2109.01401},
	file = {Akula 等。 - 2021 - CX-ToM Counterfactual Explanations with Theory-of.pdf:/home/red0orange/Zotero/storage/P8944ZG6/Akula 等。 - 2021 - CX-ToM Counterfactual Explanations with Theory-of.pdf:application/pdf},
}

@article{akula_cocox_2020,
	title = {{CoCoX}: {Generating} {Conceptual} and {Counterfactual} {Explanations} via {Fault}-{Lines}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{CoCoX}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/5643},
	doi = {10.1609/aaai.v34i03.5643},
	abstract = {We present CoCoX (short for Conceptual and Counterfactual Explanations), a model for explaining decisions made by a deep convolutional neural network (CNN). In Cognitive Psychology, the factors (or semantic-level features) that humans zoom in on when they imagine an alternative to a model prediction are often referred to as fault-lines. Motivated by this, our CoCoX model explains decisions made by a CNN using fault-lines. Specifically, given an input image I for which a CNN classification model M predicts class cpred, our fault-line based explanation identifies the minimal semantic-level features (e.g., stripes on zebra, pointed ears of dog), referred to as explainable concepts, that need to be added to or deleted from I in order to alter the classification category of I by M to another specified class calt. We argue that, due to the conceptual and counterfactual nature of fault-lines, our CoCoX explanations are practical and more natural for both expert and non-expert users to understand the internal workings of complex deep learning models. Extensive quantitative and qualitative experiments verify our hypotheses, showing that CoCoX significantly outperforms the state-of-the-art explainable AI models. Our implementation is available at https://github.com/arjunakula/CoCoX},
	language = {en},
	number = {03},
	urldate = {2022-02-23},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Akula, Arjun and Wang, Shuai and Zhu, Song-Chun},
	month = apr,
	year = {2020},
	pages = {2594--2601},
	file = {Akula 等。 - 2020 - CoCoX Generating Conceptual and Counterfactual Ex.pdf:/home/red0orange/Zotero/storage/JE8WT9I8/Akula 等。 - 2020 - CoCoX Generating Conceptual and Counterfactual Ex.pdf:application/pdf},
}

@inproceedings{tateno_cnn-slam_2017,
	address = {Honolulu, HI},
	title = {{CNN}-{SLAM}: {Real}-{Time} {Dense} {Monocular} {SLAM} with {Learned} {Depth} {Prediction}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{CNN}-{SLAM}},
	url = {http://ieeexplore.ieee.org/document/8100178/},
	doi = {10.1109/CVPR.2017.695},
	abstract = {Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efﬁciently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tateno, Keisuke and Tombari, Federico and Laina, Iro and Navab, Nassir},
	month = jul,
	year = {2017},
	pages = {6565--6574},
	file = {2017-CNN-SLAM.pdf:/home/red0orange/Zotero/storage/7W7XUIG3/2017-CNN-SLAM.pdf:application/pdf},
}

@inproceedings{mur-artal_fast_2014,
	address = {Hong Kong, China},
	series = {{ORB}-{SLAM所使用的闭环和重定位方法}},
	title = {Fast relocalisation and loop closing in keyframe-based {SLAM}},
	isbn = {978-1-4799-3685-4},
	url = {http://ieeexplore.ieee.org/document/6906953/},
	doi = {10.1109/ICRA.2014.6906953},
	abstract = {In this paper we present for the ﬁrst time a relocalisation method for keyframe-based SLAM that can deal with severe viewpoint change, at frame-rate, in maps containing thousands of keyframes. As this method relies on local features, it permits the interoperability between cameras, allowing a camera to relocalise in a map built by a different camera. We also perform loop closing (detection + correction), at keyframerate, in loops containing hundreds of keyframes. For both relocalisation and loop closing, we propose a bag of words place recognizer with ORB features, which is able to recognize places spending less than 39 ms, including feature extraction, in databases containing 10K images (without geometrical veriﬁcation). We evaluate the performance of this recognizer in four different datasets, achieving high recall and no false matches, and getting better results than the state-of-art in place recognition, being one order of magnitude faster.},
	language = {en},
	urldate = {2022-02-18},
	booktitle = {2014 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Mur-Artal, Raul and Tardos, Juan D.},
	month = may,
	year = {2014},
	pages = {846--853},
	file = {Mur-Artal 和 Tardos - 2014 - Fast relocalisation and loop closing in keyframe-b.pdf:/home/red0orange/Zotero/storage/ZBAGTSBM/Mur-Artal 和 Tardos - 2014 - Fast relocalisation and loop closing in keyframe-b.pdf:application/pdf},
}

@article{mur-artal_orb-slam2_2017,
	title = {{ORB}-{SLAM2}: an {Open}-{Source} {SLAM} {System} for {Monocular}, {Stereo} and {RGB}-{D} {Cameras}},
	volume = {33},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{ORB}-{SLAM2}},
	url = {http://arxiv.org/abs/1610.06475},
	doi = {10.1109/TRO.2017.2705103},
	abstract = {We present ORB-SLAM2 a complete SLAM system for monocular, stereo and RGB-D cameras, including map reuse, loop closing and relocalization capabilities. The system works in real-time on standard CPUs in a wide variety of environments from small hand-held indoors sequences, to drones ﬂying in industrial environments and cars driving around a city. Our back-end based on bundle adjustment with monocular and stereo observations allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches to map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the beneﬁt of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other ﬁelds.},
	language = {en},
	number = {5},
	urldate = {2022-02-18},
	journal = {IEEE Transactions on Robotics},
	author = {Mur-Artal, Raul and Tardos, Juan D.},
	month = oct,
	year = {2017},
	note = {arXiv: 1610.06475},
	pages = {1255--1262},
	file = {Mur-Artal 和 Tardos - 2017 - ORB-SLAM2 an Open-Source SLAM System for Monocula.pdf:/home/red0orange/Zotero/storage/MMSU7U8C/Mur-Artal 和 Tardos - 2017 - ORB-SLAM2 an Open-Source SLAM System for Monocula.pdf:application/pdf},
}

@article{nixon_measuring_nodate,
	series = {{校准指标SCE}、{ACE}},
	title = {Measuring {Calibration} in {Deep} {Learning}},
	abstract = {The reliability of a machine learning model’s conﬁdence in its predictions is critical for high-risk applications. Calibration—the idea that a model’s predicted probabilities of outcomes reﬂect true probabilities of those outcomes—formalizes this notion. Current calibration metrics fail to consider all of the predictions made by machine learning models, and are inefﬁcient in their estimation of the calibration error. We design the Adaptive Calibration Error (ACE) metric to resolve these pathologies and show that it outperforms other metrics, especially in settings where predictions beyond the maximum prediction that is chosen as the output class matter.},
	language = {en},
	author = {Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
	keywords = {三星},
	pages = {4},
	file = {Nixon 等。 - Measuring Calibration in Deep Learning.pdf:/home/red0orange/Zotero/storage/TCEEJUHB/Nixon 等。 - Measuring Calibration in Deep Learning.pdf:application/pdf},
}

@article{gawlikowski_survey_2022,
	series = {综述，包含评价指标{SCE}、{ACE讲解}},
	title = {A {Survey} of {Uncertainty} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2107.03342},
	abstract = {Over the last decade, neural networks have reached almost every ﬁeld of science and became a crucial part of various real world applications. Due to the increasing spread, conﬁdence in neural network predictions became more and more important. However, basic neural networks do not deliver certainty estimates or suffer from over or under conﬁdence, i.e. are badly calibrated. To overcome this, many researchers have been working on understanding and quantifying uncertainty in a neural network’s prediction. As a result, different types and sources of uncertainty have been identiﬁed and a variety of approaches to measure and quantify uncertainty in neural networks have been proposed. This work gives a comprehensive overview of uncertainty estimation in neural networks, reviews recent advances in the ﬁeld, highlights current challenges, and identiﬁes potential research opportunities. It is intended to give anyone interested in uncertainty estimation in neural networks a broad overview and introduction, without presupposing prior knowledge in this ﬁeld. For that, a comprehensive introduction to the most crucial sources of uncertainty is given and their separation into reducible model uncertainty and not reducible data uncertainty is presented. The modeling of these uncertainties based on deterministic neural networks, Bayesian neural networks, ensemble of neural networks, and test-time data augmentation approaches is introduced and different branches of these ﬁelds as well as the latest developments are discussed. For a practical application, we discuss different measures of uncertainty, approaches for the calibration of neural networks and give an overview of existing baselines and available implementations. Different examples from the wide spectrum of challenges in the ﬁelds of medical image analysis, robotic and earth observation give an idea of the needs and challenges regarding uncertainties in practical applications of neural networks. Additionally, the practical limitations of uncertainty quantiﬁcation methods in neural networks for mission- and safety-critical real world applications are discussed and an outlook on the next steps towards a broader usage of such methods is given.},
	language = {en},
	urldate = {2022-02-15},
	journal = {arXiv:2107.03342 [cs, stat]},
	author = {Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao Xiang},
	month = jan,
	year = {2022},
	note = {arXiv: 2107.03342},
	keywords = {Statistics - Machine Learning},
	file = {Gawlikowski 等。 - 2022 - A Survey of Uncertainty in Deep Neural Networks.pdf:/home/red0orange/Zotero/storage/UFJETYYY/Gawlikowski 等。 - 2022 - A Survey of Uncertainty in Deep Neural Networks.pdf:application/pdf},
}

@article{tomasev_pushing_2022,
	series = {Figure3 评估模型学到了有效的特征},
	title = {Pushing the limits of self-supervised {ResNets}: {Can} we outperform supervised learning without labels on {ImageNet}?},
	shorttitle = {Pushing the limits of self-supervised {ResNets}},
	url = {http://arxiv.org/abs/2201.05119},
	abstract = {Despite recent progress made by self-supervised methods in representation learning with residual networks, they still underperform supervised learning on the ImageNet classiﬁcation benchmark, limiting their applicability in performancecritical settings. Building on prior theoretical insights (Mitrovic et al., 2021) we propose RELICv2 which combines an explicit invariance loss with a contrastive objective over a varied set of appropriately constructed data views. RELICv2 achieves 77.1\% top-1 classiﬁcation accuracy on ImageNet using linear evaluation with a ResNet50 architecture and 80.6\% with larger ResNet models, outperforming previous state-ofthe-art self-supervised approaches by a wide margin. Most notably, RELICv2 is the ﬁrst representation learning method to consistently outperform the supervised baseline in a like-for-like comparison using a range of standard ResNet architectures. Finally we show that despite using ResNet encoders, RELICv2 is comparable to state-of-theart self-supervised vision transformers.},
	language = {en},
	urldate = {2022-01-20},
	journal = {arXiv:2201.05119 [cs, stat]},
	author = {Tomasev, Nenad and Bica, Ioana and McWilliams, Brian and Buesing, Lars and Pascanu, Razvan and Blundell, Charles and Mitrovic, Jovana},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.05119},
	keywords = {Statistics - Machine Learning},
	file = {Tomasev 等。 - 2022 - Pushing the limits of self-supervised ResNets Can.pdf:/home/red0orange/Zotero/storage/44I9VN92/Tomasev 等。 - 2022 - Pushing the limits of self-supervised ResNets Can.pdf:application/pdf},
}

@article{kompa_second_2021,
	series = {医学不确定性的说明文章},
	title = {Second opinion needed: communicating uncertainty in medical machine learning},
	volume = {4},
	issn = {2398-6352},
	shorttitle = {Second opinion needed},
	url = {http://www.nature.com/articles/s41746-020-00367-3},
	doi = {10.1038/s41746-020-00367-3},
	abstract = {Abstract
            There is great excitement that medical artificial intelligence (AI) based on machine learning (ML) can be used to improve decision making at the patient level in a variety of healthcare settings. However, the quantification and communication of uncertainty for individual predictions is often neglected even though uncertainty estimates could lead to more principled decision-making and enable machine learning models to automatically or semi-automatically abstain on samples for which there is high uncertainty. In this article, we provide an overview of different approaches to uncertainty quantification and abstention for machine learning and highlight how these techniques could improve the safety and reliability of current ML systems being used in healthcare settings. Effective quantification and communication of uncertainty could help to engender trust with healthcare workers, while providing safeguards against known failure modes of current machine learning approaches. As machine learning becomes further integrated into healthcare environments, the ability to say “I’m not sure” or “I don’t know” when uncertain is a necessary capability to enable safe clinical deployment.},
	language = {en},
	number = {1},
	urldate = {2022-01-09},
	journal = {npj Digital Medicine},
	author = {Kompa, Benjamin and Snoek, Jasper and Beam, Andrew L.},
	month = dec,
	year = {2021},
	note = {00000},
	keywords = {三星, 指标说明参考},
	pages = {4},
	file = {Kompa 等。 - 2021 - Second opinion needed communicating uncertainty i.pdf:/home/red0orange/Zotero/storage/HVYBFFMZ/Kompa 等。 - 2021 - Second opinion needed communicating uncertainty i.pdf:application/pdf},
}

@article{minderer_revisiting_2021,
	series = {Calibration综述，比较通俗，主要关注“模型架构”家族自带的校准能力，基本不探讨提升校准能力的具体方法},
	title = {Revisiting the {Calibration} of {Modern} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.07998},
	abstract = {Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classiﬁcation models. We systematically relate model calibration and accuracy, and ﬁnd that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.},
	language = {en},
	urldate = {2022-01-09},
	journal = {arXiv:2106.07998 [cs]},
	author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
	month = oct,
	year = {2021},
	note = {00000 
arXiv: 2106.07998},
	file = {Minderer 等。 - 2021 - Revisiting the Calibration of Modern Neural Networ.pdf:/home/red0orange/Zotero/storage/DYBKYNG5/Minderer 等。 - 2021 - Revisiting the Calibration of Modern Neural Networ.pdf:application/pdf},
}

@article{leibig_leveraging_2017,
	series = {医学诊断的不确定性问题},
	title = {Leveraging uncertainty information from deep neural networks for disease detection},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-17876-z},
	doi = {10.1038/s41598-017-17876-z},
	language = {en},
	number = {1},
	urldate = {2022-01-09},
	journal = {Scientific Reports},
	author = {Leibig, Christian and Allken, Vaneeda and Ayhan, Murat Seçkin and Berens, Philipp and Wahl, Siegfried},
	month = dec,
	year = {2017},
	note = {00000},
	pages = {17816},
	file = {Leibig 等。 - 2017 - Leveraging uncertainty information from deep neura.pdf:/home/red0orange/Zotero/storage/M6IXBC8H/Leibig 等。 - 2017 - Leveraging uncertainty information from deep neura.pdf:application/pdf},
}

@article{kim_self-knowledge_nodate,
	series = {有{ECE指标的简要解释}，{ECE指标评价泛化能力}},
	title = {Self-{Knowledge} {Distillation} with {Progressive} {Refinement} of {Targets}},
	abstract = {The generalization capability of deep neural networks has been substantially improved by applying a wide spectrum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model’s own knowledge to soften hard targets (i.e., one-hot vectors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard targets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal ranking. Extensive experimental results on three different tasks, image classification, object detection, and machine translation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.},
	language = {en},
	author = {Kim, Kyungyul and Ji, ByeongMoon and Yoon, Doyoung and Hwang, Sangheum},
	note = {00000},
	pages = {17},
	file = {Kim 等。 - Self-Knowledge Distillation with Progressive Refin.pdf:/home/red0orange/Zotero/storage/85YRIL2S/Kim 等。 - Self-Knowledge Distillation with Progressive Refin.pdf:application/pdf},
}

@article{guo_calibration_nodate,
	series = {{ECE校准指标}，和拒识别表现应该正相关},
	title = {On {Calibration} of {Modern} {Neural} {Networks}},
	abstract = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	language = {en},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
	note = {00000},
	pages = {10},
	file = {Guo 等。 - On Calibration of Modern Neural Networks.pdf:/home/red0orange/Zotero/storage/VVYT8I6W/Guo 等。 - On Calibration of Modern Neural Networks.pdf:application/pdf},
}

@inproceedings{hein_why_2019,
	address = {Long Beach, CA, USA},
	series = {mmc},
	title = {Why {ReLU} {Networks} {Yield} {High}-{Confidence} {Predictions} {Far} {Away} {From} the {Training} {Data} and {How} to {Mitigate} the {Problem}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953721/},
	doi = {10.1109/CVPR.2019.00013},
	abstract = {Classiﬁers used in the wild, in particular for safetycritical systems, should not only have good generalization properties but also should know when they don’t know, in particular make low conﬁdence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classiﬁer function fail in this regard as they produce almost always high conﬁdence predictions far away from the training data. For bounded domains like images we propose a new robust optimization technique similar to adversarial training which enforces low conﬁdence predictions far away from the training data. We show that this technique is surprisingly eﬀective in reducing the conﬁdence of predictions far away from the training data while maintaining high conﬁdence predictions and test error on the original classiﬁcation task compared to standard training.},
	language = {en},
	urldate = {2022-01-09},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hein, Matthias and Andriushchenko, Maksym and Bitterwolf, Julian},
	month = jun,
	year = {2019},
	note = {00000},
	pages = {41--50},
	file = {Hein 等。 - 2019 - Why ReLU Networks Yield High-Confidence Prediction.pdf:/home/red0orange/Zotero/storage/LQHZLHDT/Hein 等。 - 2019 - Why ReLU Networks Yield High-Confidence Prediction.pdf:application/pdf},
}

@article{chang_devil_2020,
	title = {The {Devil} is in the {Channels}: {Mutual}-{Channel} {Loss} for {Fine}-{Grained} {Image} {Classification}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	shorttitle = {The {Devil} is in the {Channels}},
	url = {http://arxiv.org/abs/2002.04264},
	doi = {10.1109/TIP.2020.2973812},
	abstract = {Key for solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms -- a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive on spatial-wise. The end result is therefore a set of feature channels that each reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any bounding-box/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve state-of-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further demonstrate the superiority of MC-Loss when compared with other recently proposed general-purpose losses for visual classification, on two different base networks. Code available at https://github.com/dongliangchang/Mutual-Channel-Loss},
	language = {en},
	urldate = {2022-01-08},
	journal = {IEEE Transactions on Image Processing},
	author = {Chang, Dongliang and Ding, Yifeng and Xie, Jiyang and Bhunia, Ayan Kumar and Li, Xiaoxu and Ma, Zhanyu and Wu, Ming and Guo, Jun and Song, Yi-Zhe},
	year = {2020},
	note = {00000 
arXiv: 2002.04264},
	keywords = {作图参考},
	pages = {4683--4695},
	file = {Chang 等。 - 2020 - The Devil is in the Channels Mutual-Channel Loss .pdf:/home/red0orange/Zotero/storage/5V3XSMZG/Chang 等。 - 2020 - The Devil is in the Channels Mutual-Channel Loss .pdf:application/pdf},
}

@inproceedings{zheng_iu-module_2020,
	address = {London, United Kingdom},
	series = {让模型关注到“更多”区别性区域，和我们的方法是同类},
	title = {{IU}-{Module}: {Intersection} and {Union} {Module} for {Fine}-{Grained} {Visual} {Classification}},
	isbn = {978-1-72811-331-9},
	shorttitle = {{IU}-{Module}},
	url = {https://ieeexplore.ieee.org/document/9102839/},
	doi = {10.1109/ICME46284.2020.9102839},
	abstract = {A predominant viewpoint in previous works of ﬁne-grained visual classiﬁcation (FGVC) is to the localize discriminative parts by auxiliary networks and extract the part-based ﬁnegrained features for classiﬁcation. In this paper, we propose a simple yet effective approach by introducing an intersection and union module (IU-Module). The IU-Module aims to capture more discriminative features by 1) dividing features into distinct groups, 2) sharing parts of interests within each group, and 3) adding a differentiation loss to reduce the similarity among those grouped feature channels. Without adding any new learnable parameters, the proposed approach imposes two straightforward operations, namely channel intersection (CI) and channel union (CU) operations, on the convolutional features and achieves competitive results compared with the state-of-the-art methods. Experimental results on three publicly available FGVC datasets show the effectiveness of the IU-Module. Ablation studies and visualizations are also provided to make further demonstrations.},
	language = {en},
	urldate = {2022-01-08},
	booktitle = {2020 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Zheng, Yixiao and Chang, Dongliang and Xie, Jiyang and Ma, Zhanyu},
	month = jul,
	year = {2020},
	note = {00000},
	keywords = {相关工作},
	pages = {1--6},
	file = {Zheng 等。 - 2020 - IU-Module Intersection and Union Module for Fine-.pdf:/home/red0orange/Zotero/storage/R7UMTBRW/Zheng 等。 - 2020 - IU-Module Intersection and Union Module for Fine-.pdf:application/pdf},
}

@article{xu_grad-cam_2021,
	title = {Grad-{CAM} guided channel-spatial attention module for fine-grained visual classification},
	url = {http://arxiv.org/abs/2101.09666},
	abstract = {Fine-grained visual classiﬁcation (FGVC) is becoming an important research ﬁeld, due to its wide applications and the rapid development of computer vision technologies. The current state-of-the-art (SOTA) methods in the FGVC usually employ attention mechanisms to ﬁrst capture the semantic parts and then discover their subtle differences between distinct classes. The channel-spatial attention mechanisms, which focus on the discriminative channels and regions simultaneously, have signiﬁcantly improved the classiﬁcation performance. However, the existing attention modules are poorly guided since part-based detectors in the FGVC depend on the network learning ability without the supervision of part annotations. As obtaining such part annotations is laborintensive, some visual localization and explanation methods, such as gradient-weighted class activation mapping (GradCAM), can be utilized for supervising the attention mechanism. We propose a Grad-CAM guided channel-spatial attention module for the FGVC, which employs the Grad-CAM to supervise and constrain the attention weights by generating the coarse localization maps. To demonstrate the effectiveness of the proposed method, we conduct comprehensive experiments on three popular FGVC datasets, including CUB-200-2011, Stanford Cars, and FGVC-Aircraft datasets. The proposed method outperforms the SOTA attention modules in the FGVC task. In addition, visualizations of feature maps also demonstrate the superiority of the proposed method against the SOTA approaches.},
	language = {en},
	urldate = {2021-12-24},
	journal = {arXiv:2101.09666 [cs]},
	author = {Xu, Shuai and Chang, Dongliang and Xie, Jiyang and Ma, Zhanyu},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09666},
	file = {Xu 等。 - 2021 - Grad-CAM guided channel-spatial attention module f.pdf:/home/red0orange/Zotero/storage/EFRJIW78/Xu 等。 - 2021 - Grad-CAM guided channel-spatial attention module f.pdf:application/pdf},
}

@article{du_fine-grained_2020,
	series = {{PGM}，关注哪些粒度最具区分性、如何跨多粒度融合信息},
	title = {Fine-{Grained} {Visual} {Classification} via {Progressive} {Multi}-{Granularity} {Training} of {Jigsaw} {Patches}},
	url = {http://arxiv.org/abs/2003.03836},
	abstract = {Fine-grained visual classification (FGVC) is much more challenging than traditional classification tasks due to the inherently subtle intra-class object variations. Recent works mainly tackle this problem by focusing on how to locate the most discriminative parts, more complementary parts, and parts of various granularities. However, less effort has been placed to which granularities are the most discriminative and how to fuse information cross multi-granularity. In this work, we propose a novel framework for fine-grained visual classification to tackle these problems. In particular, we propose: (i) a progressive training strategy that effectively fuses features from different granularities, and (ii) a random jigsaw patch generator that encourages the network to learn features at specific granularities. We obtain state-of-the-art performances on several standard FGVC benchmark datasets, where the proposed method consistently outperforms existing methods or delivers competitive results. The code will be available at https://github.com/PRIS-CV/PMG-Progressive-Multi-Granularity-Training.},
	language = {en},
	urldate = {2021-12-20},
	journal = {arXiv:2003.03836 [cs]},
	author = {Du, Ruoyi and Chang, Dongliang and Bhunia, Ayan Kumar and Xie, Jiyang and Ma, Zhanyu and Song, Yi-Zhe and Guo, Jun},
	month = jul,
	year = {2020},
	note = {00000 
arXiv: 2003.03836},
	keywords = {作图参考},
	file = {Du 等。 - 2020 - Fine-Grained Visual Classification via Progressive.pdf:/home/red0orange/Zotero/storage/8INWPBSL/Du 等。 - 2020 - Fine-Grained Visual Classification via Progressive.pdf:application/pdf},
}

@article{guo_attention_2021,
	series = {注意力最新综述},
	title = {Attention {Mechanisms} in {Computer} {Vision}: {A} {Survey}},
	shorttitle = {Attention {Mechanisms} in {Computer} {Vision}},
	url = {http://arxiv.org/abs/2111.07624},
	abstract = {Humans can naturally and effectively ﬁnd salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classiﬁcation, object detection, semantic segmentation, video understanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention and branch attention; a related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research.},
	language = {en},
	urldate = {2021-12-26},
	journal = {arXiv:2111.07624 [cs]},
	author = {Guo, Meng-Hao and Xu, Tian-Xing and Liu, Jiang-Jiang and Liu, Zheng-Ning and Jiang, Peng-Tao and Mu, Tai-Jiang and Zhang, Song-Hai and Martin, Ralph R. and Cheng, Ming-Ming and Hu, Shi-Min},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.07624},
	file = {Guo 等。 - 2021 - Attention Mechanisms in Computer Vision A Survey.pdf:/home/red0orange/Zotero/storage/TYWCXMAX/Guo 等。 - 2021 - Attention Mechanisms in Computer Vision A Survey.pdf:application/pdf},
}

@article{liu_adcm_2020,
	series = {{Attention结合Dropout}，且分析了不同层热图},
	title = {{ADCM}: attention dropout convolutional module},
	volume = {394},
	issn = {09252312},
	shorttitle = {{ADCM}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092523122030182X},
	doi = {10.1016/j.neucom.2020.02.007},
	language = {en},
	urldate = {2021-12-26},
	journal = {Neurocomputing},
	author = {Liu, Zhigang and Du, Juan and Wang, Mei and Ge, Shuzhi Sam},
	month = jun,
	year = {2020},
	pages = {95--104},
	file = {Liu 等。 - 2020 - ADCM attention dropout convolutional module.pdf:/home/red0orange/Zotero/storage/NZQYWEJV/Liu 等。 - 2020 - ADCM attention dropout convolutional module.pdf:application/pdf},
}

@article{yang_sa-net_2021,
	series = {{CBAM变种}},
	title = {{SA}-{Net}: {Shuffle} {Attention} for {Deep} {Convolutional} {Neural} {Networks}},
	shorttitle = {{SA}-{Net}},
	url = {http://arxiv.org/abs/2102.00240},
	abstract = {Attention mechanisms, which enable a neural network to accurately focus on all the relevant elements of the input, have become an essential component to improve the performance of deep neural networks. There are mainly two attention mechanisms widely used in computer vision studies, spatial attention and channel attention, which aim to capture the pixel-level pairwise relationship and channel dependency, respectively. Although fusing them together may achieve better performance than their individual implementations, it will inevitably increase the computational overhead. In this paper, we propose an efﬁcient Shufﬂe Attention (SA) module to address this issue, which adopts Shufﬂe Units to combine two types of attention mechanisms effectively. Speciﬁcally, SA ﬁrst groups channel dimensions into multiple sub-features before processing them in parallel. Then, for each sub-feature, SA utilizes a Shufﬂe Unit to depict feature dependencies in both spatial and channel dimensions. After that, all sub-features are aggregated and a “channel shufﬂe” operator is adopted to enable information communication between different sub-features. The proposed SA module is efﬁcient yet effective, e.g., the parameters and computations of SA against the backbone ResNet50 are 300 vs. 25.56M and 2.76e-3 GFLOPs vs. 4.12 GFLOPs, respectively, and the performance boost is more than 1.34\% in terms of Top-1 accuracy. Extensive experimental results on commonused benchmarks, including ImageNet-1k for classiﬁcation, MS COCO for object detection, and instance segmentation, demonstrate that the proposed SA outperforms the current SOTA methods signiﬁcantly by achieving higher accuracy while having lower model complexity. The code and models are available at https://github.com/wofmanaf/SA-Net.},
	language = {en},
	urldate = {2021-12-26},
	journal = {arXiv:2102.00240 [cs]},
	author = {Yang, Qing-Long Zhang Yu-Bin},
	month = jan,
	year = {2021},
	note = {arXiv: 2102.00240},
	file = {Yang - 2021 - SA-Net Shuffle Attention for Deep Convolutional N.pdf:/home/red0orange/Zotero/storage/FF3663EL/Yang - 2021 - SA-Net Shuffle Attention for Deep Convolutional N.pdf:application/pdf},
}

@inproceedings{jiang_multi-label_2020,
	address = {Montreal, QC, Canada},
	series = {写作思路可能可以借鉴},
	title = {A {Multi}-{Label} {Deep} {Learning} {Model} with {Interpretable} {Grad}-{CAM} for {Diabetic} {Retinopathy} {Classification}},
	isbn = {978-1-72811-990-8},
	url = {https://ieeexplore.ieee.org/document/9175884/},
	doi = {10.1109/EMBC44109.2020.9175884},
	abstract = {The characteristics of diabetic retinopathy (DR) fundus images generally consist of multiple types of lesions which provided strong evidence for the ophthalmologists to make diagnosis. It is particularly significant to figure out an efficient method to not only accurately classify DR fundus images but also recognize all kinds of lesions on them. In this paper, a deep learning-based multi-label classification model with Gradient-weighted Class Activation Mapping (Grad-CAM) was proposed, which can both make DR classification and automatically locate the regions of different lesions. To reducing laborious annotation work and improve the efficiency of labeling, this paper innovatively considered different types of lesions as different labels for a fundus image so that this paper changed the task of lesion detection into that of image classification. A total of five labels were pre-defined and 3228 fundus images were collected for developing our model. The architecture of deep learning model was designed by ourselves based on ResNet. Through experiments on the test images, this method acquired a sensitive of 93.9\% and a specificity of 94.4\% on DR classification. Moreover, the corresponding regions of lesions were reasonably outlined on the DR fundus images.},
	language = {en},
	urldate = {2021-12-24},
	booktitle = {2020 42nd {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
	publisher = {IEEE},
	author = {Jiang, Hongyang and Xu, Jie and Shi, Rongjie and Yang, Kang and Zhang, Dongdong and Gao, Mengdi and Ma, He and Qian, Wei},
	month = jul,
	year = {2020},
	pages = {1560--1563},
	file = {Jiang 等。 - 2020 - A Multi-Label Deep Learning Model with Interpretab.pdf:/home/red0orange/Zotero/storage/KKAZA7EF/Jiang 等。 - 2020 - A Multi-Label Deep Learning Model with Interpretab.pdf:application/pdf},
}

@incollection{ferrari_cbam_2018,
	address = {Cham},
	series = {{CBAM}},
	title = {{CBAM}: {Convolutional} {Block} {Attention} {Module}},
	volume = {11211},
	isbn = {978-3-030-01233-5 978-3-030-01234-2},
	shorttitle = {{CBAM}},
	url = {http://link.springer.com/10.1007/978-3-030-01234-2_1},
	abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet eﬀective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature reﬁnement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classiﬁcation and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.},
	language = {en},
	urldate = {2021-12-24},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01234-2_1},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {3--19},
	file = {Woo 等。 - 2018 - CBAM Convolutional Block Attention Module.pdf:/home/red0orange/Zotero/storage/Y2QXAMGC/Woo 等。 - 2018 - CBAM Convolutional Block Attention Module.pdf:application/pdf},
}

@article{selvaraju_grad-cam_2020,
	series = {Grad-{CAM}},
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable.},
	language = {en},
	number = {2},
	urldate = {2021-12-23},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {00000 
arXiv: 1610.02391},
	pages = {336--359},
	file = {Selvaraju 等。 - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:/home/red0orange/Zotero/storage/SAIVAQYR/Selvaraju 等。 - 2020 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf},
}

@inproceedings{zhang_intra-class_2021,
	address = {Waikoloa, HI, USA},
	series = {数据增强灵感},
	title = {Intra-class {Part} {Swapping} for {Fine}-{Grained} {Image} {Classification}},
	isbn = {978-1-66540-477-8},
	url = {https://ieeexplore.ieee.org/document/9423080/},
	doi = {10.1109/WACV48630.2021.00325},
	abstract = {Recent works such as Mixup and CutMix have demonstrated the effectiveness of augmenting training data for deep models. These methods generate new data by generally blending random image contents and mixing their labels proportionally. However, this strategy tends to produce unreasonable training samples for ﬁne-grained recognition, leading to limited improvement. This is because mixing random image contents may potentially produce images containing destructed object structures. Further, as the category differences mainly reside in small part regions, mixing labels proportionally to the number of mixed pixels might result in label noisy problem. To augment more reasonable training data, we propose Intra-class Part Swapping (InPS) that produces new data by performing attention-guided content swapping on input pairs from the same class. Compared with previous approaches, InPS avoids introducing noisy labels and ensures a likely holistic structure of objects in generated images. We demonstrate InPS outperforms the most recent augmentation approaches in both ﬁne-grained recognition and weakly object localization. Further, by simply incorporating the mid-level feature learning, our proposed method achieves state-of-the-art performance in the literature while maintaining the simplicity and inference efﬁciency. Our code is publicly available†.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Zhang, Lianbo and Huang, Shaoli and Liu, Wei},
	month = jan,
	year = {2021},
	pages = {3208--3217},
	file = {Zhang 等。 - 2021 - Intra-class Part Swapping for Fine-Grained Image C.pdf:/home/red0orange/Zotero/storage/XKM72ZGP/Zhang 等。 - 2021 - Intra-class Part Swapping for Fine-Grained Image C.pdf:application/pdf},
}

@inproceedings{xie_hyper-class_2015,
	address = {Boston, MA, USA},
	series = {数据增强灵感},
	title = {Hyper-class augmented and regularized deep learning for fine-grained image classification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298880/},
	doi = {10.1109/CVPR.2015.7298880},
	abstract = {Deep convolutional neural networks (CNN) have seen tremendous success in large-scale generic object recognition. In comparison with generic object recognition, ﬁnegrained image classiﬁcation (FGIC) is much more challenging because (i) ﬁne-grained labeled data is much more expensive to acquire (usually requiring domain expertise); (ii) there exists large intra-class and small inter-class variance. Most recent work exploiting deep CNN for image recognition with small training data adopts a simple strategy: pre-train a deep CNN on a large-scale external dataset (e.g., ImageNet) and ﬁne-tune on the small-scale target data to ﬁt the speciﬁc classiﬁcation task. In this paper, beyond the ﬁne-tuning strategy, we propose a systematic framework of learning a deep CNN that addresses the challenges from two new perspectives: (i) identifying easily annotated hyper-classes inherent in the ﬁne-grained data and acquiring a large number of hyper-class-labeled images from readily available external sources (e.g., image search engines), and formulating the problem into multitask learning; (ii) a novel learning model by exploiting a regularization between the ﬁne-grained recognition model and the hyper-class recognition model. We demonstrate the success of the proposed framework on two small-scale ﬁnegrained datasets (Stanford Dogs and Stanford Cars) and on a large-scale car dataset that we collected.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xie, Saining and Yang, Tianbao and {Xiaoyu Wang} and {Yuanqing Lin}},
	month = jun,
	year = {2015},
	pages = {2645--2654},
	file = {Xie 等。 - 2015 - Hyper-class augmented and regularized deep learnin.pdf:/home/red0orange/Zotero/storage/4UA4QHD4/Xie 等。 - 2015 - Hyper-class augmented and regularized deep learnin.pdf:application/pdf},
}

@article{li_attribute_2020,
	series = {数据增强灵感},
	title = {Attribute {Mix}: {Semantic} {Data} {Augmentation} for {Fine} {Grained} {Recognition}},
	shorttitle = {Attribute {Mix}},
	url = {http://arxiv.org/abs/2004.02684},
	abstract = {Collecting ﬁne-grained labels usually requires expert-level domain knowledge and is prohibitive to scale up. In this paper, we propose Attribute Mix, a data augmentation strategy at attribute level to expand the ﬁne-grained samples. The principle lies in that attribute features are shared among ﬁne-grained sub-categories, and can be seamlessly transferred among images. Toward this goal, we propose an automatic attribute mining approach to discover attributes that belong to the same super-category, and Attribute Mix is operated by mixing semantically meaningful attribute features from two images. Attribute Mix is a simple but eﬀective data augmentation strategy that can signiﬁcantly improve the recognition performance without increasing the inference budgets. Furthermore, since attributes can be shared among images from the same super-category, we further enrich the training samples with attribute level labels using images from the generic domain. Experiments on widely used ﬁne-grained benchmarks demonstrate the eﬀectiveness of our proposed method.},
	language = {en},
	urldate = {2021-12-20},
	journal = {arXiv:2004.02684 [cs]},
	author = {Li, Hao and Zhang, Xiaopeng and Xiong, Hongkai and Tian, Qi},
	month = jul,
	year = {2020},
	note = {arXiv: 2004.02684},
	file = {Li 等。 - 2020 - Attribute Mix Semantic Data Augmentation for Fine.pdf:/home/red0orange/Zotero/storage/LW8ASG64/Li 等。 - 2020 - Attribute Mix Semantic Data Augmentation for Fine.pdf:application/pdf},
}

@incollection{leibe_unreasonable_2016,
	address = {Cham},
	title = {The {Unreasonable} {Effectiveness} of {Noisy} {Data} for {Fine}-{Grained} {Recognition}},
	volume = {9907},
	isbn = {978-3-319-46486-2 978-3-319-46487-9},
	url = {http://link.springer.com/10.1007/978-3-319-46487-9_19},
	abstract = {Current approaches for ﬁne-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving ﬁne-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has beneﬁts in both performance and scalability. We demonstrate its eﬃcacy on four ﬁne-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show ﬁrst results at scaling to more than 10,000 ﬁne-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3 \% on CUB-200-2011, 85.4 \% on Birdsnap, 93.4 \% on FGVC-Aircraft, and 80.8 \% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding ﬁne-grained datasets.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Krause, Jonathan and Sapp, Benjamin and Howard, Andrew and Zhou, Howard and Toshev, Alexander and Duerig, Tom and Philbin, James and Fei-Fei, Li},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46487-9_19},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {301--320},
	file = {Krause 等。 - 2016 - The Unreasonable Effectiveness of Noisy Data for F.pdf:/home/red0orange/Zotero/storage/SUIFKK3D/Krause 等。 - 2016 - The Unreasonable Effectiveness of Noisy Data for F.pdf:application/pdf},
}

@misc{batra_rearrangement_2020,
	series = {大牛合作的一篇很强的文章，综述了{Rearrangement}},
	title = {Rearrangement: {A} {Challenge} for {Embodied} {AI}},
	shorttitle = {Rearrangement},
	url = {http://arxiv.org/abs/2011.01975},
	abstract = {We describe a framework for research and evaluation in Embodied AI. Our proposal is based on a canonical task: Rearrangement. A standard task can focus the development of new techniques and serve as a source of trained models that can be transferred to other settings. In the rearrangement task, the goal is to bring a given physical environment into a speciﬁed state. The goal state can be speciﬁed by object poses, by images, by a description in language, or by letting the agent experience the environment in the goal state. We characterize rearrangement scenarios along different axes and describe metrics for benchmarking rearrangement performance. To facilitate research and exploration, we present experimental testbeds of rearrangement scenarios in four different simulation environments. We anticipate that other datasets will be released and new simulation platforms will be built to support training of rearrangement agents and their deployment on physical systems.},
	language = {en},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {Batra, Dhruv and Chang, Angel X. and Chernova, Sonia and Davison, Andrew J. and Deng, Jia and Koltun, Vladlen and Levine, Sergey and Malik, Jitendra and Mordatch, Igor and Mottaghi, Roozbeh and Savva, Manolis and Su, Hao},
	month = nov,
	year = {2020},
	note = {Number: arXiv:2011.01975
arXiv:2011.01975 [cs]},
	file = {Batra 等。 - 2020 - Rearrangement A Challenge for Embodied AI.pdf:/home/red0orange/Zotero/storage/VDVBXLIX/Batra 等。 - 2020 - Rearrangement A Challenge for Embodied AI.pdf:application/pdf},
}

@article{langer_where_2022,
	series = {Langer 2022最新文章},
	title = {Where {Does} {It} {Belong}? {Autonomous} {Object} {Mapping} in {Open}-{World} {Settings}},
	volume = {9},
	issn = {2296-9144},
	shorttitle = {Where {Does} {It} {Belong}?},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2022.828732/full},
	doi = {10.3389/frobt.2022.828732},
	abstract = {Detecting changes such as moved, removed, or new objects is the essence for numerous indoor applications in robotics such as tidying-up, patrolling, and fetch/carry tasks. The problem is particularly challenging in open-world scenarios where novel objects may appear at any time. The main idea of this paper is to detect objects from partial 3D reconstructions of interesting areas in the environment. In our pipeline we ﬁrst identify planes, consider clusters on top as objects, and compute their point-pair-features. They are used to match potential objects and categorize them robustly into static, moved, removed, and novel objects even in the presence of partial object reconstructions and clutter. Our approach dissolves heaps of objects without speciﬁc object knowledge, but only with the knowledge acquired from change detection. The evaluation is performed on real-world data that includes challenges affecting the quality of the reconstruction as a result of noisy input data. We present the novel dataset ObChange for quantitative evaluation, and we compare our method against a baseline using learning-based object detection. The results show that, even with a targeted training set, our approach outperforms the baseline for most test cases. Lastly, we also demonstrate our method’s effectiveness in real robot experiments.},
	language = {en},
	urldate = {2022-05-16},
	journal = {Frontiers in Robotics and AI},
	author = {Langer, Edith and Patten, Timothy and Vincze, Markus},
	month = apr,
	year = {2022},
	pages = {828732},
	file = {Langer 等。 - 2022 - Where Does It Belong Autonomous Object Mapping in.pdf:/home/red0orange/Zotero/storage/TES9TNHG/Langer 等。 - 2022 - Where Does It Belong Autonomous Object Mapping in.pdf:application/pdf},
}

@inproceedings{langer_robust_2020,
	address = {Las Vegas, NV, USA},
	series = {Langer 2020文章},
	title = {Robust and {Efficient} {Object} {Change} {Detection} by {Combining} {Global} {Semantic} {Information} and {Local} {Geometric} {Verification}},
	isbn = {978-1-72816-212-6},
	url = {https://ieeexplore.ieee.org/document/9341664/},
	doi = {10.1109/IROS45743.2020.9341664},
	abstract = {Identifying new, moved or missing objects is an important capability for robot tasks such as surveillance or maintaining order in homes, ofﬁces and industrial settings. However, current approaches do not distinguish between novel objects or simple scene readjustments nor do they sufﬁciently deal with localization error and sensor noise. To overcome these limitations, we combine the strengths of global and local methods for efﬁcient detection of novel objects in 3D reconstructions of indoor environments. Global structure, determined from 3D semantic information, is exploited to establish object candidates. These are then locally veriﬁed by comparing isolated geometry to a reference reconstruction provided by the task. We evaluate our approach on a novel dataset containing different types of rooms with 31 scenes and 260 annotated objects. Experiments show that our proposed approach signiﬁcantly outperforms baseline methods.},
	language = {en},
	urldate = {2022-05-16},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Langer, Edith and Patten, Timothy and Vincze, Markus},
	month = oct,
	year = {2020},
	pages = {8453--8460},
	file = {Langer 等。 - 2020 - Robust and Efficient Object Change Detection by Co.pdf:/home/red0orange/Zotero/storage/X2YGLIZQ/Langer 等。 - 2020 - Robust and Efficient Object Change Detection by Co.pdf:application/pdf},
}

@misc{furrer_incremental_2018,
	title = {Incremental {Object} {Database}: {Building} {3D} {Models} from {Multiple} {Partial} {Observations}},
	shorttitle = {Incremental {Object} {Database}},
	url = {http://arxiv.org/abs/1808.00760},
	abstract = {Collecting 3D object datasets involves a large amount of manual work and is time consuming. Getting complete models of objects either requires a 3D scanner that covers all the surfaces of an object or one needs to rotate it to completely observe it. We present a system that incrementally builds a database of objects as a mobile agent traverses a scene. Our approach requires no prior knowledge of the shapes present in the scene. Object-like segments are extracted from a global segmentation map, which is built online using the input of segmented RGB-D images. These segments are stored in a database, matched among each other, and merged with other previously observed instances. This allows us to create and improve object models on the ﬂy and to use these merged models to reconstruct also unobserved parts of the scene. The database contains each (potentially merged) object model only once, together with a set of poses where it was observed. We evaluate our pipeline with one public dataset, and on a newly created Google Tango dataset containing four indoor scenes with some of the objects appearing multiple times, both within and across scenes.},
	language = {en},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {Furrer, Fadri and Novkovic, Tonci and Fehr, Marius and Gawel, Abel and Grinvald, Margarita and Sattler, Torsten and Siegwart, Roland and Nieto, Juan},
	month = aug,
	year = {2018},
	note = {Number: arXiv:1808.00760
arXiv:1808.00760 [cs]},
	file = {Furrer 等。 - 2018 - Incremental Object Database Building 3D Models fr.pdf:/home/red0orange/Zotero/storage/8PY72JF6/Furrer 等。 - 2018 - Incremental Object Database Building 3D Models fr.pdf:application/pdf},
}

@misc{grinvald_tsdf_2021,
	title = {{TSDF}++: {A} {Multi}-{Object} {Formulation} for {Dynamic} {Object} {Tracking} and {Reconstruction}},
	shorttitle = {{TSDF}++},
	url = {http://arxiv.org/abs/2105.07468},
	abstract = {The ability to simultaneously track and reconstruct multiple objects moving in the scene is of the utmost importance for robotic tasks such as autonomous navigation and interaction. Virtually all of the previous attempts to map multiple dynamic objects have evolved to store individual objects in separate reconstruction volumes and track the relative pose between them. While simple and intuitive, such formulation does not scale well with respect to the number of objects in the scene and introduces the need for an explicit occlusion handling strategy. In contrast, we propose a map representation that allows maintaining a single volume for the entire scene and all the objects therein. To this end, we introduce a novel multi-object TSDF formulation that can encode multiple object surfaces at any given location in the map. In a multiple dynamic object tracking and reconstruction scenario, our representation allows maintaining accurate reconstruction of surfaces even while they become temporarily occluded by other objects moving in their proximity. We evaluate the proposed TSDF++ formulation on a public synthetic dataset and demonstrate its ability to preserve reconstructions of occluded surfaces when compared to the standard TSDF map representation. Code is available at https://github.com/ethz-asl/tsdf-plusplus.},
	language = {en},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {Grinvald, Margarita and Tombari, Federico and Siegwart, Roland and Nieto, Juan},
	month = may,
	year = {2021},
	note = {Number: arXiv:2105.07468
arXiv:2105.07468 [cs]},
	file = {Grinvald 等。 - 2021 - TSDF++ A Multi-Object Formulation for Dynamic Obj.pdf:/home/red0orange/Zotero/storage/GMRT22MS/Grinvald 等。 - 2021 - TSDF++ A Multi-Object Formulation for Dynamic Obj.pdf:application/pdf},
}

@article{qiu_3d-aware_2020,
	title = {{3D}-{Aware} {Scene} {Change} {Captioning} {From} {Multiview} {Images}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/9120195/},
	doi = {10.1109/LRA.2020.3003290},
	abstract = {In this paper, we propose a framework that recognizes and describes changes that occur in a scene observed from multiple viewpoints in natural language text. The ability to recognize and describe changes that occurred in a 3D scene plays an essential role in a variety of human-robot interaction applications. However, most current 3D vision studies have focused on understanding the static 3D scene. Existing scene change captioning approaches recognize and generate change captions from single-view images. Those methods have limited ability to deal with camera movement, object occlusion, which are common in real-world settings. To resolve these problems, we propose a framework that observes every scene from multiple viewpoints and describes the scene change based on an understanding of the underlying 3D structure of scenes. We build three synthetic datasets consisting of primitive 3D object and scanned real object models for evaluation. The results indicate that our method outperforms the previous state-of-theart 2D-based method by a large margin in terms of sentence generation and change understanding correctness. In addition, our method is more robust to camera movements compared to the previous method and also performs better for scenes with occlusions. Moreover, our method also shows encouraging results in a realistic scene-setting, which indicates the possibility of adapting our framework to a more complicated and extensive scene-settings.},
	language = {en},
	number = {3},
	urldate = {2022-05-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Qiu, Yue and Satoh, Yutaka and Suzuki, Ryota and Iwata, Kenji and Kataoka, Hirokatsu},
	month = jul,
	year = {2020},
	pages = {4743--4750},
	file = {Qiu 等。 - 2020 - 3D-Aware Scene Change Captioning From Multiview Im.pdf:/home/red0orange/Zotero/storage/LKR9TIB4/Qiu 等。 - 2020 - 3D-Aware Scene Change Captioning From Multiview Im.pdf:application/pdf},
}

@inproceedings{fehr_tsdf-based_2017,
	address = {Singapore, Singapore},
	title = {{TSDF}-based change detection for consistent long-term dense reconstruction and dynamic object discovery},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989614/},
	doi = {10.1109/ICRA.2017.7989614},
	abstract = {Robots that are operating for extended periods of time need to be able to deal with changes in their environment and represent them adequately in their maps. In this paper, we present a novel 3D reconstruction algorithm based on an extended Truncated Signed Distance Function (TSDF) that enables to continuously reﬁne the static map while simultaneously obtaining 3D reconstructions of dynamic objects in the scene. This is a challenging problem because map updates happen incrementally and are often incomplete. Previous work typically performs change detection on point clouds, surfels or maps, which are not able to distinguish between unexplored and empty space. In contrast, our TSDF-based representation naturally contains this information and thus allows us to more robustly solve the scene differencing problem. We demonstrate the algorithms performance as part of a system for unsupervised object discovery and class recognition. We evaluated our algorithm on challenging datasets that we recorded over several days with RGB-D enabled tablets. To stimulate further research in this area, all of our datasets are publicly available3.},
	language = {en},
	urldate = {2022-05-14},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Fehr, Marius and Furrer, Fadri and Dryanovski, Ivan and Sturm, Jurgen and Gilitschenski, Igor and Siegwart, Roland and Cadena, Cesar},
	month = may,
	year = {2017},
	pages = {5237--5244},
	file = {Fehr 等。 - 2017 - TSDF-based change detection for consistent long-te.pdf:/home/red0orange/Zotero/storage/FT45MV75/Fehr 等。 - 2017 - TSDF-based change detection for consistent long-te.pdf:application/pdf},
}

@article{wald_learning_2022,
	series = {整理机器人-tc recom},
	title = {Learning {3D} {Semantic} {Scene} {Graphs} with {Instance} {Embeddings}},
	volume = {130},
	issn = {0920-5691, 1573-1405},
	url = {https://link.springer.com/10.1007/s11263-021-01546-9},
	doi = {10.1007/s11263-021-01546-9},
	abstract = {A 3D scene is more than the geometry and classes of the objects it comprises. An essential aspect beyond object-level perception is the scene context, described as a dense semantic network of interconnected nodes. Scene graphs have become a common representation to encode the semantic richness of images, where nodes in the graph are object entities connected by edges, so-called relationships. Such graphs have been shown to be useful in achieving state-of-the-art performance in image captioning, visual question answering and image generation or editing. While scene graph prediction methods so far focused on images, we propose instead a novel neural network architecture for 3D data, where the aim is to learn to regress semantic graphs from a given 3D scene. With this work, we go beyond object-level perception, by exploring relations between object entities. Our method learns instance embeddings alongside a scene segmentation and is able to predict semantics for object nodes and edges. We leverage 3DSSG, a large scale dataset based on 3RScan that features scene graphs of changing 3D scenes. Finally, we show the effectiveness of graphs as an intermediate representation on a retrieval task.},
	language = {en},
	number = {3},
	urldate = {2022-05-13},
	journal = {International Journal of Computer Vision},
	author = {Wald, Johanna and Navab, Nassir and Tombari, Federico},
	month = mar,
	year = {2022},
	pages = {630--651},
	file = {Wald 等。 - 2022 - Learning 3D Semantic Scene Graphs with Instance Em.pdf:/home/red0orange/Zotero/storage/RF6ZXIIR/Wald 等。 - 2022 - Learning 3D Semantic Scene Graphs with Instance Em.pdf:application/pdf},
}

@misc{zhu_hierarchical_2021,
	series = {整理机器人-tc recom},
	title = {Hierarchical {Planning} for {Long}-{Horizon} {Manipulation} with {Geometric} and {Symbolic} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2012.07277},
	abstract = {We present a visually grounded hierarchical planning algorithm for long-horizon manipulation tasks. Our algorithm offers a joint framework of neuro-symbolic task planning and low-level motion generation conditioned on the specified goal. At the core of our approach is a two-level scene graph representation, namely geometric scene graph and symbolic scene graph. This hierarchical representation serves as a structured, object-centric abstraction of manipulation scenes. Our model uses graph neural networks to process these scene graphs for predicting high-level task plans and low-level motions. We demonstrate that our method scales to long-horizon tasks and generalizes well to novel task goals. We validate our method in a kitchen storage task in both physical simulation and the real world. Our experiments show that our method achieved over 70\% success rate and nearly 90\% of subgoal completion rate on the real robot while being four orders of magnitude faster in computation time compared to standard search-based task-and-motion planner.},
	language = {en},
	urldate = {2022-05-13},
	publisher = {arXiv},
	author = {Zhu, Yifeng and Tremblay, Jonathan and Birchfield, Stan and Zhu, Yuke},
	month = mar,
	year = {2021},
	note = {Number: arXiv:2012.07277
arXiv:2012.07277 [cs]},
	file = {Zhu 等。 - 2021 - Hierarchical Planning for Long-Horizon Manipulatio.pdf:/home/red0orange/Zotero/storage/M7B9YU47/Zhu 等。 - 2021 - Hierarchical Planning for Long-Horizon Manipulatio.pdf:application/pdf},
}

@article{hagenow_registering_2022,
	title = {Registering {Articulated} {Objects} {With} {Human}-in-the-loop {Corrections}},
	url = {http://arxiv.org/abs/2203.06018},
	abstract = {Remotely programming robots to execute tasks often relies on registering objects of interest in the robot’s environment. Frequently, these tasks involve articulating objects such as opening or closing a valve. However, existing humanin-the-loop methods for registering objects do not consider articulations and the corresponding impact to the geometry of the object, which can cause the methods to fail. In this work, we present an approach where the registration system attempts to automatically determine the object model, pose, and articulation for user-selected points using a nonlinear iterative closest point algorithm. When the automated ﬁtting is incorrect, the operator can iteratively intervene with corrections after which the system will reﬁt the object. We present an implementation of our ﬁtting procedure and evaluate it with a user study that shows that it can improve user performance, in measures of time on task and task load, ease of use, and usefulness compared to a manual registration approach. We also present a situated example that demonstrates the integration of our method in an end-to-end system for articulating a remote valve.},
	language = {en},
	urldate = {2022-05-09},
	journal = {arXiv:2203.06018 [cs]},
	author = {Hagenow, Michael and Senft, Emmanuel and Laske, Evan and Hambuchen, Kimberly and Fong, Terrence and Radwin, Robert and Gleicher, Michael and Mutlu, Bilge and Zinn, Michael},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.06018},
	file = {Hagenow 等。 - 2022 - Registering Articulated Objects With Human-in-the-.pdf:/home/red0orange/Zotero/storage/26WAVZZX/Hagenow 等。 - 2022 - Registering Articulated Objects With Human-in-the-.pdf:application/pdf},
}

@article{zhi_-place_2021,
	title = {In-{Place} {Scene} {Labelling} and {Understanding} with {Implicit} {Scene} {Representation}},
	url = {http://arxiv.org/abs/2103.15875},
	abstract = {Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-deﬁned properties. We extend neural radiance ﬁelds (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations speciﬁc to the scene. The intrinsic multi-view consistency and smoothness of NeRF beneﬁt semantics by enabling sparse labels to efﬁciently propagate. We show the beneﬁt of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efﬁcient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.},
	language = {en},
	urldate = {2022-05-08},
	journal = {arXiv:2103.15875 [cs]},
	author = {Zhi, Shuaifeng and Laidlow, Tristan and Leutenegger, Stefan and Davison, Andrew J.},
	month = aug,
	year = {2021},
	note = {arXiv: 2103.15875},
	file = {Zhi 等。 - 2021 - In-Place Scene Labelling and Understanding with Im.pdf:/home/red0orange/Zotero/storage/7PWTE5PQ/Zhi 等。 - 2021 - In-Place Scene Labelling and Understanding with Im.pdf:application/pdf},
}

@article{zhi_ilabel_2021,
	title = {{ILabel}: {Interactive} {Neural} {Scene} {Labelling}},
	shorttitle = {{ILabel}},
	url = {http://arxiv.org/abs/2111.14637},
	abstract = {Joint representation of geometry, colour and semantics using a 3D neural ﬁeld enables accurate dense labelling from ultra-sparse interactions as a user reconstructs a scene in real-time using a handheld RGB-D sensor. Our iLabel system requires no training data, yet can densely label scenes more accurately than standard methods trained on large, expensively labelled image datasets. Furthermore, it works in an ‘open set’ manner, with semantic classes deﬁned on the ﬂy by the user.},
	language = {en},
	urldate = {2022-05-08},
	journal = {arXiv:2111.14637 [cs]},
	author = {Zhi, Shuaifeng and Sucar, Edgar and Mouton, Andre and Haughton, Iain and Laidlow, Tristan and Davison, Andrew J.},
	month = dec,
	year = {2021},
	note = {arXiv: 2111.14637},
	file = {Zhi 等。 - 2021 - ILabel Interactive Neural Scene Labelling.pdf:/home/red0orange/Zotero/storage/K9U4TEKH/Zhi 等。 - 2021 - ILabel Interactive Neural Scene Labelling.pdf:application/pdf},
}

@article{xie_pix2vox_2020,
	title = {{Pix2Vox}++: {Multi}-scale {Context}-aware {3D} {Object} {Reconstruction} from {Single} and {Multiple} {Images}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{Pix2Vox}++},
	url = {http://arxiv.org/abs/2006.12250},
	doi = {10.1007/s11263-020-01347-6},
	abstract = {Recovering the 3D shape of an object from single or multiple images with deep neural networks has been attracting increasing attention in the past few years. Mainstream works (e.g. 3D-R2N2) use recurrent neural networks (RNNs) to sequentially fuse feature maps of input images. However, RNN-based approaches are unable to produce consistent reconstruction results when given the same input images with different orders. Moreover, RNNs may forget important features from early input images due to long-term memory loss. To address these issues, we propose a novel framework for single-view and multi-view 3D object reconstruction, named Pix2Vox++. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. A multi-scale context-aware fusion module is then introduced to adaptively select high-quality reconstructions for different parts from all coarse 3D volumes to obtain a fused 3D volume. To further correct the wrongly recovered parts in the fused 3D volume, a refiner is adopted to generate the final output. Experimental results on the ShapeNet, Pix3D, and Things3D benchmarks show that Pix2Vox++ performs favorably against state-of-the-art methods in terms of both accuracy and efficiency.},
	language = {en},
	number = {12},
	urldate = {2022-05-07},
	journal = {International Journal of Computer Vision},
	author = {Xie, Haozhe and Yao, Hongxun and Zhang, Shengping and Zhou, Shangchen and Sun, Wenxiu},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.12250},
	pages = {2919--2935},
	file = {Xie 等。 - 2020 - Pix2Vox++ Multi-scale Context-aware 3D Object Rec.pdf:/home/red0orange/Zotero/storage/HVPJIK5F/Xie 等。 - 2020 - Pix2Vox++ Multi-scale Context-aware 3D Object Rec.pdf:application/pdf},
}

@article{choy_3d-r2n2_2016,
	title = {{3D}-{R2N2}: {A} {Unified} {Approach} for {Single} and {Multi}-view {3D} {Object} {Reconstruction}},
	shorttitle = {{3D}-{R2N2}},
	url = {http://arxiv.org/abs/1604.00449},
	abstract = {Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data [1]. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-theart methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline).},
	language = {en},
	urldate = {2022-05-07},
	journal = {arXiv:1604.00449 [cs]},
	author = {Choy, Christopher B. and Xu, Danfei and Gwak, JunYoung and Chen, Kevin and Savarese, Silvio},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.00449},
	file = {Choy 等。 - 2016 - 3D-R2N2 A Unified Approach for Single and Multi-v.pdf:/home/red0orange/Zotero/storage/G62XJUCF/Choy 等。 - 2016 - 3D-R2N2 A Unified Approach for Single and Multi-v.pdf:application/pdf},
}

@article{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, φ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally diﬀerentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to eﬀectively optimize neural radiance ﬁelds to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	language = {en},
	urldate = {2022-05-07},
	journal = {arXiv:2003.08934 [cs]},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.08934},
	file = {Mildenhall 等。 - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:/home/red0orange/Zotero/storage/87YI48F9/Mildenhall 等。 - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf},
}

@article{zhang_robotic_2022,
	title = {Robotic {Grasping} from {Classical} to {Modern}: {A} {Survey}},
	shorttitle = {Robotic {Grasping} from {Classical} to {Modern}},
	url = {http://arxiv.org/abs/2202.03631},
	abstract = {Robotic Grasping has always been an active topic in robotics since grasping is one of the fundamental but most challenging skills of robots. It demands the coordination of robotic perception, planning, and control for robustness and intelligence. However, current solutions are still far behind humans, especially when confronting unstructured scenarios. In this paper, we survey the advances of robotic grasping, starting from the classical formulations and solutions to the modern ones. By reviewing the history of robotic grasping, we want to provide a complete view of this community, and perhaps inspire the combination and fusion of different ideas, which we think would be helpful to touch and explore the essence of robotic grasping problems. In detail, we ﬁrstly give an overview of the analytic methods for robotic grasping. After that, we provide a discussion on the recent state-ofthe-art data-driven grasping approaches rising in recent years. With the development of computer vision, semantic grasping is being widely investigated and can be the basis of intelligent manipulation and skill learning for autonomous robotic systems in the future. Therefore, in our survey, we also brieﬂy review the recent progress in this topic. Finally, we discuss the open problems and the future research directions that may be important for the human-level robustness, autonomy, and intelligence of robots.},
	language = {en},
	urldate = {2022-05-06},
	journal = {arXiv:2202.03631 [cs]},
	author = {Zhang, Hanbo and Tang, Jian and Sun, Shiguang and Lan, Xuguang},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.03631},
	file = {Zhang 等。 - 2022 - Robotic Grasping from Classical to Modern A Surve.pdf:/home/red0orange/Zotero/storage/X43V7BS2/Zhang 等。 - 2022 - Robotic Grasping from Classical to Modern A Surve.pdf:application/pdf},
}

@inproceedings{niemeyer_differentiable_2020,
	address = {Seattle, WA, USA},
	title = {Differentiable {Volumetric} {Rendering}: {Learning} {Implicit} {3D} {Representations} {Without} {3D} {Supervision}},
	isbn = {978-1-72817-168-5},
	shorttitle = {Differentiable {Volumetric} {Rendering}},
	url = {https://ieeexplore.ieee.org/document/9157495/},
	doi = {10.1109/CVPR42600.2020.00356},
	abstract = {Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our singleview reconstructions rival those learned with full 3D supervision. Moreover, we ﬁnd that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.},
	language = {en},
	urldate = {2022-05-06},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Niemeyer, Michael and Mescheder, Lars and Oechsle, Michael and Geiger, Andreas},
	month = jun,
	year = {2020},
	pages = {3501--3512},
	file = {Niemeyer 等。 - 2020 - Differentiable Volumetric Rendering Learning Impl.pdf:/home/red0orange/Zotero/storage/9JYW3VZT/Niemeyer 等。 - 2020 - Differentiable Volumetric Rendering Learning Impl.pdf:application/pdf},
}

@article{chen_robustness_2021,
	title = {Robustness {Improvement} of {Using} {Pre}-{Trained} {Network} in {Visual} {Odometry} for {On}-{Road} {Driving}},
	volume = {70},
	issn = {0018-9545, 1939-9359},
	url = {https://ieeexplore.ieee.org/document/9573476/},
	doi = {10.1109/TVT.2021.3120214},
	abstract = {Robustness in on-road driving Visual Odometry (VO) systems is critical, as it determines the reliable performance in various scenarios and environments. Especially with the development of data-driven technology, the combination of data-driven VO and model-based VO has achieved accurate tracking performance. However, the lack of generalization of pre-trained deep neural networks (DNN) limits the robustness of such a combination in unseen environments. In this study, we introduce a novel framework with appropriate usage of DNN prediction and improve the robustness in the self-driving application. Based on the characteristic of on-road self-driving motion and the DNN output, we propose a two-step optimization strategy with a variable degree of freedom (DoF), i.e., the use of two types of DoF representations during pose estimation. Speciﬁcally, our two-step optimization operates according to the residual of the optimization with the motion label classiﬁcation from the pre-trained DNN, as well as our proposed Motion Evaluation by essential matrix construction. Experimental results show that our framework obtains better tracking accuracy than the existing methods.},
	language = {en},
	number = {12},
	urldate = {2022-05-04},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Chen, Weinan and Zhu, Lei and Loo, Shing Yan and Wang, Jiankun and Wang, Chaoqun and Meng, Max Q.-H. and Zhang, Hong},
	month = dec,
	year = {2021},
	keywords = {陈炜楠},
	pages = {12415--12426},
	file = {Chen 等。 - 2021 - Robustness Improvement of Using Pre-Trained Networ.pdf:/home/red0orange/Zotero/storage/RIAXHW4R/Chen 等。 - 2021 - Robustness Improvement of Using Pre-Trained Networ.pdf:application/pdf},
}

@inproceedings{zhu_random_2019,
	address = {Dali, China},
	title = {Random {Walk} {Network} for {3D} {Point} {Cloud} {Classification} and {Segmentation}},
	isbn = {978-1-72816-321-5},
	url = {https://ieeexplore.ieee.org/document/8961535/},
	doi = {10.1109/ROBIO49542.2019.8961535},
	abstract = {Object classiﬁcation and segmentation via point cloud are essential for mobile robot navigation and operation. A lot of researches ranging from 3D voxels, mesh gird and multi-view were proposed based on point cloud. However, an accurate point cloud classiﬁcation is still a challenging problem. Inspired by multi-label classiﬁcation in images and convolutional neural networks (CNN), in this paper we present a novel network, named Random Walk Network (RWNet), which directly processes raw 3D point cloud data to classify points and, as a result, segment one 3D scene. State-of-theart methods mainly focus on the features of one point while spatial relationships are also essential in point classiﬁcation. To deal with this issue, we combine both appearance features and spatial information of feature points to restrain the point cloud processing. We employ PointNet ﬁrst to generate initial point labels and adopt point labels with high conﬁdence as seeds. We then construct the similarity matrix between seeds and low-conﬁdence-labeled points according to their structural and spatial similarity and use Random Walk to obtain the ﬁnal classiﬁcation. We demonstrate our method in 3D classiﬁcation task in various scenes and compare with some benchmark methods. Experimental results show that RWNet has a better performance than others.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {2019 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	publisher = {IEEE},
	author = {Zhu, Lei and Chen, Weinan and Lin, Xubin and He, Li and Guan, Yisheng and Zhang, Hong},
	month = dec,
	year = {2019},
	pages = {1921--1926},
	file = {Zhu 等。 - 2019 - Random Walk Network for 3D Point Cloud Classificat.pdf:/home/red0orange/Zotero/storage/NHZ6G2SW/Zhu 等。 - 2019 - Random Walk Network for 3D Point Cloud Classificat.pdf:application/pdf},
}

@inproceedings{dai_keypoint_2020,
	address = {Paris, France},
	title = {Keypoint {Description} by {Descriptor} {Fusion} {Using} {Autoencoders}},
	isbn = {978-1-72817-395-5},
	url = {https://ieeexplore.ieee.org/document/9197205/},
	doi = {10.1109/ICRA40945.2020.9197205},
	abstract = {Keypoint matching is an important operation in computer vision and its applications such as visual simultaneous localization and mapping (SLAM) in robotics. This matching operation heavily depends on the descriptors of the keypoints, and it must be performed reliably when images undergo conditional changes such as those in illumination and viewpoint. In this paper, a descriptor fusion model (DFM) is proposed to create a robust keypoint descriptor by fusing CNN-based descriptors using autoencoders. Our DFM architecture can be adapted to either trained or pre-trained CNN models. Based on the performance of existing CNN descriptors, we choose HardNet and DenseNet169 as representatives of trained and pre-trained descriptors. Our proposed DFM is evaluated on the latest benchmark datasets in computer vision with challenging conditional changes. The experimental results show that DFM is able to achieve state-of-the-art performance, with the mean mAP that is 6.45\% and 6.53\% higher than HardNet and DenseNet169, respectively.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Dai, Zhuang and Huang, Xinghong and Chen, Weinan and Chen, Chuangbing and He, Li and Wen, Shuhuan and Zhang, Hong},
	month = may,
	year = {2020},
	pages = {65--71},
	file = {Dai 等。 - 2020 - Keypoint Description by Descriptor Fusion Using Au.pdf:/home/red0orange/Zotero/storage/CMZSSEGJ/Dai 等。 - 2020 - Keypoint Description by Descriptor Fusion Using Au.pdf:application/pdf},
}

@article{wang_efficient_2020,
	title = {Efficient {Autonomous} {Exploration} {With} {Incrementally} {Built} {Topological} {Map} in 3-{D} {Environments}},
	volume = {69},
	issn = {0018-9456, 1557-9662},
	url = {https://ieeexplore.ieee.org/document/9115087/},
	doi = {10.1109/TIM.2020.3001816},
	abstract = {Autonomous 3-D exploration with unmanned aerial vehicles (UAVs) is increasingly prevalent for environment monitoring without human intervention. In this article, we present a systematic solution toward efﬁcient UAV exploration in 3-D environments. Innovatively, a road map is incrementally built and maintained along with the exploration process, which explicitly exhibits the topological structure of the 3-D environment. By simplifying the environment, the road map can efﬁciently provide the information gain and the cost-to-go for a candidate region to be explored, which are two quantities for next-best-view (NBV) evaluation, thus prompting the efﬁciency for NBV determination. In addition, with reference to the global plan queried on the road map, we propose a local planner based on the potential ﬁeld method that drives the robot to the information-rich area during the navigation process, which further improves the exploration efﬁciency. The proposed framework and its composed modules are veriﬁed in various 3-D environments, which exhibit their distinctive features in NBV selection and better performance in improving the exploration efﬁciency than other methods.},
	language = {en},
	number = {12},
	urldate = {2022-05-04},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Wang, Chaoqun and Ma, Han and Chen, Weinan and Liu, Li and Meng, Max Q.-H.},
	month = dec,
	year = {2020},
	pages = {9853--9865},
	file = {Wang 等。 - 2020 - Efficient Autonomous Exploration With Incrementall.pdf:/home/red0orange/Zotero/storage/7FMFTXQH/Wang 等。 - 2020 - Efficient Autonomous Exploration With Incrementall.pdf:application/pdf},
}

@article{chen_dynamic_2022,
	title = {Dynamic {Strategy} of {Keyframe} {Selection} {With} {PD} {Controller} for {VSLAM} {Systems}},
	volume = {27},
	issn = {1083-4435, 1941-014X},
	url = {https://ieeexplore.ieee.org/document/9352537/},
	doi = {10.1109/TMECH.2021.3058617},
	abstract = {Keyframe (KF) selection in a KF-based visual simultaneous localization and mapping (VSLAM) system is critical. In previous studies, static thresholds have been used for KF selection decision making; however, suboptimal performance can result from the use of such thresholds. To obtain a better KF setting than that obtained with the existing methods, in this article, we introduce a dynamic KF selection strategy. By considering both the view change between camera observation and KFs in the built map and the rate of this change, we propose to dynamically adjust the threshold for KF selection. A proportion and derivative (PD) controller is designed with the feedback of estimated view change, where the PD controller output is used for KF selection. According to the experimental results, compared with the existing studies, our method can improve the precision of visual tracking by 17.5\% and 16.7\% based on two popular VSLAM systems.},
	language = {en},
	number = {1},
	urldate = {2022-05-04},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Chen, Weinan and Zhu, Lei and Lin, Xubin and He, Li and Guan, Yisheng and Zhang, Hong},
	month = feb,
	year = {2022},
	pages = {115--125},
	file = {Chen 等。 - 2022 - Dynamic Strategy of Keyframe Selection With PD Con.pdf:/home/red0orange/Zotero/storage/UDUANUP8/Chen 等。 - 2022 - Dynamic Strategy of Keyframe Selection With PD Con.pdf:application/pdf},
}

@article{chen_ceb-map_2021,
	title = {{CEB}-{Map}: {Visual} {Localization} {Error} {Prediction} for {Safe} {Navigation}},
	volume = {21},
	issn = {1530-437X, 1558-1748, 2379-9153},
	shorttitle = {{CEB}-{Map}},
	url = {https://ieeexplore.ieee.org/document/9109263/},
	doi = {10.1109/JSEN.2020.2999641},
	abstract = {For safe visual navigation, areas with high localization errors should be concentrated and could be further reﬁned by additional mapping operations. Given an environment map, we propose to predict the visual localization error and hence to either improve the navigation performance or call an additional mapping to reﬁne the built map. Previous work adopts the uncertainty of landmarks for the error prediction. In our work, we take into account both the spatial distribution of visual landmarks and the uncertainty of landmarks. Our main idea is that standing at one place, a good spatial distribution of landmarks means a sufﬁcient enough visible landmarks from all views at that place, i.e., enough landmarks under arbitrary view-direction. Combining the spatial distribution and the uncertainty of landmarks, we propose a new framework to predict the error of visual localization. Furthermore, we show that additional mapping in the area with high predicted error can signiﬁcantly improve the visual localization precision. Experimental results show that there is a strong relationship between the predicted error and the real error, of which the absolute value of correlation coefﬁcient is between 0.707 to 0.915. We apply our method to conduct an optimal reﬁning policy on the built map and the experimental results show the improved localization precision. Applications on navigation test verify the superiority of our proposed method.},
	language = {en},
	number = {10},
	urldate = {2022-05-04},
	journal = {IEEE Sensors Journal},
	author = {Chen, Weinan and Zhu, Lei and Wang, Chaoqun and He, Li and Meng, Max Q.-H.},
	month = may,
	year = {2021},
	pages = {11769--11780},
	file = {Chen 等。 - 2021 - CEB-Map Visual Localization Error Prediction for .pdf:/home/red0orange/Zotero/storage/IL82Z6B3/Chen 等。 - 2021 - CEB-Map Visual Localization Error Prediction for .pdf:application/pdf},
}

@inproceedings{lin_robust_2021,
	address = {Xi'an, China},
	title = {Robust {Improvement} in {3D} {Object} {Landmark} {Inference} for {Semantic} {Mapping}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561596/},
	doi = {10.1109/ICRA48506.2021.9561596},
	abstract = {Recent works on semantic Simultaneous Localization and Mapping (SLAM) utilizing object landmarks have shown superiority in terms of robustness and accuracy in tracking and localization. 3D object landmarks represented by a cubic or quadric surface are inferred from 2D object bounding boxes which are typically captured from multiple views by an object detector. Nevertheless, bounding box noises and small camera baseline may lead to an inaccurate 3D object landmark inference. Inspired by the dual quadric enveloping property, in this work, we introduce the horizontal support assumption to constrain rotation w.r.t. roll and pitch for a quadric representation. As the result, we reduce the number of quadric parameters and narrow down the solution space, and ultimately produce a relatively accurate inference. Extensive experimental evaluations under both simulated and real scenarios are conducted in this paper. Quantitative results demonstrate that our approach outperforms the state-of-the-art.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lin, Xubin and Yang, Yirui and He, Li and Chen, Weinan and Guan, Yisheng and Zhang, Hong},
	month = may,
	year = {2021},
	pages = {13011--13017},
	file = {Lin 等。 - 2021 - Robust Improvement in 3D Object Landmark Inference.pdf:/home/red0orange/Zotero/storage/B84G7W86/Lin 等。 - 2021 - Robust Improvement in 3D Object Landmark Inference.pdf:application/pdf},
}

@article{ye_condition-invariant_2022,
	title = {Condition-{Invariant} and {Compact} {Visual} {Place} {Description} by {Convolutional} {Autoencoder}},
	url = {http://arxiv.org/abs/2204.07350},
	abstract = {Visual place recognition (VPR) in condition-varying environments is still an open problem. Popular solutions are CNN-based image descriptors, which have been shown to outperform traditional image descriptors based on hand-crafted visual features. However, there are two drawbacks of current CNN-based descriptors: a) their high dimension and b) lack of generalization, leading to low eﬃciency and poor performance in applications. In this paper, we propose to use a convolutional autoencoder (CAE) to tackle this problem. We employ a high-level layer of a pre-trained CNN to generate features, and train a CAE to map the features to a low-dimensional space to improve the condition invariance property of the descriptor and reduce its dimension at the same time. We verify our method in three challenging datasets involving signiﬁcant illumination changes, and our method is shown to be superior to the state-of-the-art. The code of our work is publicly available in https://github.com/MedlarTea/CAE-VPR.},
	language = {en},
	urldate = {2022-05-04},
	journal = {arXiv:2204.07350 [cs]},
	author = {Ye, Hanjing and Chen, Weinan and Yu, Jingwen and He, Li and Guan, Yisheng and Zhang, Hong},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.07350},
	keywords = {叶汉京},
	file = {Ye 等。 - 2022 - Condition-Invariant and Compact Visual Place Descr.pdf:/home/red0orange/Zotero/storage/PETM26RY/Ye 等。 - 2022 - Condition-Invariant and Compact Visual Place Descr.pdf:application/pdf},
}

@article{ye_following_2022,
	title = {Following {Closely}: {A} {Robust} {Monocular} {Person} {Following} {System} for {Mobile} {Robot}},
	shorttitle = {Following {Closely}},
	url = {http://arxiv.org/abs/2204.10540},
	abstract = {Monocular person following (MPF) is a capability that supports many useful applications of a mobile robot. However, existing MPF solutions are not completely satisfactory. Firstly, they often fail to track the target at a close distance either because they are based on visual servo or they need the observation of the full body by the robot. Secondly, their target Re-IDentiﬁcation (Re-ID) abilities are weak in cases of target appearance change and highly similar appearance of distracting people. To remove the assumption of full-body observation, we propose a width-based tracking module, which relies on the target width, which can be observed even at a close distance. For handling issues related to appearance variation, we use a global CNN (convolutional neural network) descriptor to represent the target and a ridge regression model to learn a target appearance model online. We adopt a sampling strategy for online classiﬁer learning, in which both long-term and short-term samples are involved. We evaluate our method in two datasets including a public person following dataset and a custom-built with challenging target appearance and target distance. Our method achieves state-of-the-art (SOTA) results on both datasets. The code and dataset of our work in this research are publicly available in https://github.com/MedlarTea/MPF GRR SLT.},
	language = {en},
	urldate = {2022-05-04},
	journal = {arXiv:2204.10540 [cs]},
	author = {Ye, Hanjing and Zhao, Jieting and Pan, Yaling and Chen, Weinan and Zhang, Hong},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.10540},
	keywords = {叶汉京},
	file = {Ye 等。 - 2022 - Following Closely A Robust Monocular Person Follo.pdf:/home/red0orange/Zotero/storage/Q2HHCAH3/Ye 等。 - 2022 - Following Closely A Robust Monocular Person Follo.pdf:application/pdf},
}

@inproceedings{ye_mapping_2021,
	address = {Sanya, China},
	title = {Mapping {While} {Following}: {2D} {LiDAR} {SLAM} in {Indoor} {Dynamic} {Environments} with a {Person} {Tracker}},
	isbn = {978-1-66540-535-5},
	shorttitle = {Mapping {While} {Following}},
	url = {https://ieeexplore.ieee.org/document/9739394/},
	doi = {10.1109/ROBIO54168.2021.9739394},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Biomimetics} ({ROBIO})},
	publisher = {IEEE},
	author = {Ye, Hanjing and Chen, Guangcheng and Chen, Weinan and He, Li and Guan, Yisheng and Zhang, Hong},
	month = dec,
	year = {2021},
	keywords = {叶汉京},
	pages = {826--832},
	file = {Ye 等。 - 2021 - Mapping While Following 2D LiDAR SLAM in Indoor D.pdf:/home/red0orange/Zotero/storage/A5WNKQIB/Ye 等。 - 2021 - Mapping While Following 2D LiDAR SLAM in Indoor D.pdf:application/pdf},
}

@inproceedings{sucar_imap_2021,
	address = {Montreal, QC, Canada},
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{iMAP}},
	url = {https://ieeexplore.ieee.org/document/9710431/},
	doi = {10.1109/ICCV48922.2021.00617},
	abstract = {We show for the ﬁrst time that a multilayer perceptron (MLP) can serve as the only scene representation in a realtime SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-speciﬁc implicit 3D model of occupancy and colour which is also immediately used for tracking.},
	language = {en},
	urldate = {2022-04-30},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	pages = {6209--6218},
	file = {Sucar 等。 - 2021 - iMAP Implicit Mapping and Positioning in Real-Tim.pdf:/home/red0orange/Zotero/storage/X2TX6L9P/Sucar 等。 - 2021 - iMAP Implicit Mapping and Positioning in Real-Tim.pdf:application/pdf},
}

@article{wada_morefusion_2020,
	title = {{MoreFusion}: {Multi}-object {Reasoning} for {6D} {Pose} {Estimation} from {Volumetric} {Fusion}},
	shorttitle = {{MoreFusion}},
	url = {http://arxiv.org/abs/2004.04336},
	abstract = {Robots and other smart devices need efﬁcient objectbased scene representations from their on-board vision systems to reason about contact, physics and occlusion. Recognized precise object models will play an important role alongside non-parametric reconstructions of unrecognized structures. We present a system which can estimate the accurate poses of multiple known objects in contact and occlusion from real-time, embodied multi-view vision. Our approach makes 3D object pose proposals from single RGBD views, accumulates pose estimates and non-parametric occupancy information from multiple views as the camera moves, and performs joint optimization to estimate consistent, non-intersecting poses for multiple objects in contact. We verify the accuracy and robustness of our approach experimentally on 2 object datasets: YCB-Video, and our own challenging Cluttered YCB-Video. We demonstrate a real-time robotics application where a robot arm precisely and orderly disassembles complicated piles of objects, using only on-board RGB-D vision.},
	language = {en},
	urldate = {2022-04-23},
	journal = {arXiv:2004.04336 [cs]},
	author = {Wada, Kentaro and Sucar, Edgar and James, Stephen and Lenton, Daniel and Davison, Andrew J.},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.04336},
	file = {Wada 等。 - 2020 - MoreFusion Multi-object Reasoning for 6D Pose Est.pdf:/home/red0orange/Zotero/storage/M9PE25SF/Wada 等。 - 2020 - MoreFusion Multi-object Reasoning for 6D Pose Est.pdf:application/pdf},
}

@article{wada_reorientbot_2022,
	series = {{ReorientBot}},
	title = {{ReorientBot}: {Learning} {Object} {Reorientation} for {Specific}-{Posed} {Placement}},
	shorttitle = {{ReorientBot}},
	url = {http://arxiv.org/abs/2202.11092},
	abstract = {Robots need the capability of placing objects in arbitrary, speciﬁc poses to rearrange the world and achieve various valuable tasks. Object reorientation plays a crucial role in this as objects may not initially be oriented such that the robot can grasp and then immediately place them in a speciﬁc goal pose. In this work, we present a vision-based manipulation system, ReorientBot, which consists of 1) visual scene understanding with pose estimation and volumetric reconstruction using an onboard RGB-D camera; 2) learned waypoint selection for successful and efﬁcient motion generation for reorientation; 3) traditional motion planning to generate a collision-free trajectory from the selected waypoints. We evaluate our method using the YCB objects in both simulation and the real world, achieving 93\% overall success, 81\% improvement in success rate, and 22\% improvement in execution time compared to a heuristic approach. We demonstrate extended multi-object rearrangement showing the general capability of the system.},
	language = {en},
	urldate = {2022-04-23},
	journal = {arXiv:2202.11092 [cs]},
	author = {Wada, Kentaro and James, Stephen and Davison, Andrew J.},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.11092},
	file = {Wada 等。 - 2022 - ReorientBot Learning Object Reorientation for Spe.pdf:/home/red0orange/Zotero/storage/DUYANR9A/Wada 等。 - 2022 - ReorientBot Learning Object Reorientation for Spe.pdf:application/pdf},
}

@article{wada_safepicking_2022,
	series = {{SafePicking}},
	title = {{SafePicking}: {Learning} {Safe} {Object} {Extraction} via {Object}-{Level} {Mapping}},
	shorttitle = {{SafePicking}},
	url = {http://arxiv.org/abs/2202.05832},
	abstract = {Robots need object-level scene understanding to manipulate objects while reasoning about contact, support, and occlusion among objects. Given a pile of objects, object recognition and reconstruction can identify the boundary of object instances, giving important cues as to how the objects form and support the pile. In this work, we present a system, SafePicking, that integrates object-level mapping and learningbased motion planning to generate a motion that safely extracts occluded target objects from a pile. Planning is done by learning a deep Q-network that receives observations of predicted poses and a depth-based heightmap to output a motion trajectory, trained to maximize a safety metric reward. Our results show that the observation fusion of poses and depth-sensing gives both better performance and robustness to the model. We evaluate our methods using the YCB objects in both simulation and the real world, achieving safe object extraction from piles.},
	language = {en},
	urldate = {2022-04-23},
	journal = {arXiv:2202.05832 [cs]},
	author = {Wada, Kentaro and James, Stephen and Davison, Andrew J.},
	month = mar,
	year = {2022},
	note = {arXiv: 2202.05832},
	file = {Wada 等。 - 2022 - SafePicking Learning Safe Object Extraction via O.pdf:/home/red0orange/Zotero/storage/DLIK5FHN/Wada 等。 - 2022 - SafePicking Learning Safe Object Extraction via O.pdf:application/pdf},
}

@inproceedings{sucar_imap_2021-1,
	address = {Montreal, QC, Canada},
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{iMAP}},
	url = {https://ieeexplore.ieee.org/document/9710431/},
	doi = {10.1109/ICCV48922.2021.00617},
	abstract = {We show for the ﬁrst time that a multilayer perceptron (MLP) can serve as the only scene representation in a realtime SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-speciﬁc implicit 3D model of occupancy and colour which is also immediately used for tracking.},
	language = {en},
	urldate = {2022-04-23},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	pages = {6209--6218},
	file = {Sucar 等。 - 2021 - iMAP Implicit Mapping and Positioning in Real-Tim.pdf:/home/red0orange/Zotero/storage/4FFW574A/Sucar 等。 - 2021 - iMAP Implicit Mapping and Positioning in Real-Tim.pdf:application/pdf},
}

@article{mccormac_fusion_2018,
	title = {Fusion++: {Volumetric} {Object}-{Level} {SLAM}},
	shorttitle = {Fusion++},
	url = {http://arxiv.org/abs/1808.08378},
	abstract = {We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally reﬁned via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is reﬁned over time and an existence probability to account for spurious instance predictions.},
	language = {en},
	urldate = {2022-04-23},
	journal = {arXiv:1808.08378 [cs]},
	author = {McCormac, John and Clark, Ronald and Bloesch, Michael and Davison, Andrew J. and Leutenegger, Stefan},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.08378},
	file = {McCormac 等。 - 2018 - Fusion++ Volumetric Object-Level SLAM.pdf:/home/red0orange/Zotero/storage/3AMSSYYT/McCormac 等。 - 2018 - Fusion++ Volumetric Object-Level SLAM.pdf:application/pdf},
}

@inproceedings{kundu_3d-rcnn_2018,
	address = {Salt Lake City, UT, USA},
	title = {{3D}-{RCNN}: {Instance}-{Level} {3D} {Object} {Reconstruction} via {Render}-and-{Compare}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{3D}-{RCNN}},
	url = {https://ieeexplore.ieee.org/document/8578473/},
	doi = {10.1109/CVPR.2018.00375},
	abstract = {We present a fast inverse-graphics framework for instance-level 3D scene understanding. We train a deep convolutional network that learns to map image regions to the full 3D shape and pose of all object instances in the image. Our method produces a compact 3D representation of the scene, which can be readily used for applications like autonomous driving. Many traditional 2D vision outputs, like instance segmentations and depth-maps, can be obtained by simply rendering our output 3D scene model. We exploit class-speciﬁc shape priors by learning a low dimensional shape-space from collections of CAD models. We present novel representations of shape and pose, that strive towards better 3D equivariance and generalization. In order to exploit rich supervisory signals in the form of 2D annotations like segmentation, we propose a differentiable Render-and-Compare loss that allows 3D shape and pose to be learned with 2D supervision. We evaluate our method on the challenging real-world datasets of Pascal3D+ and KITTI, where we achieve state-of-the-art results.},
	language = {en},
	urldate = {2022-04-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kundu, Abhijit and Li, Yin and Rehg, James M.},
	month = jun,
	year = {2018},
	pages = {3559--3568},
	file = {Kundu 等。 - 2018 - 3D-RCNN Instance-Level 3D Object Reconstruction v.pdf:/home/red0orange/Zotero/storage/WR6IKUC8/Kundu 等。 - 2018 - 3D-RCNN Instance-Level 3D Object Reconstruction v.pdf:application/pdf},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2022-04-22},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	file = {He 等。 - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/red0orange/Zotero/storage/2IGVYVK4/He 等。 - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{kong_foveabox_2020,
	title = {{FoveaBox}: {Beyound} {Anchor}-{Based} {Object} {Detection}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	shorttitle = {{FoveaBox}},
	url = {https://ieeexplore.ieee.org/document/9123553/},
	doi = {10.1109/TIP.2020.3002345},
	abstract = {We present FoveaBox, an accurate, ﬂexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predeﬁned anchors to enumerate possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox, an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters related to anchor boxes, which are often sensitive to the ﬁnal detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object detection. The code has been made publicly available at https://github.com/taokong/FoveaBox.},
	language = {en},
	urldate = {2022-04-22},
	journal = {IEEE Transactions on Image Processing},
	author = {Kong, Tao and Sun, Fuchun and Liu, Huaping and Jiang, Yuning and Li, Lei and Shi, Jianbo},
	year = {2020},
	pages = {7389--7398},
	file = {Kong 等。 - 2020 - FoveaBox Beyound Anchor-Based Object Detection.pdf:/home/red0orange/Zotero/storage/J8CQY8XY/Kong 等。 - 2020 - FoveaBox Beyound Anchor-Based Object Detection.pdf:application/pdf},
}

@article{sucar_nodeslam_2020,
	title = {{NodeSLAM}: {Neural} {Object} {Descriptors} for {Multi}-{View} {Shape} {Reconstruction}},
	shorttitle = {{NodeSLAM}},
	url = {http://arxiv.org/abs/2004.04485},
	abstract = {The choice of scene representation is crucial in both the shape inference algorithms it requires and the smart applications it enables. We present efﬁcient and optimisable multi-class learned object descriptors together with a novel probabilistic and differential rendering engine, for principled full object shape inference from one or more RGB-D images. Our framework allows for accurate and robust 3D object reconstruction which enables multiple applications including robot grasping and placing, augmented reality, and the ﬁrst object-level SLAM system capable of optimising object poses and shapes jointly with camera trajectory.},
	language = {en},
	urldate = {2022-04-20},
	journal = {arXiv:2004.04485 [cs]},
	author = {Sucar, Edgar and Wada, Kentaro and Davison, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.04485},
	file = {Sucar 等。 - 2020 - NodeSLAM Neural Object Descriptors for Multi-View.pdf:/home/red0orange/Zotero/storage/SEGQWEBE/Sucar 等。 - 2020 - NodeSLAM Neural Object Descriptors for Multi-View.pdf:application/pdf},
}

@article{yang_cubeslam_2019,
	title = {{CubeSLAM}: {Monocular} 3-{D} {Object} {SLAM}},
	volume = {35},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{CubeSLAM}},
	url = {https://ieeexplore.ieee.org/document/8708251/},
	doi = {10.1109/TRO.2019.2909168},
	abstract = {In this paper, we present a method for single image three-dimensional (3-D) cuboid object detection and multiview object simultaneous localization and mapping in both static and dynamic environments, and demonstrate that the two parts can improve each other. First, for single image object detection, we generate high-quality cuboid proposals from two-dimensional (2D) bounding boxes and vanishing points sampling. The proposals are further scored and selected based on the alignment with image edges. Second, multiview bundle adjustment with new object measurements is proposed to jointly optimize poses of cameras, objects, and points. Objects can provide long-range geometric and scale constraints to improve camera pose estimation and reduce monocular drift. Instead of treating dynamic regions as outliers, we utilize object representation and motion model constraints to improve the camera pose estimation. The 3-D detection experiments on SUN RGBD and KITTI show better accuracy and robustness over existing approaches. On the public TUM, KITTI odometry and our own collected datasets, our SLAM method achieves the state-of-the-art monocular camera pose estimation and at the same time, improves the 3-D object detection accuracy.},
	language = {en},
	number = {4},
	urldate = {2022-04-20},
	journal = {IEEE Transactions on Robotics},
	author = {Yang, Shichao and Scherer, Sebastian},
	month = aug,
	year = {2019},
	pages = {925--938},
	file = {Yang 和 Scherer - 2019 - CubeSLAM Monocular 3-D Object SLAM.pdf:/home/red0orange/Zotero/storage/BU7IU6DJ/Yang 和 Scherer - 2019 - CubeSLAM Monocular 3-D Object SLAM.pdf:application/pdf},
}

@article{nicholson_quadricslam_2019,
	title = {{QuadricSLAM}: {Dual} {Quadrics} {From} {Object} {Detections} as {Landmarks} in {Object}-{Oriented} {SLAM}},
	volume = {4},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{QuadricSLAM}},
	url = {https://ieeexplore.ieee.org/document/8440105/},
	doi = {10.1109/LRA.2018.2866205},
	abstract = {In this letter, we use two-dimensional (2-D) object detections from multiple views to simultaneously estimate a 3-D quadric surface for each object and localize the camera position. We derive a simultaneous localization and mapping (SLAM) formulation that uses dual quadrics as 3-D landmark representations, exploiting their ability to compactly represent the size, position and orientation of an object, and show how 2-D object detections can directly constrain the quadric parameters via a novel geometric error formulation. We develop a sensor model for object detectors that addresses the challenge of partially visible objects, and demonstrate how to jointly estimate the camera pose and constrained dual quadric parameters in factor graph based SLAM with a general perspective camera.},
	language = {en},
	number = {1},
	urldate = {2022-04-20},
	journal = {IEEE Robotics and Automation Letters},
	author = {Nicholson, Lachlan and Milford, Michael and Sunderhauf, Niko},
	month = jan,
	year = {2019},
	pages = {1--8},
	file = {Nicholson 等。 - 2019 - QuadricSLAM Dual Quadrics From Object Detections .pdf:/home/red0orange/Zotero/storage/8R6KGRQ9/Nicholson 等。 - 2019 - QuadricSLAM Dual Quadrics From Object Detections .pdf:application/pdf},
}

@article{liao_object-oriented_2020,
	series = {{SO}-{SLAM作者的前一个工作}},
	title = {Object-oriented {SLAM} using {Quadrics} and {Symmetry} {Properties} for {Indoor} {Environments}},
	abstract = {Aiming at the application environment of indoor mobile robots, this paper proposes a sparse object-level SLAM algorithm based on an RGB-D camera. A quadric representation is used as a landmark to compactly model objects, including their position, orientation, and occupied space. The state-of-art quadric-based SLAM algorithm faces the observability problem caused by the limited perspective under the plane trajectory of the mobile robot. To solve the problem, the proposed algorithm fuses both object detection and point cloud data to estimate the quadric parameters. It finishes the quadric initialization based on a single frame of RGB-D data, which significantly reduces the requirements for perspective changes. As objects are often observed locally, the proposed algorithm uses the symmetrical properties of indoor artificial objects to estimate the occluded parts to obtain more accurate quadric parameters. Experiments have shown that compared with the state-of-art algorithm, especially on the forward trajectory of mobile robots, the proposed algorithm significantly improves the accuracy and convergence speed of quadric reconstruction. Finally, we made available an opensource implementation to replicate the experiments.},
	language = {en},
	author = {Liao, Ziwei and Wang, Wei and Qi, Xianyu and Zhang, Xiaoyu and Xue, Lin and Jiao, Jianzhen and Wei, Ran},
	year = {2020},
	pages = {8},
	file = {Liao 等。 - Object-oriented SLAM using Quadrics and Symmetry P.pdf:/home/red0orange/Zotero/storage/52DHL7RZ/Liao 等。 - Object-oriented SLAM using Quadrics and Symmetry P.pdf:application/pdf},
}

@article{liao_so-slam_2022,
	title = {{SO}-{SLAM}: {Semantic} {Object} {SLAM} {With} {Scale} {Proportional} and {Symmetrical} {Texture} {Constraints}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{SO}-{SLAM}},
	url = {https://ieeexplore.ieee.org/document/9705562/},
	doi = {10.1109/LRA.2022.3148465},
	abstract = {Object SLAM introduces the concept of objects into Simultaneous Localization and Mapping (SLAM) and helps understand indoor scenes for mobile robots and object-level interactive applications. The state-of-art object SLAM systems face challenges such as partial observations, occlusions, unobservable problems, limiting the mapping accuracy and robustness. This paper proposes a novel monocular Semantic Object SLAM (SO-SLAM) system that addresses the introduction of object spatial constraints. We explore three representative spatial constraints, including scale proportional constraint, symmetrical texture constraint and plane supporting constraint. Based on these semantic constraints, we propose two new methods - a more robust object initialization method and an orientation fine optimization method. We have verified the performance of the algorithm on the public datasets and an author-recorded mobile robot dataset and achieved a significant improvement on mapping effects. We will release the code here: https://github.com/XunshanMan/SoSLAM.},
	language = {en},
	number = {2},
	urldate = {2022-04-20},
	journal = {IEEE Robotics and Automation Letters},
	author = {Liao, Ziwei and Hu, Yutong and Zhang, Jiadong and Qi, Xianyu and Zhang, Xiaoyu and Wang, Wei},
	month = apr,
	year = {2022},
	pages = {4008--4015},
	file = {Liao 等。 - 2022 - SO-SLAM Semantic Object SLAM With Scale Proportio.pdf:/home/red0orange/Zotero/storage/C5YFA87G/Liao 等。 - 2022 - SO-SLAM Semantic Object SLAM With Scale Proportio.pdf:application/pdf},
}

@article{han_semantic_2021,
	title = {Semantic {Mapping} for {Mobile} {Robots} in {Indoor} {Scenes}: {A} {Survey}},
	volume = {12},
	issn = {2078-2489},
	shorttitle = {Semantic {Mapping} for {Mobile} {Robots} in {Indoor} {Scenes}},
	url = {https://www.mdpi.com/2078-2489/12/2/92},
	doi = {10.3390/info12020092},
	abstract = {Sensing and mapping its surroundings is an essential requirement for a mobile robot. Geometric maps endow robots with the capacity of basic tasks, e.g., navigation. To co-exist with human beings in indoor scenes, the need to attach semantic information to a geometric map, which is called a semantic map, has been realized in the last two decades. A semantic map can help robots to behave in human rules, plan and perform advanced tasks, and communicate with humans on the conceptual level. This survey reviews methods about semantic mapping in indoor scenes. To begin with, we answered the question, what is a semantic map for mobile robots, by its deﬁnitions. After that, we reviewed works about each of the three modules of semantic mapping, i.e., spatial mapping, acquisition of semantic information, and map representation, respectively. Finally, though great progress has been made, there is a long way to implement semantic maps in advanced tasks for robots, thus challenges and potential future directions are discussed before a conclusion at last.},
	language = {en},
	number = {2},
	urldate = {2022-04-19},
	journal = {Information},
	author = {Han, Xiaoning and Li, Shuailong and Wang, Xiaohui and Zhou, Weijia},
	month = feb,
	year = {2021},
	pages = {92},
	file = {Han 等。 - 2021 - Semantic Mapping for Mobile Robots in Indoor Scene.pdf:/home/red0orange/Zotero/storage/PX3QINC5/Han 等。 - 2021 - Semantic Mapping for Mobile Robots in Indoor Scene.pdf:application/pdf},
}

@article{lluvia_active_2021,
	title = {Active {Mapping} and {Robot} {Exploration}: {A} {Survey}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {Active {Mapping} and {Robot} {Exploration}},
	url = {https://www.mdpi.com/1424-8220/21/7/2445},
	doi = {10.3390/s21072445},
	abstract = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data obtained from one or more sensors. In most situations, the robot is driven by a human operator, but some systems are capable of navigating autonomously while mapping, which is called native simultaneous localization and mapping. This strategy focuses on actively calculating the trajectories to explore the environment while building a map with a minimum error. In this paper, a comprehensive review of the research work developed in this ﬁeld is provided, targeting the most relevant contributions in indoor mobile robotics.},
	language = {en},
	number = {7},
	urldate = {2022-04-19},
	journal = {Sensors},
	author = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
	month = apr,
	year = {2021},
	pages = {2445},
	file = {Lluvia 等。 - 2021 - Active Mapping and Robot Exploration A Survey.pdf:/home/red0orange/Zotero/storage/AI9PDHGU/Lluvia 等。 - 2021 - Active Mapping and Robot Exploration A Survey.pdf:application/pdf},
}

@article{wang_efficient_2019,
	title = {Efficient {Autonomous} {Robotic} {Exploration} {With} {Semantic} {Road} {Map} in {Indoor} {Environments}},
	volume = {4},
	issn = {2377-3766, 2377-3774},
	url = {https://ieeexplore.ieee.org/document/8737705/},
	doi = {10.1109/LRA.2019.2923368},
	abstract = {This letter presents a novel and integrated framework for Next-Best-View (NBV) selection toward autonomous robotic exploration in indoor environments. A topological map, named semantic road map (SRM), is proposed to represent the explored environment during the exploration. The basic concept of the SRM is to construct a graph with nodes containing the exploration states and with edges satisfying the collision-free constraints. Especially, the SRM integrates both semantic and structure information of the environment, which possesses the beneﬁcial properties of using a topological map in the exploration. It is worth noting that the proposed SRM is incrementally built along with the exploration process, thereby, avoiding the unnecessary reconsideration of the explored areas when constructing the topological map. Based on the SRM, a novel decision model with semantic information is presented for determining the NBV during the exploration. Moreover, the decision model takes into account both information gain and cost-to-go of a candidate NBV, which can be queried efﬁciently on the SRM, enabling the efﬁcient exploration of the environment. The effectiveness and efﬁciency of the proposed system are assessed and demonstrated using both simulated and real-world indoor experiments.},
	language = {en},
	number = {3},
	urldate = {2022-04-19},
	journal = {IEEE Robotics and Automation Letters},
	author = {Wang, Chaoqun and Zhu, Delong and Li, Teng and Meng, Max Q.-H. and de Silva, Clarence W.},
	month = jul,
	year = {2019},
	pages = {2989--2996},
	file = {Wang 等。 - 2019 - Efficient Autonomous Robotic Exploration With Sema.pdf:/home/red0orange/Zotero/storage/GVMFNLSG/Wang 等。 - 2019 - Efficient Autonomous Robotic Exploration With Sema.pdf:application/pdf},
}

@article{campos_orb-slam3_2021,
	title = {{ORB}-{SLAM3}: {An} {Accurate} {Open}-{Source} {Library} for {Visual}, {Visual}-{Inertial} and {Multi}-{Map} {SLAM}},
	volume = {37},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{ORB}-{SLAM3}},
	url = {http://arxiv.org/abs/2007.11898},
	doi = {10.1109/TRO.2021.3075644},
	abstract = {This paper presents ORB-SLAM3, the ﬁrst system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and ﬁsheye lens models.},
	language = {en},
	number = {6},
	urldate = {2022-04-17},
	journal = {IEEE Transactions on Robotics},
	author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and Montiel, José M. M. and Tardós, Juan D.},
	month = dec,
	year = {2021},
	note = {arXiv: 2007.11898},
	pages = {1874--1890},
	file = {Campos 等。 - 2021 - ORB-SLAM3 An Accurate Open-Source Library for Vis.pdf:/home/red0orange/Zotero/storage/ZXMQBB4F/Campos 等。 - 2021 - ORB-SLAM3 An Accurate Open-Source Library for Vis.pdf:application/pdf},
}

@article{wang_deep_2019,
	title = {Deep learning approach to peripheral leukocyte recognition},
	volume = {14},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0218808},
	doi = {10.1371/journal.pone.0218808},
	language = {en},
	number = {6},
	urldate = {2022-04-15},
	journal = {PLOS ONE},
	author = {Wang, Qiwei and Bi, Shusheng and Sun, Minglei and Wang, Yuliang and Wang, Di and Yang, Shaobao},
	editor = {Zhang, Jie},
	month = jun,
	year = {2019},
	pages = {e0218808},
	file = {Wang 等。 - 2019 - Deep learning approach to peripheral leukocyte rec.pdf:/home/red0orange/Zotero/storage/E2H9VLQM/Wang 等。 - 2019 - Deep learning approach to peripheral leukocyte rec.pdf:application/pdf},
}

@article{reena_deep_2020,
	series = {Deeplabv3+{ALexNet} 白细胞5类 语义分割+分类},
	title = {Deep learning approach to peripheral leukocyte recognition},
	volume = {126},
	issn = {00104825},
	shorttitle = {Localization and recognition of leukocytes in peripheral blood},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482520303656},
	doi = {10.1016/j.compbiomed.2020.104034},
	language = {en},
	urldate = {2021-10-30},
	journal = {Computers in Biology and Medicine},
	author = {Reena, M. Roy and Ameer, P.M.},
	month = nov,
	year = {2020},
	keywords = {双阶段},
	pages = {104034},
	file = {2020-Localization and recognition of leukocytes in peripheral blood.pdf:/home/red0orange/Zotero/storage/GFSKC224/2020-Localization and recognition of leukocytes in peripheral blood.pdf:application/pdf},
}

@article{he_incremental_2018,
	title = {Incremental {3D} {Line} {Segment} {Extraction} from {Semi}-dense {SLAM}},
	url = {http://arxiv.org/abs/1708.03275},
	abstract = {Although semi-dense Simultaneous Localization and Mapping (SLAM) has been becoming more popular over the last few years, there is a lack of efﬁcient methods for representing and processing their large scale point clouds. In this paper, we propose using 3D line segments to simplify the point clouds generated by semi-dense SLAM. Speciﬁcally, we present a novel incremental approach for 3D line segment extraction. This approach reduces a 3D line segment ﬁtting problem into two 2D line segment ﬁtting problems and takes advantage of both images and depth maps. In our method, 3D line segments are ﬁtted incrementally along detected edge segments via minimizing ﬁtting errors on two planes. By clustering the detected line segments, the resulting 3D representation of the scene achieves a good balance between compactness and completeness. Our experimental results show that the 3D line segments generated by our method are highly accurate. As an application, we demonstrate that these line segments greatly improve the quality of 3D surface reconstruction compared to a feature point based baseline.},
	language = {en},
	urldate = {2022-04-15},
	journal = {arXiv:1708.03275 [cs]},
	author = {He, Shida and Qin, Xuebin and Zhang, Zichen and Jagersand, Martin},
	month = apr,
	year = {2018},
	note = {arXiv: 1708.03275},
	file = {He 等。 - 2018 - Incremental 3D Line Segment Extraction from Semi-d.pdf:/home/red0orange/Zotero/storage/ZLEGMWBC/He 等。 - 2018 - Incremental 3D Line Segment Extraction from Semi-d.pdf:application/pdf},
}

@article{_fisherslam_2021,
	title = {{基于Fisher信息的室内主动SLAM}},
	issn = {1002-8331},
	abstract = {针对传统的视觉同步定位与地图创建(Visual Simultaneous Localization and Mapping， VSLAM)算法在室内弱纹理场景中容易因为特征缺失而定位失败的问题，提出了一种基于最大Fisher信息量云台控制的主动SLAM算法。该方法在经典的ORB-SLAM2框架上进行扩展，增加了Fisher信息场构建模块与云台控制模块。首先，在视觉跟踪的同时，将三维空间划分成若干个体素，根据特征点的空间位置分布更新每个体素的Fisher信息，完成Fisher信息场的构建；其次，当相机获取的图像遇到特征缺失的情况，先找到离相机光心欧式距离最近的体素，以该体素Fisher信息量最大的方向作为相机此时的最优观测方向；最后，计算出坐标变换后相机的偏转角度，通过机载云台实现相机转动到最优观测方向，重新获得场景特征，使得算法在丢失特征之后能够实现自主重定位。将改进后的算法运用到四旋翼无人机仿真平台，结果表明在传统算法失效的情况下，所提算法仍能实时准确地估计无人机位姿，提高了系统的鲁棒性。},
	language = {中文},
	journal = {计算机工程与应用},
	author = {席永辉;胡士强;},
	year = {2021},
	pages = {1--10},
	file = {基于Fisher信息的室内主动SLAM_席永辉.pdf:/home/red0orange/Zotero/storage/43EZNJSS/基于Fisher信息的室内主动SLAM_席永辉.pdf:application/pdf},
}

@article{_plp-slamslam_2017,
	title = {{PLP}-{SLAM}:基于点、线、面特征融合的视觉{SLAM方法}},
	issn = {1002-0446},
	doi = {10.13973/j.cnki.robot.2017.0214},
	abstract = {基于点特征的视觉SLAM(同时定位与地图构建)算法存在计算量大、环境存储空间负荷高、定位误差较大的问题,为此,提出了一种基于点、线段、平面特征融合的视觉SLAM算法——PLP-SLAM.在扩展卡尔曼滤波(EKF)框架下,首先利用点特征估计机器人当前位姿,然后构建了基于点、线、平面特征的观测模型,最后建立了带平面约束的线段特征数据关联方法及系统状态更新模型,并利用线段和平面特征描述环境信息.在公开数据集上进行了实验,结果表明,本文PLP-SLAM算法能够成功完成SLAM任务,平均定位误差为2.3 m,优于基于点特征的SLAM方法,并通过基于不同特征的SLAM实验表明了本文提出的点、线、面特征融合的优越性.},
	language = {中文;},
	number = {02 vo 39},
	journal = {机器人},
	author = {李海丰;胡遵河;陈新伟;},
	year = {2017},
	keywords = {点线面特征融合, 扩展卡尔曼滤波, 平面特征 simultaneous localization and mapping, 同时定位与地图构建, 线段特征, extended Kalman filter, line segment feature, plane feature, point-line-plane feature fusion},
	pages = {214--220+229},
	file = {李海丰\;胡遵河\;陈新伟\; - 2017 - PLP-SLAM基于点、线、面特征融合的视觉SLAM方法.pdf:/home/red0orange/Zotero/storage/XNXBS5PU/李海丰\;胡遵河\;陈新伟\; - 2017 - PLP-SLAM基于点、线、面特征融合的视觉SLAM方法.pdf:application/pdf},
}

@article{刘光伟2020室内移动机器人主动,
	title = {室内移动机器人主动 {SLAM} 技术研究},
	volume = {3},
	journal = {机械设计与制造},
	author = {{刘光伟} and {王巍} and {祁贤雨} and {李明博}},
	year = {2020},
	pages = {246--249},
	file = {室内移动机器人主动SLAM技术研究_刘光伟.pdf:/home/red0orange/Zotero/storage/6B44JTVZ/室内移动机器人主动SLAM技术研究_刘光伟.pdf:application/pdf},
}

@inproceedings{trivun_active_2015,
	address = {Seville},
	title = {Active {SLAM}-based algorithm for autonomous exploration with mobile robot},
	isbn = {978-1-4799-7800-7},
	url = {https://ieeexplore.ieee.org/document/7125079},
	doi = {10.1109/ICIT.2015.7125079},
	abstract = {In this paper, we present an algorithm for fully autonomous exploration and mapping of an unknown indoor robot environment. This algorithm is based on the active SLAM (simultaneous localization and mapping) approach. The mobile robot equipped with laser sensor builds a map of an environment, while keeping track of its current location. Autonomy is introduced to this system by automatically setting goal points so that either previously unknown space is mapped, or known landmarks are revisited in order to increase map accuracy. Final aim is to maximize both map coverage and accuracy. The proposed procedure is experimentally veriﬁed on Pioneer 3-DX mobile robot in real environment, using ROS framework for implementation.},
	language = {en},
	urldate = {2022-04-11},
	booktitle = {2015 {IEEE} {International} {Conference} on {Industrial} {Technology} ({ICIT})},
	publisher = {IEEE},
	author = {Trivun, Darko and Salaka, Edin and Osmankovic, Dinko and Velagic, Jasmin and Osmic, Nedim},
	month = mar,
	year = {2015},
	pages = {74--79},
	file = {Trivun 等。 - 2015 - Active SLAM-based algorithm for autonomous explora.pdf:/home/red0orange/Zotero/storage/5AYHSHNH/Trivun 等。 - 2015 - Active SLAM-based algorithm for autonomous explora.pdf:application/pdf},
}

@article{cadena_past_2016,
	series = {2016 {SLAM综述}，有一章提到{Active} {SLAM}},
	title = {Past, {Present}, and {Future} of {Simultaneous} {Localization} and {Mapping}: {Toward} the {Robust}-{Perception} {Age}},
	volume = {32},
	issn = {1552-3098, 1941-0468},
	shorttitle = {Past, {Present}, and {Future} of {Simultaneous} {Localization} and {Mapping}},
	url = {http://ieeexplore.ieee.org/document/7747236/},
	doi = {10.1109/TRO.2016.2624754},
	language = {en},
	number = {6},
	urldate = {2022-04-11},
	journal = {IEEE Transactions on Robotics},
	author = {Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jose and Reid, Ian and Leonard, John J.},
	month = dec,
	year = {2016},
	pages = {1309--1332},
	file = {Cadena 等。 - 2016 - Past, Present, and Future of Simultaneous Localiza.pdf:/home/red0orange/Zotero/storage/UWLAY4DB/Cadena 等。 - 2016 - Past, Present, and Future of Simultaneous Localiza.pdf:application/pdf},
}

@article{yang_cubeslam_nodate,
	title = {{CubeSLAM}: {Monocular} {3D} {Object} {Detection} and {SLAM} without {Prior} {Models}},
	abstract = {We present a method for single image 3D cuboid object detection and multi-view object SLAM without prior object model, and demonstrate that the two aspects can beneﬁt each other. For 3D detection, we generate high quality cuboid proposals from 2D bounding boxes and vanishing points sampling. The proposals are further scored and selected to align with image edges. Experiments on SUN RGBD and KITTI shows the eﬃciency and accuracy over existing approaches. Then in the second part, multi-view bundle adjustment with novel measurement functions is proposed to jointly optimize camera poses, objects and points, utilizing single view detection results. Objects can provide more geometric constraints and scale consistency compared to points. On the collected and public TUM and KITTI odometry datasets, we achieve better pose estimation accuracy over the state-of-the-art monocular SLAM while also improve the 3D object detection accuracy at the same time.},
	language = {en},
	author = {Yang, Shichao and Scherer, Sebastian},
	keywords = {好文},
	pages = {17},
	file = {Yang 和 Scherer - CubeSLAM Monocular 3D Object Detection and SLAM w.pdf:/home/red0orange/Zotero/storage/2SSRQJ8U/Yang 和 Scherer - CubeSLAM Monocular 3D Object Detection and SLAM w.pdf:application/pdf},
}

@article{wu_eao-slam_2020,
	series = {{ASLAM} + {Semantic} 前身},
	title = {{EAO}-{SLAM}: {Monocular} {Semi}-{Dense} {Object} {SLAM} {Based} on {Ensemble} {Data} {Association}},
	shorttitle = {{EAO}-{SLAM}},
	url = {http://arxiv.org/abs/2004.12730},
	doi = {10.1109/IROS45743.2020.9341757},
	abstract = {Object-level data association and pose estimation play a fundamental role in semantic SLAM, which remain unsolved due to the lack of robust and accurate algorithms. In this work, we propose an ensemble data associate strategy for integrating the parametric and nonparametric statistic tests. By exploiting the nature of different statistics, our method can effectively aggregate the information of different measurements, and thus signiﬁcantly improve the robustness and accuracy of data association. We then present an accurate object pose estimation framework, in which an outliers-robust centroid and scale estimation algorithm and an object pose initialization algorithm are developed to help improve the optimality of pose estimation results. Furthermore, we build a SLAM system that can generate semi-dense or lightweight object-oriented maps with a monocular camera. Extensive experiments are conducted on three publicly available datasets and a real scenario. The results show that our approach signiﬁcantly outperforms stateof-the-art techniques in accuracy and robustness. The source code is available on https://github.com/yanmin-wu/ EAO-SLAM.},
	language = {en},
	urldate = {2022-04-11},
	journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Wu, Yanmin and Zhang, Yunzhou and Zhu, Delong and Feng, Yonghui and Coleman, Sonya and Kerr, Dermot},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.12730},
	pages = {4966--4973},
	file = {Wu 等。 - 2020 - EAO-SLAM Monocular Semi-Dense Object SLAM Based o.pdf:/home/red0orange/Zotero/storage/FEF4AWJU/Wu 等。 - 2020 - EAO-SLAM Monocular Semi-Dense Object SLAM Based o.pdf:application/pdf},
}

@article{wu_object_2021,
	series = {{ASLAM} + {Semantic}},
	title = {Object {SLAM}-{Based} {Active} {Mapping} and {Robotic} {Grasping}},
	url = {http://arxiv.org/abs/2012.01788},
	doi = {10.1109/3DV53792.2021.00144},
	abstract = {This paper presents the ﬁrst active object mapping framework for complex robotic manipulation and autonomous perception tasks. The framework is built on an object SLAM system integrated with a simultaneous multiobject pose estimation process that is optimized for robotic grasping. Aiming to reduce the observation uncertainty on target objects and increase their pose estimation accuracy, we also design an object-driven exploration strategy to guide the object mapping process, enabling autonomous mapping and high-level perception. Combining the mapping module and the exploration strategy, an accurate object map that is compatible with robotic grasping can be generated. Additionally, quantitative evaluations also indicate that the proposed framework has a very high mapping accuracy. Experiments with manipulation (including object grasping and placement) and augmented reality signiﬁcantly demonstrate the effectiveness and advantages of our proposed framework.},
	language = {en},
	urldate = {2022-04-11},
	journal = {2021 International Conference on 3D Vision (3DV)},
	author = {Wu, Yanmin and Zhang, Yunzhou and Zhu, Delong and Chen, Xin and Coleman, Sonya and Sun, Wenkai and Hu, Xinggang and Deng, Zhiqiang},
	month = dec,
	year = {2021},
	note = {arXiv: 2012.01788},
	pages = {1372--1381},
	file = {Wu 等。 - 2021 - Object SLAM-Based Active Mapping and Robotic Grasp.pdf:/home/red0orange/Zotero/storage/Y6GRFUS8/Wu 等。 - 2021 - Object SLAM-Based Active Mapping and Robotic Grasp.pdf:application/pdf},
}

@article{zeng_semantic_2018,
	series = {{SLAM} {Help} {Semantic}},
	title = {Semantic {Mapping} with {Simultaneous} {Object} {Detection} and {Localization}},
	url = {http://arxiv.org/abs/1810.11525},
	abstract = {We present a ﬁltering-based method for semantic mapping to simultaneously detect objects and localize their 6 degree-of-freedom pose. For our method, called Contextual Temporal Mapping (or CT-Map), we represent the semantic map as a belief over object classes and poses across an observed scene. Inference for the semantic mapping problem is then modeled in the form of a Conditional Random Field (CRF). CT-Map is a CRF that considers two forms of relationship potentials to account for contextual relations between objects and temporal consistency of object poses, as well as a measurement potential on observations. A particle ﬁltering algorithm is then proposed to perform inference in the CT-Map model. We demonstrate the efﬁcacy of the CT-Map method with a Michigan Progress Fetch robot equipped with a RGB-D sensor. Our results demonstrate that the particle ﬁltering based inference of CT-Map provides improved object detection and pose estimation with respect to baseline methods that treat observations as independent samples of a scene.},
	language = {en},
	urldate = {2022-04-11},
	journal = {arXiv:1810.11525 [cs]},
	author = {Zeng, Zhen and Zhou, Yunwen and Jenkins, Odest Chadwicke and Desingh, Karthik},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.11525},
	file = {Zeng 等。 - 2018 - Semantic Mapping with Simultaneous Object Detectio.pdf:/home/red0orange/Zotero/storage/RV25LESR/Zeng 等。 - 2018 - Semantic Mapping with Simultaneous Object Detectio.pdf:application/pdf},
}

@article{lluvia_active_2021-1,
	title = {Active {Mapping} and {Robot} {Exploration}: {A} {Survey}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {Active {Mapping} and {Robot} {Exploration}},
	url = {https://www.mdpi.com/1424-8220/21/7/2445},
	doi = {10.3390/s21072445},
	abstract = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data obtained from one or more sensors. In most situations, the robot is driven by a human operator, but some systems are capable of navigating autonomously while mapping, which is called native simultaneous localization and mapping. This strategy focuses on actively calculating the trajectories to explore the environment while building a map with a minimum error. In this paper, a comprehensive review of the research work developed in this ﬁeld is provided, targeting the most relevant contributions in indoor mobile robotics.},
	language = {en},
	number = {7},
	urldate = {2022-04-09},
	journal = {Sensors},
	author = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
	month = apr,
	year = {2021},
	pages = {2445},
	file = {Lluvia 等。 - 2021 - Active Mapping and Robot Exploration A Survey.pdf:/home/red0orange/Zotero/storage/BKGNJKUF/Lluvia 等。 - 2021 - Active Mapping and Robot Exploration A Survey.pdf:application/pdf},
}

@inproceedings{rambach_slamcraft_2019,
	address = {Tokyo, Japan},
	title = {{SlamCraft}: {Dense} {Planar} {RGB} {Monocular} {SLAM}},
	isbn = {978-4-901122-18-4},
	shorttitle = {{SlamCraft}},
	url = {https://ieeexplore.ieee.org/document/8757982/},
	doi = {10.23919/MVA.2019.8757982},
	abstract = {Monocular Simultaneous Localization and Mapping (SLAM) approaches have progressed signiﬁcantly over the last two decades. However, keypoint-based approaches only provide limited structural information in a 3D point cloud which does not fulﬁl the requirements of applications such as Augmented Reality (AR). SLAM systems that provide dense environment maps are either computationally intensive or require depth information from additional sensors. In this paper, we use a deep neural network that estimates planar regions from RGB input images and fuses its output iteratively with the point cloud map of a SLAM system to create an eﬃcient monocular planar SLAM system. We present qualitative results of the created maps, as well as an evaluation of the tracking accuracy and runtime of our approach.},
	language = {en},
	urldate = {2022-04-07},
	booktitle = {2019 16th {International} {Conference} on {Machine} {Vision} {Applications} ({MVA})},
	publisher = {IEEE},
	author = {Rambach, Jason and Lesur, Paul and Pagani, Alain and Stricker, Didier},
	month = may,
	year = {2019},
	pages = {1--6},
	file = {Rambach 等。 - 2019 - SlamCraft Dense Planar RGB Monocular SLAM.pdf:/home/red0orange/Zotero/storage/R27NEJU3/Rambach 等。 - 2019 - SlamCraft Dense Planar RGB Monocular SLAM.pdf:application/pdf},
}

@article{xie_planerecnet_2022,
	title = {{PlaneRecNet}: {Multi}-{Task} {Learning} with {Cross}-{Task} {Consistency} for {Piece}-{Wise} {Plane} {Detection} and {Reconstruction} from a {Single} {RGB} {Image}},
	shorttitle = {{PlaneRecNet}},
	url = {http://arxiv.org/abs/2110.11219},
	abstract = {Piece-wise 3D planar reconstruction provides holistic scene understanding of man-made environments, especially for indoor scenarios. Most recent approaches focused on improving the segmentation and reconstruction results by introducing advanced network architectures but overlooked the dual characteristics of piece-wise planes as objects and geometric models. Different from other existing approaches, we start from enforcing cross-task consistency for our multi-task convolutional neural network, PlaneRecNet, which integrates a single-stage instance segmentation network for piece-wise planar segmentation and a depth decoder to reconstruct the scene from a single RGB image. To achieve this, we introduce several novel loss functions (geometric constraint) that jointly improve the accuracy of piece-wise planar segmentation and depth estimation. Meanwhile, a novel Plane Prior Attention module is used to guide depth estimation with the awareness of plane instances. Exhaustive experiments are conducted in this work to validate the effectiveness and efficiency of our method.},
	language = {en},
	urldate = {2022-04-07},
	journal = {arXiv:2110.11219 [cs]},
	author = {Xie, Yaxu and Shu, Fangwen and Rambach, Jason and Pagani, Alain and Stricker, Didier},
	month = jan,
	year = {2022},
	note = {arXiv: 2110.11219},
	file = {Xie 等。 - 2022 - PlaneRecNet Multi-Task Learning with Cross-Task C.pdf:/home/red0orange/Zotero/storage/ZURUVPK5/Xie 等。 - 2022 - PlaneRecNet Multi-Task Learning with Cross-Task C.pdf:application/pdf},
}

@inproceedings{pumarola_pl-slam_2017,
	address = {Singapore, Singapore},
	series = {{首个单目PL}-{SLAM}},
	title = {{PL}-{SLAM}: {Real}-time monocular visual {SLAM} with points and lines},
	isbn = {978-1-5090-4633-1},
	shorttitle = {{PL}-{SLAM}},
	url = {http://ieeexplore.ieee.org/document/7989522/},
	doi = {10.1109/ICRA.2017.7989522},
	abstract = {Low textured scenes are well known to be one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate line-based geometric primitives, for instance in city and indoor scenes, or in the so-called “Manhattan worlds”, where structured edges are predominant. In this paper we propose a solution to handle these situations. Speciﬁcally, we build upon ORBSLAM, presumably the current state-of-the-art solution both in terms of accuracy as efﬁciency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efﬁciency.},
	language = {en},
	urldate = {2022-04-05},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Pumarola, Albert and Vakhitov, Alexander and Agudo, Antonio and Sanfeliu, Alberto and Moreno-Noguer, Francese},
	month = may,
	year = {2017},
	keywords = {ICRA},
	pages = {4503--4508},
	file = {Pumarola 等。 - 2017 - PL-SLAM Real-time monocular visual SLAM with poin.pdf:/home/red0orange/Zotero/storage/9TPYQ46S/Pumarola 等。 - 2017 - PL-SLAM Real-time monocular visual SLAM with poin.pdf:application/pdf},
}

@inproceedings{yang_visual-inertial_2019,
	address = {Macau, China},
	series = {{谢晓佳后续结合IMU}},
	title = {Visual-{Inertial} {Odometry} with {Point} and {Line} {Features}},
	isbn = {978-1-72814-004-9},
	url = {https://ieeexplore.ieee.org/document/8967905/},
	doi = {10.1109/IROS40897.2019.8967905},
	abstract = {In this paper, we present a tightly-coupled monocular visual-inertial navigation system (VINS) using points and lines with degenerate motion analysis for 3D line triangulation. Based on line segment measurements from images, we propose two sliding window based 3D line triangulation algorithms and compare their performance. Analysis of the proposed algorithms reveals 3 degenerate camera motions that cause triangulation failures. Both geometrical interpretation and MonteCarlo simulations are provided to verify these degenerate motions which prevent triangulation. In addition, commonly used line representations are compared through a monocular visual SLAM Monte-Carlo simulation. Finally, real-world experiments are conducted to validate the implementation of the proposed VINS system using the “closest point” line representation.},
	language = {en},
	urldate = {2022-04-05},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Yang, Yulin and Geneva, Patrick and Eckenhoff, Kevin and Huang, Guoquan},
	month = nov,
	year = {2019},
	keywords = {IROS},
	pages = {2447--2454},
	file = {Yang 等。 - 2019 - Visual-Inertial Odometry with Point and Line Featu.pdf:/home/red0orange/Zotero/storage/WJU9LYVH/Yang 等。 - 2019 - Visual-Inertial Odometry with Point and Line Featu.pdf:application/pdf},
}

@article{li_structure-slam_2020,
	series = {单目 版本的 结构{SLAM}，结构{SLAM包含点线面特征}，是基于{PL}-{SLAM的进一步工作}},
	title = {Structure-{SLAM}: {Low}-{Drift} {Monocular} {SLAM} in {Indoor} {Environments}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Structure-{SLAM}},
	url = {https://ieeexplore.ieee.org/document/9165014/},
	doi = {10.1109/LRA.2020.3015456},
	abstract = {In this paper a low-drift monocular SLAM method is proposed targeting indoor scenarios, where monocular SLAM often fails due to the lack of textured surfaces. Our approach decouples rotation and translation estimation of the tracking process to reduce the long-term drift in indoor environments. In order to take full advantage of the available geometric information in the scene, surface normals are predicted by a convolutional neural network from each input RGB image in real-time. First, a drift-free rotation is estimated based on lines and surface normals using spherical mean-shift clustering, leveraging the weak Manhattan World assumption. Then translation is computed from point and line features. Finally, the estimated poses are reﬁned with a map-toframe optimization strategy. The proposed method outperforms the state of the art on common SLAM benchmarks such as ICL-NUIM and TUM RGB-D.},
	language = {en},
	number = {4},
	urldate = {2022-04-05},
	journal = {IEEE Robotics and Automation Letters},
	author = {Li, Yanyan and Brasch, Nikolas and Wang, Yida and Navab, Nassir and Tombari, Federico},
	month = oct,
	year = {2020},
	keywords = {IROS},
	pages = {6583--6590},
	file = {Li 等。 - 2020 - Structure-SLAM Low-Drift Monocular SLAM in Indoor.pdf:/home/red0orange/Zotero/storage/NXH6KRC7/Li 等。 - 2020 - Structure-SLAM Low-Drift Monocular SLAM in Indoor.pdf:application/pdf},
}

@article{xie_planesegnet_2021,
	title = {{PlaneSegNet}: {Fast} and {Robust} {Plane} {Estimation} {Using} a {Single}-stage {Instance} {Segmentation} {CNN}},
	shorttitle = {{PlaneSegNet}},
	url = {http://arxiv.org/abs/2103.15428},
	abstract = {Instance segmentation of planar regions in indoor scenes beneﬁts visual SLAM and other applications such as augmented reality (AR) where scene understanding is required. Existing methods built upon two-stage frameworks show satisfactory accuracy but are limited by low frame rates. In this work, we propose a real-time deep neural architecture that estimates piece-wise planar regions from a single RGB image. Our model employs a variant of a fast single-stage CNN architecture to segment plane instances. Considering the particularity of the target detected, we propose Fast Feature Nonmaximum Suppression (FF-NMS) to reduce the suppression errors resulted from overlapping bounding boxes of planes. We also utilize a Residual Feature Augmentation module in the Feature Pyramid Network (FPN) . Our method achieves signiﬁcantly higher frame-rates and comparable segmentation accuracy against two-stage methods. We automatically label over 70,000 images as ground truth from the Stanford 2D-3DSemantics dataset. Moreover, we incorporate our method with a state-of-the-art planar SLAM and validate its beneﬁts.},
	language = {en},
	urldate = {2022-04-07},
	journal = {arXiv:2103.15428 [cs]},
	author = {Xie, Yaxu and Rambach, Jason and Shu, Fangwen and Stricker, Didier},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.15428},
	keywords = {ICRA},
	file = {Xie 等。 - 2021 - PlaneSegNet Fast and Robust Plane Estimation Usin.pdf:/home/red0orange/Zotero/storage/5VT9EJZD/Xie 等。 - 2021 - PlaneSegNet Fast and Robust Plane Estimation Usin.pdf:application/pdf},
}

@article{zhang_point-plane_2019,
	series = {{PS}-{SLAM} 结合了{plane的SLAM}，实现了对低纹理场景的高鲁棒性},
	title = {Point-{Plane} {SLAM} {Using} {Supposed} {Planes} for {Indoor} {Environments}},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/17/3795},
	doi = {10.3390/s19173795},
	abstract = {Simultaneous localization and mapping (SLAM) is a fundamental problem for various applications. For indoor environments, planes are predominant features that are less affected by measurement noise. In this paper, we propose a novel point-plane SLAM system using RGB-D cameras. First, we extract feature points from RGB images and planes from depth images. Then plane correspondences in the global map can be found using their contours. Considering the limited size of real planes, we exploit constraints of plane edges. In general, a plane edge is an intersecting line of two perpendicular planes. Therefore, instead of line-based constraints, we calculate and generate supposed perpendicular planes from edge lines, resulting in more plane observations and constraints to reduce estimation errors. To exploit the orthogonal structure in indoor environments, we also add structural (parallel or perpendicular) constraints of planes. Finally, we construct a factor graph using all of these features. The cost functions are minimized to estimate camera poses and global map. We test our proposed system on public RGB-D benchmarks, demonstrating its robust and accurate pose estimation results, compared with other state-of-the-art SLAM systems.},
	language = {en},
	number = {17},
	urldate = {2022-04-05},
	journal = {Sensors},
	author = {Zhang, Xiaoyu and Wang, Wei and Qi, Xianyu and Liao, Ziwei and Wei, Ran},
	month = sep,
	year = {2019},
	pages = {3795},
	file = {Zhang 等。 - 2019 - Point-Plane SLAM Using Supposed Planes for Indoor .pdf:/home/red0orange/Zotero/storage/AI3NUE3U/Zhang 等。 - 2019 - Point-Plane SLAM Using Supposed Planes for Indoor .pdf:application/pdf},
}

@article{li_rgb-d_2021,
	series = {{RGB} 版本的 结构{SLAM}},
	title = {{RGB}-{D} {SLAM} with {Structural} {Regularities}},
	url = {http://arxiv.org/abs/2010.07997},
	abstract = {This work proposes a RGB-D SLAM system speciﬁcally designed for structured environments and aimed at improved tracking and mapping accuracy by relying on geometric features that are extracted from the surrounding. Structured environments offer, in addition to points, also an abundance of geometrical features such as lines and planes, which we exploit to design both the tracking and mapping components of our SLAM system. For the tracking part, we explore geometric relationships between these features based on the assumption of a Manhattan World (MW). We propose a decoupling-reﬁnement method based on points, lines, and planes, as well as the use of Manhattan relationships in an additional pose reﬁnement module. For the mapping part, different levels of maps from sparse to dense are reconstructed at a low computational cost. We propose an instance-wise meshing strategy to build a dense map by meshing plane instances independently. The overall performance in terms of pose estimation and reconstruction is evaluated on public benchmarks and shows improved performance compared to state-of-the-art methods. The code is released at https:// github.com/yanyan-li/PlanarSLAM.},
	language = {en},
	urldate = {2022-04-05},
	journal = {arXiv:2010.07997 [cs]},
	author = {Li, Yanyan and Yunus, Raza and Brasch, Nikolas and Navab, Nassir and Tombari, Federico},
	month = mar,
	year = {2021},
	note = {arXiv: 2010.07997},
	file = {Li 等。 - 2021 - RGB-D SLAM with Structural Regularities.pdf:/home/red0orange/Zotero/storage/E8IB2R99/Li 等。 - 2021 - RGB-D SLAM with Structural Regularities.pdf:application/pdf},
}

@article{fu_pl-vins_2020,
	series = {{PL}+{VINS} 首个实现实时的，张老师指导},
	title = {{PL}-{VINS}: {Real}-{Time} {Monocular} {Visual}-{Inertial} {SLAM} with {Point} and {Line} {Features}},
	shorttitle = {{PL}-{VINS}},
	url = {http://arxiv.org/abs/2009.07462},
	abstract = {Leveraging line features to improve localization accuracy of point-based visual-inertial SLAM (VINS) is gaining interest as they provide additional constraints on scene structure. However, real-time performance when incorporating line features in VINS has not been addressed. This paper presents PL-VINS, a real-time optimization-based monocular VINS method with point and line features, developed based on the state-of-the-art point-based VINS-Mono [1]. We observe that current works use the LSD [2] algorithm to extract line features; however, LSD is designed for scene shape representation instead of the pose estimation problem, which becomes the bottleneck for the real-time performance due to its high computational cost. In this paper, a modiﬁed LSD algorithm is presented by studying a hidden parameter tuning and length rejection strategy. The modiﬁed LSD can run at least three times as fast as LSD. Further, by representing space lines with the Plu¨ cker coordinates, the residual error in line estimation is modeled in terms of the point-to-line distance, which is then minimized by iteratively updating the minimum four-parameter orthonormal representation of the Plu¨ cker coordinates. Experiments in a public benchmark dataset show that the localization error of our method is 12-16\% less than that of VINS-Mono at the same pose update frequency. The source code of our method is available at: https://github.com/cnqiangfu/PL-VINS.},
	language = {en},
	urldate = {2022-04-05},
	journal = {arXiv:2009.07462 [cs]},
	author = {Fu, Qiang and Wang, Jialong and Yu, Hongshan and Ali, Islam and Guo, Feng and He, Yijia and Zhang, Hong},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.07462},
	file = {Fu 等。 - 2020 - PL-VINS Real-Time Monocular Visual-Inertial SLAM .pdf:/home/red0orange/Zotero/storage/WMGYMNGQ/Fu 等。 - 2020 - PL-VINS Real-Time Monocular Visual-Inertial SLAM .pdf:application/pdf},
}

@article{gomez-ojeda_pl-slam_2017,
	series = {{首个双目PL}-{SLAM}，{PL}-{SVO的后续}},
	title = {{PL}-{SLAM}: a {Stereo} {SLAM} {System} through the {Combination} of {Points} and {Line} {Segments}},
	abstract = {Traditional approaches to stereo visual SLAM rely on point features to estimate the camera trajectory and build a map of the environment. In lowtextured environments, though , it is often diﬃcult to ﬁnd a suﬃcient number of reliable point features and, as a consequence, the performance of such algorithms degrades. This paper proposes PL-SLAM, a stereo visual SLAM system that combines both points and line segments to work robustly in a wider variety of scenarios, particularly in those where point features are scarce or not well-distributed in the image. PLSLAM leverages both points and segments at all the instances of the process: visual odometry, keyframe selection, bundle adjustment, etc. We contribute also with a loop closure procedure through a novel bag-ofwords approach that exploits the combined descriptive power of the two kinds of features. Additionally, the resulting map is richer and more diverse in 3D elements, which can be exploited to infer valuable, highlevel scene structures like planes, empty spaces, ground plane, etc. (not addressed in this work). Our proposal has been tested with several popular datasets (such as KITTI and EuRoC), and is compared to state of the art methods like ORB-SLAM, revealing superior performance in most of the experiments, while still running in real-time. An open source version of the PL-SLAM C++ code will be released for the beneﬁt of the community.},
	language = {en},
	author = {Gomez-Ojeda, Ruben and Moreno, Francisco-Angel and Scaramuzza, Davide and Gonzalez-Jimenez, Javier},
	year = {2017},
	pages = {12},
	file = {Gomez-Ojeda 等。 - PL-SLAM a Stereo SLAM System through the Combinat.pdf:/home/red0orange/Zotero/storage/UWMSV7G5/Gomez-Ojeda 等。 - PL-SLAM a Stereo SLAM System through the Combinat.pdf:application/pdf},
}

@article{zuo_robust_2017,
	series = {谢晓佳，对应有中文硕士论文},
	title = {Robust {Visual} {SLAM} with {Point} and {Line} {Features}},
	url = {http://arxiv.org/abs/1711.08654},
	abstract = {In this paper, we develop a robust efﬁcient visual SLAM system that utilizes heterogeneous point and line features. By leveraging ORB-SLAM [1], the proposed system consists of stereo matching, frame tracking, local mapping, loop detection, and bundle adjustment of both point and line features. In particular, as the main theoretical contributions of this paper, we, for the ﬁrst time, employ the orthonormal representation as the minimal parameterization to model line features along with point features in visual SLAM and analytically derive the Jacobians of the re-projection errors with respect to the line parameters, which signiﬁcantly improves the SLAM solution. The proposed SLAM has been extensively tested in both synthetic and real-world experiments whose results demonstrate that the proposed system outperforms the state-of-the-art methods in various scenarios.},
	language = {en},
	urldate = {2022-04-05},
	journal = {arXiv:1711.08654 [cs]},
	author = {Zuo, Xingxing and Xie, Xiaojia and Liu, Yong and Huang, Guoquan},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.08654},
	file = {Zuo 等。 - 2017 - Robust Visual SLAM with Point and Line Features.pdf:/home/red0orange/Zotero/storage/KLLZ8K5H/Zuo 等。 - 2017 - Robust Visual SLAM with Point and Line Features.pdf:application/pdf},
}

@inproceedings{wei_real-time_2019,
	address = {Shanghai, China},
	series = {{略微优于原来的单目PL}-{SLAM}，实验对比了},
	title = {Real-{Time} {Monocular} {Visual} {SLAM} by {Combining} {Points} and {Lines}},
	isbn = {978-1-5386-9552-4},
	url = {https://ieeexplore.ieee.org/document/8784968/},
	doi = {10.1109/ICME.2019.00026},
	abstract = {This paper presents a real-time monocular SLAM algorithm which combines points and line segments. We extend traditional point-based SLAM system with line features which are usually abundant in man-made scenes. The system is more robust and accurate than traditional point-based and directbased monocular SLAM algorithms. In order to improve the timeliness of multi-feature based SLAM, we propose a novel feature level parallel processing framework and a fast line matching algorithm. For improving the reconstruction accuracy of 3D line segments which is usually affected by unreliable line endpoints, a sample point-based 3D reconstruction algorithm for line segments is proposed. Our system is implemented based on a popular monocular SLAM known as ORB-SLAM and tested on the TUM RGB-D benchmark. The experiment results demonstrate that the proposed system performs better than current state-of-the-art visual SLAM with respect to accuracy.},
	language = {en},
	urldate = {2022-04-05},
	booktitle = {2019 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Wei, Xinyu and Huang, Jun and Ma, Xiaoyuan},
	month = jul,
	year = {2019},
	pages = {103--108},
	file = {Wei 等。 - 2019 - Real-Time Monocular Visual SLAM by Combining Point.pdf:/home/red0orange/Zotero/storage/MYXM2P7V/Wei 等。 - 2019 - Real-Time Monocular Visual SLAM by Combining Point.pdf:application/pdf},
}

@inproceedings{yang_improved_2021,
	address = {Macau China},
	title = {An {Improved} {Monocular} {PL}-{SlAM} {Method} with {Point}-{Line} {Feature} {Fusion} under {Low}-{Texture} {Environment}},
	isbn = {978-1-4503-9047-7},
	url = {https://dl.acm.org/doi/10.1145/3484274.3484293},
	doi = {10.1145/3484274.3484293},
	abstract = {Traditional visual SLAM only relies on the point features in the scene to complete positioning and mapping. When the texture information in the scene is missing, it affects the accuracy of pose estimation and mapping. In the artificial structured environment, there are a lot of structured lines that can be utilized. Compared with point features, line features contain richer information. For example, structure lines can be used to construct surface features. To improve the robustness and stability of visual SLAM positioning in a low-texture environment, we propose a new point-line feature Visual inertial navigation system based on traditional SLAM method, which makes full use of the structural line features in the scene. Compared to the traditional SLAM system which use point-line features, we adopt a new point-line feature error reprojection model-cross-product of between projection line feature and detected line feature and nonlinear optimization strategy under long line, aiming to increase the robustness in a low-texture environment. The proposed algorithm has been verified by EuRoc dataset and real-world scenarios, and the results show that our algorithm has a greater improvement in accuracy.},
	language = {en},
	urldate = {2022-04-05},
	booktitle = {2021 4th {International} {Conference} on {Control} and {Computer} {Vision}},
	publisher = {ACM},
	author = {Yang, Gaochao and Wang, Qing and Liu, Pengfei and Zhang, Huan},
	month = aug,
	year = {2021},
	pages = {119--125},
	file = {Yang 等。 - 2021 - An Improved Monocular PL-SlAM Method with Point-Li.pdf:/home/red0orange/Zotero/storage/YV6AEI7P/Yang 等。 - 2021 - An Improved Monocular PL-SlAM Method with Point-Li.pdf:application/pdf},
}

@article{zhu_pld-vins_2021,
	series = {{PLD}-{VINS}，{PL}+{D实现}},
	title = {{PLD}-{VINS}: {RGBD} visual-inertial {SLAM} with point and line features},
	volume = {119},
	issn = {12709638},
	shorttitle = {{PLD}-{VINS}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1270963821006957},
	doi = {10.1016/j.ast.2021.107185},
	language = {en},
	urldate = {2022-04-05},
	journal = {Aerospace Science and Technology},
	author = {Zhu, Yeqing and Jin, Rui and Lou, Tai-shan and Zhao, Liangyu},
	month = dec,
	year = {2021},
	pages = {107185},
	file = {Zhu 等。 - 2021 - PLD-VINS RGBD visual-inertial SLAM with point and.pdf:/home/red0orange/Zotero/storage/P8FFKTBQ/Zhu 等。 - 2021 - PLD-VINS RGBD visual-inertial SLAM with point and.pdf:application/pdf},
}

@article{qin_vins-mono_2017,
	series = {{VINS}},
	title = {{VINS}-{Mono}: {A} {Robust} and {Versatile} {Monocular} {Visual}-{Inertial} {State} {Estimator}},
	shorttitle = {{VINS}-{Mono}},
	url = {http://arxiv.org/abs/1708.03852},
	doi = {10.1109/TRO.2018.2853729},
	abstract = {A monocular visual-inertial system (VINS), consisting of a camera and a low-cost inertial measurement unit (IMU), forms the minimum sensor suite for metric six degreesof-freedom (DOF) state estimation. However, the lack of direct distance measurement poses signiﬁcant challenges in terms of IMU processing, estimator initialization, extrinsic calibration, and nonlinear optimization. In this work, we present VINSMono: a robust and versatile monocular visual-inertial state estimator. Our approach starts with a robust procedure for estimator initialization and failure recovery. A tightly-coupled, nonlinear optimization-based method is used to obtain high accuracy visual-inertial odometry by fusing pre-integrated IMU measurements and feature observations. A loop detection module, in combination with our tightly-coupled formulation, enables relocalization with minimum computation overhead. We additionally perform four degrees-of-freedom pose graph optimization to enforce global consistency. We validate the performance of our system on public datasets and real-world experiments and compare against other state-of-the-art algorithms. We also perform onboard closed-loop autonomous ﬂight on the MAV platform and port the algorithm to an iOS-based demonstration. We highlight that the proposed work is a reliable, complete, and versatile system that is applicable for different applications that require high accuracy localization. We open source our implementations for both PCs1 and iOS mobile devices2.},
	language = {en},
	urldate = {2022-04-05},
	journal = {arXiv:1708.03852 [cs]},
	author = {Qin, Tong and Li, Peiliang and Shen, Shaojie},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.03852},
	file = {Qin 等。 - 2017 - VINS-Mono A Robust and Versatile Monocular Visual.pdf:/home/red0orange/Zotero/storage/PBNVE2C9/Qin 等。 - 2017 - VINS-Mono A Robust and Versatile Monocular Visual.pdf:application/pdf},
}

@article{yang_pop-up_2017,
	title = {Pop-up {SLAM}: {Semantic} {Monocular} {Plane} {SLAM} for {Low}-texture {Environments}},
	shorttitle = {Pop-up {SLAM}},
	url = {http://arxiv.org/abs/1703.07334},
	abstract = {Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67\%.},
	language = {en},
	urldate = {2022-04-04},
	journal = {arXiv:1703.07334 [cs]},
	author = {Yang, Shichao and Song, Yu and Kaess, Michael and Scherer, Sebastian},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.07334},
	file = {Yang 等。 - 2017 - Pop-up SLAM Semantic Monocular Plane SLAM for Low.pdf:/home/red0orange/Zotero/storage/9RQCH8Z9/Yang 等。 - 2017 - Pop-up SLAM Semantic Monocular Plane SLAM for Low.pdf:application/pdf},
}

@article{grompone_von_gioi_lsd_2012,
	series = {Line {Segment} {Detector}({LSD})},
	title = {{LSD}: a {Line} {Segment} {Detector}},
	volume = {2},
	issn = {2105-1232},
	shorttitle = {{LSD}},
	url = {https://www.ipol.im/pub/art/2012/gjmr-lsd/?utm_source=doi},
	doi = {10.5201/ipol.2012.gjmr-lsd},
	abstract = {We propose a linear-time line segment detector that gives accurate results, a controlled number of false detections, and requires no parameter tuning. This algorithm is tested and compared to state-of-the-art algorithms on a wide set of natural images.},
	language = {en},
	urldate = {2022-04-05},
	journal = {Image Processing On Line},
	author = {Grompone von Gioi, Rafael and Jakubowicz, Jérémie and Morel, Jean-Michel and Randall, Gregory},
	month = mar,
	year = {2012},
	pages = {35--55},
	file = {Grompone von Gioi 等。 - 2012 - LSD a Line Segment Detector.pdf:/home/red0orange/Zotero/storage/I99DVLY8/Grompone von Gioi 等。 - 2012 - LSD a Line Segment Detector.pdf:application/pdf},
}

@article{dai_uncertainty-driven_2021,
	title = {Uncertainty-driven active view planning in feature-based monocular {vSLAM}},
	volume = {108},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494621003823},
	doi = {10.1016/j.asoc.2021.107459},
	abstract = {Traditional feature-based monocular visual simultaneous localization and mapping (vSLAM) methods suffer from frequent tracking failure in low-texture scenes. Although tracking stability can be guaranteed by actively adjusting the camera’s orientation to track known landmarks, it can easily lead to the problem of over-exploitation. This means that instead of discovering new landmarks, the camera focuses on an area that has already been observed, which is not conducive to fully exploring unknown environments. To address this problem, an uncertainty-driven active view planning framework is proposed to actively adjust the orientation of the monocular camera equipped on a three degree of freedom (3-DoF) pan–tilt. As a result, a trade-off between exploitation, i.e., making full use of known information, and exploration, i.e., obtaining more information of unknown environments can be achieved. First, a novel landmark uncertainty model is established to represent the uncertainty of environmental information. Second, the trade-off problem is formulated as an inequality-constrained optimization mathematical model, whose objective function is related to landmark uncertainty. Last, Karush–Kuhn–Tucker (KKT) conditions are utilized to solve the optimization problem. Experimental results on a publicly available monocular dataset and in a real-world environment show that this framework reduces the rate of tracking failure by 50\% on average. The localization error is also reduced by 0.07 m for translation and 0.004 rad/m for rotation on average. Meanwhile, the number of reconstructed landmarks increases by 17.86\% on average, which indicates an appropriate trade-off between exploitation and exploration.},
	language = {en},
	urldate = {2022-04-04},
	journal = {Applied Soft Computing},
	author = {Dai, Xu-Yang and Meng, Qing-Hao and Jin, Sheng},
	month = sep,
	year = {2021},
	pages = {107459},
	file = {Dai 等。 - 2021 - Uncertainty-driven active view planning in feature.pdf:/home/red0orange/Zotero/storage/4KJCY7H8/Dai 等。 - 2021 - Uncertainty-driven active view planning in feature.pdf:application/pdf},
}

@article{xiao2021survey,
	series = {中文版本综述},
	title = {Survey of simultaneous localization and mapping based on environmental semantic information},
	volume = {43},
	number = {6},
	journal = {工程科学学报},
	author = {Xiao-qian, LI and Wei, HE and Shi-qiang, ZHU and Yue-hua, LI and Tian, XIE},
	year = {2021},
	note = {Publisher: 工程科学学报},
	keywords = {综述},
	pages = {754--767},
	file = {Survey+of+simultaneous+localization+and+mapping+based+on+environmental+semantic+information.pdf:/home/red0orange/Zotero/storage/7FXCXSIG/Survey+of+simultaneous+localization+and+mapping+based+on+environmental+semantic+information.pdf:application/pdf},
}

@article{cui_sof-slam_2019,
	title = {{SOF}-{SLAM}: {A} {Semantic} {Visual} {SLAM} for {Dynamic} {Environments}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{SOF}-{SLAM}},
	url = {https://ieeexplore.ieee.org/document/8894002/},
	doi = {10.1109/ACCESS.2019.2952161},
	abstract = {Simultaneous Localization and Mapping (SLAM) plays an important role in the computer vision and robotics ﬁeld. The traditional SLAM framework adopts a strong static world assumption for analysis convenience. How to cope with dynamic environments is of vital importance and attracts more attentions. Existing SLAM systems toward dynamic scenes either solely utilize semantic information, solely utilize geometry information, or naively combine the results from them in a loosely coupled way. In this paper, we present SOF-SLAM: Semantic Optical Flow SLAM, a visual semantic SLAM system toward dynamic environments, which is built on RGB-D mode of ORB-SLAM2. A new dynamic features detection approach called semantic optical ﬂow is proposed, which is a kind of tightly coupled way and can fully take advantage of feature’s dynamic characteristic hidden in semantic and geometry information to remove dynamic features effectively and reasonably. The pixel-wise semantic segmentation results generated by SegNet serve as mask in the proposed semantic optical ﬂow to get a reliable fundamental matrix, which is then used to ﬁlter out the truly dynamic features. Only the remaining static features are reserved in the tracking and optimization module to achieve accurate camera pose estimation in dynamic environments. Experiments on public TUM RGB-D dataset and in real-world environment are conducted. Compared with ORB-SLAM2, the proposed SOF-SLAM achieves averagely 96.73\% improvements in high-dynamic scenarios. It also outperforms the other four state-of-the-art SLAM systems which cope with the dynamic environments.},
	language = {en},
	urldate = {2022-04-04},
	journal = {IEEE Access},
	author = {Cui, Linyan and Ma, Chaowei},
	year = {2019},
	pages = {166528--166539},
	file = {Cui 和 Ma - 2019 - SOF-SLAM A Semantic Visual SLAM for Dynamic Envir.pdf:/home/red0orange/Zotero/storage/WZHDKL3K/Cui 和 Ma - 2019 - SOF-SLAM A Semantic Visual SLAM for Dynamic Envir.pdf:application/pdf},
}

@article{bescos_dynaslam_2018,
	title = {{DynaSLAM}: {Tracking}, {Mapping} and {Inpainting} in {Dynamic} {Scenes}},
	volume = {3},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{DynaSLAM}},
	url = {http://arxiv.org/abs/1806.05620},
	doi = {10.1109/LRA.2018.2860039},
	abstract = {The assumption of scene rigidity is typical in SLAM algorithms. Such a strong assumption limits the use of most visual SLAM systems in populated real-world environments, which are the target of several relevant applications like service robotics or autonomous vehicles.},
	language = {en},
	number = {4},
	urldate = {2022-04-04},
	journal = {IEEE Robotics and Automation Letters},
	author = {Bescos, Berta and Fácil, José M. and Civera, Javier and Neira, José},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.05620},
	pages = {4076--4083},
	file = {Bescos 等。 - 2018 - DynaSLAM Tracking, Mapping and Inpainting in Dyna.pdf:/home/red0orange/Zotero/storage/UHEEUB44/Bescos 等。 - 2018 - DynaSLAM Tracking, Mapping and Inpainting in Dyna.pdf:application/pdf},
}

@inproceedings{schonberger2018semantic,
	title = {Semantic visual localization},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Schönberger, Johannes L and Pollefeys, Marc and Geiger, Andreas and Sattler, Torsten},
	year = {2018},
	pages = {6896--6906},
}

@inproceedings{naseer_semantics-aware_2017,
	address = {Singapore, Singapore},
	title = {Semantics-aware visual localization under challenging perceptual conditions},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989305/},
	doi = {10.1109/ICRA.2017.7989305},
	abstract = {Visual place recognition under difﬁcult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.},
	language = {en},
	urldate = {2022-04-04},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Naseer, Tayyab and Oliveira, Gabriel L. and Brox, Thomas and Burgard, Wolfram},
	month = may,
	year = {2017},
	pages = {2614--2620},
	file = {Naseer 等。 - 2017 - Semantics-aware visual localization under challeng.pdf:/home/red0orange/Zotero/storage/BIYR8RHB/Naseer 等。 - 2017 - Semantics-aware visual localization under challeng.pdf:application/pdf},
}

@article{qin_avp-slam_2020,
	series = {{AVP}-{SLAM}},
	title = {{AVP}-{SLAM}: {Semantic} {Visual} {Mapping} and {Localization} for {Autonomous} {Vehicles} in the {Parking} {Lot}},
	shorttitle = {{AVP}-{SLAM}},
	url = {http://arxiv.org/abs/2007.01813},
	abstract = {Autonomous valet parking is a speciﬁc application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visualbased methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surroundview cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
	language = {en},
	urldate = {2022-03-22},
	journal = {arXiv:2007.01813 [cs]},
	author = {Qin, Tong and Chen, Tongqing and Chen, Yilun and Su, Qing},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01813},
	file = {Qin 等。 - 2020 - AVP-SLAM Semantic Visual Mapping and Localization.pdf:/home/red0orange/Zotero/storage/C8UMTP3A/Qin 等。 - 2020 - AVP-SLAM Semantic Visual Mapping and Localization.pdf:application/pdf},
}

@article{garg_semantics_2020,
	title = {Semantics for {Robotic} {Mapping}, {Perception} and {Interaction}: {A} {Survey}},
	volume = {8},
	issn = {1935-8253, 1935-8261},
	shorttitle = {Semantics for {Robotic} {Mapping}, {Perception} and {Interaction}},
	url = {http://arxiv.org/abs/2101.00443},
	doi = {10.1561/2300000059},
	abstract = {For robots to navigate and interact more richly with the world around them, they will likely require a deeper understanding of the world in which they operate. In robotics and related research ﬁelds, the study of understanding is often referred to as semantics, which dictates what does the world ‘mean’ to a robot, and is strongly tied to the question of how to represent that meaning. With humans and robots increasingly operating in the same world, the prospects of human-robot interaction also bring semantics and ontology of natural language into the picture. Driven by need, as well as by enablers like increasing availability of training data and computational resources, semantics is a rapidly growing research area in robotics. The ﬁeld has received signiﬁcant attention in the research literature to date, but most reviews and surveys have focused on particular aspects of the topic: the technical research issues regarding its use in speciﬁc robotic topics like mapping or segmentation, or its relevance to one particular application domain like autonomous driving. A new treatment is therefore required, and is also timely because so much relevant research has occurred since many of the key surveys were published. This survey paper therefore provides an overarching snapshot of where semantics in robotics stands today. We establish a taxonomy for semantics research in or relevant to robotics, split into four broad categories of activity, in which semantics are extracted, used, or both. Within these broad categories we survey dozens of major topics including fundamentals from the computer vision ﬁeld and key robotics research areas utilizing semantics, including mapping, navigation and interaction with the world. The paper also covers key practical considerations, including enablers like increased data availability and improved computational hardware, and major application areas where semantics is or is likely to play a key role. In creating this survey, we hope to provide researchers across academia and industry with a comprehensive reference that helps facilitate future research in this exciting ﬁeld.},
	language = {en},
	number = {1–2},
	urldate = {2022-03-30},
	journal = {Foundations and Trends® in Robotics},
	author = {Garg, Sourav and Sünderhauf, Niko and Dayoub, Feras and Morrison, Douglas and Cosgun, Akansel and Carneiro, Gustavo and Wu, Qi and Chin, Tat-Jun and Reid, Ian and Gould, Stephen and Corke, Peter and Milford, Michael},
	year = {2020},
	note = {arXiv: 2101.00443},
	keywords = {未阅读, 综述},
	pages = {1--224},
	file = {Garg 等。 - 2020 - Semantics for Robotic Mapping, Perception and Inte.pdf:/home/red0orange/Zotero/storage/IY3ER43R/Garg 等。 - 2020 - Semantics for Robotic Mapping, Perception and Inte.pdf:application/pdf},
}

@inproceedings{li2018stereo,
	title = {Stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving},
	booktitle = {Proceedings of the european conference on computer vision ({ECCV})},
	author = {Li, Peiliang and Qin, Tong and {others}},
	year = {2018},
	pages = {646--661},
}

@inproceedings{yu2018ds,
	title = {{DS}-{SLAM}: {A} semantic visual {SLAM} towards dynamic environments},
	booktitle = {2018 {IEEE}/{RSJ} international conference on intelligent robots and systems ({IROS})},
	author = {Yu, Chao and Liu, Zuxin and Liu, Xin-Jun and Xie, Fugui and Yang, Yi and Wei, Qi and Fei, Qiao},
	year = {2018},
	note = {tex.organization: IEEE},
	pages = {1168--1174},
}

@article{ganti2018visual,
	title = {Visual slam with network uncertainty informed feature selection},
	journal = {arXiv preprint arXiv:1811.11946},
	author = {Ganti, Pranav and Waslander, Steven L},
	year = {2018},
}

@article{xue2019low,
	title = {Low redundancy feature selection using conditional mutual information for short-term load forecasting. {J}. {Northeast} {Electr}},
	volume = {39},
	number = {2},
	journal = {Power Univ},
	author = {Xue, L and Huang, NT and Zhao, SY and Wang, PP},
	year = {2019},
	pages = {30--38},
}

@inproceedings{8317859,
	title = {Utilizing semantic visual landmarks for precise vehicle navigation},
	doi = {10.1109/ITSC.2017.8317859},
	booktitle = {2017 {IEEE} 20th international conference on intelligent transportation systems ({ITSC})},
	author = {Murali, Varun and Chiu, Han-Pang and Samarasekera, Supun and Kumar, Rakesh Teddy},
	year = {2017},
	pages = {1--8},
}

@inproceedings{deng_feature-constrained_2018,
	address = {Brisbane, QLD},
	title = {Feature-constrained {Active} {Visual} {SLAM} for {Mobile} {Robot} {Navigation}},
	isbn = {978-1-5386-3081-5},
	url = {https://ieeexplore.ieee.org/document/8460721/},
	doi = {10.1109/ICRA.2018.8460721},
	abstract = {This paper focuses on tracking failure avoidance during vision-based navigation to a desired goal in unknown environments. While using feature-based Visual Simultaneous Localization and Mapping (VSLAM), continuous identiﬁcation and association of map points are required during motion. Thus, we discuss a motion planning framework that takes into account sensory constraints for a reliable navigation. We use information available in the SLAM and propose a data-driven approach to predict the number of map points associated in a given pose. Then, a distance-optimal path planner utilizes the model to constrain paths such that the number of associated map points in each pose is above a threshold. We also include an online mapping of the environment for collision avoidance. Overall, we propose an iterative motion planning framework that enables real-time replanning after the acquisition of more information. Experiments in two environments demonstrate the performance of the proposed framework.},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Deng, Xinke and Zhang, Zixu and Sintov, Avishai and Huang, Jing and Bretl, Timothy},
	month = may,
	year = {2018},
	keywords = {师兄推荐},
	pages = {7233--7238},
	file = {Deng 等。 - 2018 - Feature-constrained Active Visual SLAM for Mobile .pdf:/home/red0orange/Zotero/storage/CFAAQ4DB/Deng 等。 - 2018 - Feature-constrained Active Visual SLAM for Mobile .pdf:application/pdf},
}

@inproceedings{lv_semantically_2021,
	address = {Xi'an, China},
	title = {Semantically {Guided} {Multi}-{View} {Stereo} for {Dense} {3D} {Road} {Mapping}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561077/},
	doi = {10.1109/ICRA48506.2021.9561077},
	abstract = {Compared to widely used LiDAR-based mapping in autonomous driving ﬁeld, image-based mapping method has the advantages of low cost, high resolution, and no need for complex calibration. However, the image-based 3D mapping depends heavily on the texture richness and always leaves holes and outliers in low-textured areas, such as the road surface. To this end, this paper proposed a novel semantically guided Multi-View Stereo method for dense 3D road mapping, which integrates semantic information into PatchMatch-based MVS pipeline and uses image semantic segmentation as soft constraints in neighbor views selection, depth-map initialization, depth propagation, and depth-map completion. Experimental results on public and our own datasets show that, with the help of semantics, the proposed method achieves superior completeness with comparable accuracy for 3D road mapping compared to state-of-the-art MVS methods.},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lv, Mingzhe and Tu, Diantao and Tang, Xincheng and Liu, Yuqian and Shen, Shuhan},
	month = may,
	year = {2021},
	keywords = {师兄推荐},
	pages = {11189--11195},
	file = {Lv 等。 - 2021 - Semantically Guided Multi-View Stereo for Dense 3D.pdf:/home/red0orange/Zotero/storage/FW66NVLV/Lv 等。 - 2021 - Semantically Guided Multi-View Stereo for Dense 3D.pdf:application/pdf},
}

@inproceedings{bowman_probabilistic_2017,
	address = {Singapore, Singapore},
	title = {Probabilistic data association for semantic {SLAM}},
	isbn = {978-1-5090-4633-1},
	url = {http://ieeexplore.ieee.org/document/7989203/},
	doi = {10.1109/ICRA.2017.7989203},
	abstract = {Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.},
	language = {en},
	urldate = {2022-04-03},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Bowman, Sean L. and Atanasov, Nikolay and Daniilidis, Kostas and Pappas, George J.},
	month = may,
	year = {2017},
	keywords = {师兄推荐},
	pages = {1722--1729},
	file = {Bowman 等。 - 2017 - Probabilistic data association for semantic SLAM.pdf:/home/red0orange/Zotero/storage/2KDL363D/Bowman 等。 - 2017 - Probabilistic data association for semantic SLAM.pdf:application/pdf},
}

@article{schmid_panoptic_2022,
	title = {Panoptic {Multi}-{TSDFs}: a {Flexible} {Representation} for {Online} {Multi}-resolution {Volumetric} {Mapping} and {Long}-term {Dynamic} {Scene} {Consistency}},
	shorttitle = {Panoptic {Multi}-{TSDFs}},
	url = {http://arxiv.org/abs/2109.10165},
	abstract = {For robotic interaction in environments shared with other agents, access to volumetric and semantic maps of the scene is crucial. However, such environments are inevitably subject to long-term changes, which the map needs to account for. We thus propose panoptic multi-TSDFs as a novel representation for multi-resolution volumetric mapping in changing environments. By leveraging high-level information for 3D reconstruction, our proposed system allocates high resolution only where needed. Through reasoning on the object level, semantic consistency over time is achieved. This enables our method to maintain up-to-date reconstructions with high accuracy while improving coverage by incorporating previous data. We show in thorough experimental evaluation that our map can be efﬁciently constructed, maintained, and queried during online operation, and that the presented approach can operate robustly on real depth sensors using non-optimized panoptic segmentation as input.},
	language = {en},
	urldate = {2022-04-03},
	journal = {arXiv:2109.10165 [cs]},
	author = {Schmid, Lukas and Delmerico, Jeffrey and Schönberger, Johannes and Nieto, Juan and Pollefeys, Marc and Siegwart, Roland and Cadena, Cesar},
	month = feb,
	year = {2022},
	note = {arXiv: 2109.10165},
	keywords = {师兄推荐},
	file = {Schmid 等。 - 2022 - Panoptic Multi-TSDFs a Flexible Representation fo.pdf:/home/red0orange/Zotero/storage/6FNY86D5/Schmid 等。 - 2022 - Panoptic Multi-TSDFs a Flexible Representation fo.pdf:application/pdf},
}

@article{liang_salientdso_2018,
	title = {{SalientDSO}: {Bringing} {Attention} to {Direct} {Sparse} {Odometry}},
	shorttitle = {{SalientDSO}},
	url = {http://arxiv.org/abs/1803.00127},
	abstract = {Although cluttered indoor scenes have a lot of useful high-level semantic information which can be used for mapping and localization, most Visual Odometry (VO) algorithms rely on the usage of geometric features such as points, lines and planes. Lately, driven by this idea, the joint optimization of semantic labels and obtaining odometry has gained popularity in the robotics community. The joint optimization is good for accurate results but is generally very slow. At the same time, in the vision community, direct and sparse approaches for VO have stricken the right balance between speed and accuracy.},
	language = {en},
	urldate = {2022-04-02},
	journal = {arXiv:1803.00127 [cs]},
	author = {Liang, Huai-Jen and Sanket, Nitin J. and Fermüller, Cornelia and Aloimonos, Yiannis},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.00127},
	file = {Liang 等。 - 2018 - SalientDSO Bringing Attention to Direct Sparse Od.pdf:/home/red0orange/Zotero/storage/XIJ6PABK/Liang 等。 - 2018 - SalientDSO Bringing Attention to Direct Sparse Od.pdf:application/pdf},
}

@article{runz_maskfusion_2018-1,
	title = {{MaskFusion}: {Real}-{Time} {Recognition}, {Tracking} and {Reconstruction} of {Multiple} {Moving} {Objects}},
	shorttitle = {{MaskFusion}},
	url = {http://arxiv.org/abs/1804.09194},
	abstract = {We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D SLAM system that goes beyond traditional systems which output a purely geometric map of a static scene. MaskFusion recognizes, segments and assigns semantic class labels to different objects in the scene, while tracking and reconstructing them even when they move independently from the camera. As an RGB-D camera scans a cluttered scene, image-based instance-level semantic segmentation creates semantic object masks that enable real-time object recognition and the creation of an object-level representation for the world map. Unlike previous recognition-based SLAM systems, MaskFusion does not require known models of the objects it can recognize, and can deal with multiple independent motions. MaskFusion takes full advantage of using instance-level semantic segmentation to enable semantic labels to be fused into an object-aware map, unlike recent semantics enabled SLAM systems that perform voxel-level semantic segmentation. We show augmented-reality applications that demonstrate the unique features of the map output by MaskFusion: instance-aware, semantic and dynamic.},
	language = {en},
	urldate = {2022-04-02},
	journal = {arXiv:1804.09194 [cs]},
	author = {Rünz, Martin and Buffier, Maud and Agapito, Lourdes},
	month = oct,
	year = {2018},
	note = {arXiv: 1804.09194},
	file = {Rünz 等。 - 2018 - MaskFusion Real-Time Recognition, Tracking and Re.pdf:/home/red0orange/Zotero/storage/KL4I5IMD/Rünz 等。 - 2018 - MaskFusion Real-Time Recognition, Tracking and Re.pdf:application/pdf},
}

@article{li_semi-dense_2016,
	series = {{SLAM帮语义}，输出带标签的{3D} {Map}},
	title = {Semi-{Dense} {3D} {Semantic} {Mapping} from {Monocular} {SLAM}},
	url = {http://arxiv.org/abs/1611.04144},
	abstract = {The bundle of geometry and appearance in computer vision has proven to be a promising solution for robots across a wide variety of applications. Stereo cameras and RGBD sensors are widely used to realise fast 3D reconstruction and trajectory tracking in a dense way. However, they lack ﬂexibility of seamless switch between different scaled environments, i.e., indoor and outdoor scenes. In addition, semantic information are still hard to acquire in a 3D mapping. We address this challenge by combining the stateof-art deep learning method and semi-dense Simultaneous Localisation and Mapping (SLAM) based on video stream from a monocular camera. In our approach, 2D semantic information are transferred to 3D mapping via correspondence between connective Keyframes with spatial consistency. There is no need to obtain a semantic segmentation for each frame in a sequence, so that it could achieve a reasonable computation time. We evaluate our method on indoor/outdoor datasets and lead to an improvement in the 2D semantic labelling over baseline single frame predictions.},
	language = {en},
	urldate = {2022-04-01},
	journal = {arXiv:1611.04144 [cs]},
	author = {Li, Xuanpeng and Belaroussi, Rachid},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.04144},
	file = {Li 和 Belaroussi - 2016 - Semi-Dense 3D Semantic Mapping from Monocular SLAM.pdf:/home/red0orange/Zotero/storage/HVCZWWIT/Li 和 Belaroussi - 2016 - Semi-Dense 3D Semantic Mapping from Monocular SLAM.pdf:application/pdf},
}

@article{chen_suma_2019,
	series = {看笔记，基于激光雷达的语义{SLAM}},
	title = {{SuMa}++: {Efficient} {LiDAR}-based {Semantic} {SLAM}},
	shorttitle = {{SuMa}++},
	url = {http://arxiv.org/abs/2105.11320},
	doi = {10.1109/IROS40897.2019.8967704},
	abstract = {Reliable and accurate localization and mapping are key components of most autonomous systems. Besides geometric information about the mapped environment, the semantics plays an important role to enable intelligent navigation behaviors. In most realistic environments, this task is particularly complicated due to dynamics caused by moving objects, which can corrupt the mapping step or derail localization. In this paper, we propose an extension of a recently published surfelbased mapping approach exploiting three-dimensional laser range scans by integrating semantic information to facilitate the mapping process. The semantic information is efﬁciently extracted by a fully convolutional neural network and rendered on a spherical projection of the laser range data. This computed semantic segmentation results in point-wise labels for the whole scan, allowing us to build a semantically-enriched map with labeled surfels. This semantic map enables us to reliably ﬁlter moving objects, but also improve the projective scan matching via semantic constraints. Our experimental evaluation on challenging highways sequences from KITTI dataset with very few static structures and a large amount of moving cars shows the advantage of our semantic SLAM approach in comparison to a purely geometric, state-of-the-art approach.},
	language = {en},
	urldate = {2022-03-30},
	journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Chen, Xieyuanli and Milioto, Andres and Palazzolo, Emanuele and Giguère, Philippe and Behley, Jens and Stachniss, Cyrill},
	month = nov,
	year = {2019},
	note = {arXiv: 2105.11320},
	pages = {4530--4537},
	file = {Chen 等。 - 2019 - SuMa++ Efficient LiDAR-based Semantic SLAM.pdf:/home/red0orange/Zotero/storage/J7AT36G3/Chen 等。 - 2019 - SuMa++ Efficient LiDAR-based Semantic SLAM.pdf:application/pdf},
}

@article{he_transfg_2021,
	title = {{TransFG}: {A} {Transformer} {Architecture} for {Fine}-grained {Recognition}},
	shorttitle = {{TransFG}},
	url = {http://arxiv.org/abs/2103.07976},
	abstract = {Fine-grained visual classiﬁcation (FGVC) which aims at recognizing objects from subcategories is a very challenging task due to the inherently subtle inter-class differences. Most existing works mainly tackle this problem by reusing the backbone network to extract features of detected discriminative regions. However, this strategy inevitably complicates the pipeline and pushes the proposed regions to contain most parts of the objects thus fails to locate the really important parts. Recently, vision transformer (ViT) shows its strong performance in the traditional classiﬁcation task. The selfattention mechanism of the transformer links every patch token to the classiﬁcation token. In this work, we ﬁrst evaluate the effectiveness of the ViT framework in the ﬁne-grained recognition setting. Then motivated by the strength of the attention link can be intuitively considered as an indicator of the importance of tokens, we further propose a novel Part Selection Module that can be applied to most of the transformer architectures where we integrate all raw attention weights of the transformer into an attention map for guiding the network to effectively and accurately select discriminative image patches and compute their relations. A contrastive loss is applied to enlarge the distance between feature representations of confusing classes. We name the augmented transformer-based model TransFG and demonstrate the value of it by conducting experiments on ﬁve popular ﬁne-grained benchmarks where we achieve state-of-the-art performance. Qualitative results are presented for better understanding of our model.},
	language = {en},
	urldate = {2021-12-20},
	journal = {arXiv:2103.07976 [cs]},
	author = {He, Ju and Chen, Jie-Neng and Liu, Shuai and Kortylewski, Adam and Yang, Cheng and Bai, Yutong and Wang, Changhu},
	month = dec,
	year = {2021},
	note = {00000 
arXiv: 2103.07976},
	file = {He 等。 - 2021 - TransFG A Transformer Architecture for Fine-grain.pdf:/home/red0orange/Zotero/storage/GR9KNXY2/He 等。 - 2021 - TransFG A Transformer Architecture for Fine-grain.pdf:application/pdf},
}

@article{ju_improving_2021,
	series = {医学图像(眼科) 带噪声样本学习},
	title = {Improving {Medical} {Image} {Classification} with {Label} {Noise} {Using} {Dual}-uncertainty {Estimation}},
	url = {http://arxiv.org/abs/2103.00528},
	abstract = {Deep neural networks are known to be data-driven and label noise can have a marked impact on model performance. Recent studies have shown great robustness to classic image recognition even under a high noisy rate. In medical applications, learning from datasets with label noise is more challenging since medical imaging datasets tend to have asymmetric (classdependent) noise and suffer from high observer variability. In this paper, we systematically discuss and deﬁne the two common types of label noise in medical images - disagreement label noise from inconsistency expert opinions and single-target label noise from wrong diagnosis record. We then propose an uncertainty estimation-based framework to handle these two label noise amid the medical image classiﬁcation task. We design a dualuncertainty estimation approach to measure the disagreement label noise and single-target label noise via improved Direct Uncertainty Prediction and Monte-Carlo-Dropout. A boostingbased curriculum training procedure is later introduced for robust learning. We demonstrate the effectiveness of our method by conducting extensive experiments on three different diseases: skin lesions, prostate cancer, and retinal diseases. We also release a large re-engineered database that consists of annotations from more than ten ophthalmologists with an unbiased golden standard dataset for evaluation and benchmarking. The dataset is available at https://mmai.group/peoples/julie/.},
	language = {en},
	urldate = {2021-12-10},
	journal = {arXiv:2103.00528 [cs]},
	author = {Ju, Lie and Wang, Xin and Wang, Lin and Mahapatra, Dwarikanath and Zhao, Xin and Harandi, Mehrtash and Drummond, Tom and Liu, Tongliang and Ge, Zongyuan},
	month = mar,
	year = {2021},
	note = {00000 
arXiv: 2103.00528},
	keywords = {医学},
	file = {Ju 等。 - 2021 - Improving Medical Image Classification with Label .pdf:/home/red0orange/Zotero/storage/K7QY58JE/Ju 等。 - 2021 - Improving Medical Image Classification with Label .pdf:application/pdf},
}

@article{liu_co-correcting_2021,
	series = {医学顶刊，医学图像带噪声学习},
	title = {Co-{Correcting}: {Noise}-tolerant {Medical} {Image} {Classification} via mutual {Label} {Correction}},
	volume = {40},
	issn = {0278-0062, 1558-254X},
	shorttitle = {Co-{Correcting}},
	url = {http://arxiv.org/abs/2109.05159},
	doi = {10.1109/TMI.2021.3091178},
	abstract = {With the development of deep learning, medical image classiﬁcation has been signiﬁcantly improved. However, deep learning requires massive data with labels. While labeling the samples by human experts is expensive and time-consuming, collecting labels from crowdsourcing suffers from the noises which may degenerate the accuracy of classiﬁers. Therefore, approaches that can effectively handle label noises are highly desired. Unfortunately, recent progress on handling label noise in deep learning has gone largely unnoticed by the medical image. To ﬁll the gap, this paper proposes a noise-tolerant medical image classiﬁcation framework named Co-Correcting, which signiﬁcantly improves classiﬁcation accuracy and obtains more accurate labels through dual-network mutual learning, label probability estimation, and curriculum label correcting. On two representative medical image datasets and the MNIST dataset, we test six latest Learning-withNoisy-Labels methods and conduct comparative studies. The experiments show that Co-Correcting achieves the best accuracy and generalization under different noise ratios in various tasks. Our project can be found at: https://github.com/JiarunLiu/Co-Correcting.},
	language = {en},
	number = {12},
	urldate = {2021-12-10},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Liu, Jiarun and Li, Ruirui and Sun, Chuan},
	month = dec,
	year = {2021},
	note = {00000 
arXiv: 2109.05159},
	keywords = {医学},
	pages = {3580--3592},
	file = {Liu 等。 - 2021 - Co-Correcting Noise-tolerant Medical Image Classi.pdf:/home/red0orange/Zotero/storage/9KCQ5XG7/Liu 等。 - 2021 - Co-Correcting Noise-tolerant Medical Image Classi.pdf:application/pdf},
}

@article{liu_timely_nodate,
	series = {医学，直接相关，提高标注的一致性},
	title = {{TIMELY}: {Improving} {Labeling} {Consistency} in {Medical} {Imaging} for {Cell} {Type} {Classiﬁcation}},
	abstract = {Diagnosing diseases such as leukemia or anemia requires reliable counts of blood cells. Hematologists usually label and count microscopy images of blood cells manually. In many cases, however, cells in different maturity states are difﬁcult to distinguish, and in combination with image noise and subjectivity, humans are prone to make labeling mistakes. This results in labels that are often not reproducible, which can directly affect the diagnoses. We introduce TIMELY, a probabilistic model that combines pseudotime inference methods with inhomogeneous hidden Markov trees, which addresses this challenge of label inconsistency. We show ﬁrst on simulation data that TIMELY is able to identify and correct wrong labels with higher precision and recall than baseline methods for labeling correction. We then apply our method to two real-world datasets of blood cell data and show that TIMELY successfully ﬁnds inconsistent labels, thereby improving the quality of human-generated labels.},
	language = {en},
	author = {Liu, Yushan and Geipel, Markus M and Tietz, Christoph and Buettner, Florian},
	note = {00000},
	keywords = {医学},
	pages = {8},
	file = {Liu 等。 - TIMELY Improving Labeling Consistency in Medical .pdf:/home/red0orange/Zotero/storage/4GFZ6M3B/Liu 等。 - TIMELY Improving Labeling Consistency in Medical .pdf:application/pdf},
}

@article{arpit_closer_nodate,
	series = {{LNL理论基础} - 深度学习模型先学习简单的样本},
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns ﬁrst. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
	language = {en},
	author = {Arpit, Devansh and Jastrzebski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	note = {00000},
	pages = {10},
	file = {Arpit 等。 - A Closer Look at Memorization in Deep Networks.pdf:/home/red0orange/Zotero/storage/YZ98XSHL/Arpit 等。 - A Closer Look at Memorization in Deep Networks.pdf:application/pdf},
}

@article{li_multi-task_2020,
	title = {Multi-task deep learning for fine-grained classification and grading in breast cancer histopathological images},
	volume = {79},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-6970-9},
	doi = {10.1007/s11042-018-6970-9},
	abstract = {Fine-grained classification and grading of breast cancer (BC) histopathological images are of great value in clinical application. However, automatic classification and grading of BC histopathological images are complicated by (1) small inter-class variance and large intraclass variance exist in BC histopathological images, and (2) features extracted from similar histopathological images with different magnification are quite different. To address these issues, an improved deep convolution neural network model is proposed and the procedure can be divided into three main stages. Firstly, in the representation learning process, multiclass recognition task and verification task of image pair are combined. Secondly, in the feature extraction process, a prior knowledge is built, which is “the variances in feature outputs between different subclasses is relatively large while the variance between the same subclass is small.” Additionally, the prior information that histopathological images with different magnification belong to the same subclass are embedded in the feature extraction process, which contributes to less sensitive with image magnification. The experimental results based on three different histopathological image datasets show that the performance of the proposed method is better than state of the art, with better robustness and generalization ability.},
	language = {en},
	number = {21-22},
	urldate = {2021-12-14},
	journal = {Multimedia Tools and Applications},
	author = {Li, Lingqiao and Pan, Xipeng and Yang, Huihua and Liu, Zhenbing and He, Yubei and Li, Zhongming and Fan, Yongxian and Cao, Zhiwei and Zhang, Longhao},
	month = jun,
	year = {2020},
	note = {00000},
	pages = {14509--14528},
	file = {Li 等。 - 2020 - Multi-task deep learning for fine-grained classifi.pdf:/home/red0orange/Zotero/storage/3M5N3CT2/Li 等。 - 2020 - Multi-task deep learning for fine-grained classifi.pdf:application/pdf},
}

@article{liu_peer_2020,
	series = {Peer {Loss}，{CORES2借鉴对象}，{Loss设计非常有意思}},
	title = {Peer {Loss} {Functions}: {Learning} from {Noisy} {Labels} without {Knowing} {Noise} {Rates}},
	shorttitle = {Peer {Loss} {Functions}},
	url = {http://arxiv.org/abs/1910.03231},
	abstract = {Learning with noisy labels is a common challenge in supervised learning. Existing approaches often require practitioners to specify noise rates, i.e., a set of parameters controlling the severity of label noises in the problem, and the speciﬁcations are either assumed to be given or estimated using additional steps. In this work, we introduce a new family of loss functions that we name as peer loss functions, which enables learning from noisy labels and does not require a priori speciﬁcation of the noise rates. Peer loss functions work within the standard empirical risk minimization (ERM) framework. We show that, under mild conditions, performing ERM with peer loss functions on the noisy data leads to the optimal or a near-optimal classiﬁer as if performing ERM over the clean training data, which we do not have access to. We pair our results with an extensive set of experiments. Peer loss provides a way to simplify model development when facing potentially noisy training labels, and can be promoted as a robust candidate loss function in such situations.},
	language = {en},
	urldate = {2021-12-14},
	journal = {arXiv:1910.03231 [cs, stat]},
	author = {Liu, Yang and Guo, Hongyi},
	month = aug,
	year = {2020},
	note = {00000 
arXiv: 1910.03231},
	file = {Liu 和 Guo - 2020 - Peer Loss Functions Learning from Noisy Labels wi.pdf:/home/red0orange/Zotero/storage/SSF48YV7/Liu 和 Guo - 2020 - Peer Loss Functions Learning from Noisy Labels wi.pdf:application/pdf},
}

@article{cheng_learning_2021,
	series = {{CORES2}：最新屠榜{Co系列}，特征级噪声},
	title = {{LEARNING} {WITH} {INSTANCE}-{DEPENDENT} {LABEL} {NOISE}: {A} {SAMPLE} {SIEVE} {APPROACH}},
	abstract = {Human-annotated labels are often prone to noise, and the presence of such noise will degrade the performance of the resulting deep neural network (DNN) models. Much of the literature (with several recent exceptions) of learning with noisy labels focuses on the case when the label noise is independent of features. Practically, annotations errors tend to be instance-dependent and often depend on the difﬁculty levels of recognizing a certain task. Applying existing results from instance-independent settings would require a signiﬁcant amount of estimation of noise rates. Therefore, providing theoretically rigorous solutions for learning with instance-dependent label noise remains a challenge. In this paper, we propose CORES2 (COnﬁdence REgularized Sample Sieve), which progressively sieves out corrupted examples. The implementation of CORES2 does not require specifying noise rates and yet we are able to provide theoretical guarantees of CORES2 in ﬁltering out the corrupted examples. This high-quality sample sieve allows us to treat clean examples and the corrupted ones separately in training a DNN solution, and such a separation is shown to be advantageous in the instance-dependent noise setting. We demonstrate the performance of CORES2 on CIFAR10 and CIFAR100 datasets with synthetic instance-dependent label noise and Clothing1M with real-world human noise. As of independent interests, our sample sieve provides a generic machinery for anatomizing noisy datasets and provides a ﬂexible interface for various robust training techniques to further improve the performance. Code is available at https://github.com/UCSC-REAL/cores.},
	language = {en},
	author = {Cheng, Hao and Zhu, Zhaowei and Li, Xingyu and Gong, Yifei and Sun, Xing and Liu, Yang},
	year = {2021},
	note = {00000},
	pages = {27},
	file = {Cheng 等。 - 2021 - LEARNING WITH INSTANCE-DEPENDENT LABEL NOISE A SA.pdf:/home/red0orange/Zotero/storage/BLU63HEF/Cheng 等。 - 2021 - LEARNING WITH INSTANCE-DEPENDENT LABEL NOISE A SA.pdf:application/pdf},
}

@inproceedings{chang_your_2021,
	address = {Nashville, TN, USA},
	series = {细粒度的分类不会帮助粗粒度的分类},
	title = {Your “{Flamingo}” is {My} “{Bird}”: {Fine}-{Grained}, or {Not}},
	isbn = {978-1-66544-509-2},
	shorttitle = {Your “{Flamingo}” is {My} “{Bird}”},
	url = {https://ieeexplore.ieee.org/document/9578853/},
	doi = {10.1109/CVPR46437.2021.01131},
	abstract = {Whether what you see in Figure 1 is a “ﬂamingo” or a “bird”, is the question we ask in this paper. While ﬁnegrained visual classiﬁcation (FGVC) strives to arrive at the former, for the majority of us non-experts just “bird” would probably sufﬁce. The real question is therefore –how can we tailor for different ﬁne-grained deﬁnitions under divergent levels of expertise. For that, we re-envisage the traditional setting of FGVC, from single-label classiﬁcation, to that of top-down traversal of a pre-deﬁned coarse-to-ﬁne label hierarchy – so that our answer becomes “bird” ⇒ “Phoenicopteriformes” ⇒ “Phoenicopteridae” ⇒ “ﬂamingo”.},
	language = {en},
	urldate = {2021-12-20},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chang, Dongliang and Pang, Kaiyue and Zheng, Yixiao and Ma, Zhanyu and Song, Yi-Zhe and Guo, Jun},
	month = jun,
	year = {2021},
	note = {00000},
	pages = {11471--11480},
	file = {Chang 等。 - 2021 - Your “Flamingo” is My “Bird” Fine-Grained, or Not.pdf:/home/red0orange/Zotero/storage/GZF5RUAX/Chang 等。 - 2021 - Your “Flamingo” is My “Bird” Fine-Grained, or Not.pdf:application/pdf},
}

@article{wei_fine-grained_2021,
	title = {Fine-{Grained} {Image} {Analysis} with {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Fine-{Grained} {Image} {Analysis} with {Deep} {Learning}},
	url = {http://arxiv.org/abs/2111.06119},
	abstract = {Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to ﬁne-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-deﬁne and broaden the ﬁeld of FGIA by consolidating two fundamental ﬁne-grained research areas – ﬁne-grained image recognition and ﬁne-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-speciﬁc applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.},
	language = {en},
	urldate = {2021-12-20},
	journal = {arXiv:2111.06119 [cs]},
	author = {Wei, Xiu-Shen and Song, Yi-Zhe and Mac Aodha, Oisin and Wu, Jianxin and Peng, Yuxin and Tang, Jinhui and Yang, Jian and Belongie, Serge},
	month = nov,
	year = {2021},
	note = {00000 
arXiv: 2111.06119},
	file = {Wei 等。 - 2021 - Fine-Grained Image Analysis with Deep Learning A .pdf:/home/red0orange/Zotero/storage/7MZRGW44/Wei 等。 - 2021 - Fine-Grained Image Analysis with Deep Learning A .pdf:application/pdf},
}

@article{he_momentum_2020,
	series = {{无监督Moco}},
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1911.05722},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-ﬂy that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classiﬁcation. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	language = {en},
	urldate = {2021-12-17},
	journal = {arXiv:1911.05722 [cs]},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = mar,
	year = {2020},
	note = {00000 
arXiv: 1911.05722},
	file = {He 等。 - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:/home/red0orange/Zotero/storage/8WQ3QDB3/He 等。 - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf:application/pdf},
}

@inproceedings{yi_probabilistic_2019,
	address = {Long Beach, CA, USA},
	series = {{PENCIL}},
	title = {Probabilistic {End}-{To}-{End} {Noise} {Correction} for {Learning} {With} {Noisy} {Labels}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953202/},
	doi = {10.1109/CVPR.2019.00718},
	abstract = {Deep learning has achieved excellent performance in various computer vision tasks, but requires a lot of training examples with clean labels. It is easy to collect a dataset with noisy labels, but such noise makes networks overﬁt seriously and accuracies drop dramatically. To address this problem, we propose an end-to-end framework called PENCIL, which can update both network parameters and label estimations as label distributions. PENCIL is independent of the backbone network structure and does not need an auxiliary clean dataset or prior information about noise, thus it is more general and robust than existing methods and is easy to apply. PENCIL outperforms previous state-of-the-art methods by large margins on both synthetic and real-world datasets with different noise types and noise rates. Experiments show that PENCIL is robust on clean datasets, too.},
	language = {en},
	urldate = {2021-12-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yi, Kun and Wu, Jianxin},
	month = jun,
	year = {2019},
	note = {00000},
	pages = {7010--7018},
	file = {Yi 和 Wu - 2019 - Probabilistic End-To-End Noise Correction for Lear.pdf:/home/red0orange/Zotero/storage/38WX6H8S/Yi 和 Wu - 2019 - Probabilistic End-To-End Noise Correction for Lear.pdf:application/pdf},
}

@article{li_dividemix_2020,
	series = {{DivideMix}：先找到噪声样本，然后噪声样本变为无标签样本，下一步进行半监督学习},
	title = {{DivideMix}: {Learning} with {Noisy} {Labels} as {Semi}-supervised {Learning}},
	shorttitle = {{DivideMix}},
	url = {http://arxiv.org/abs/2002.07394},
	abstract = {Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix .},
	urldate = {2021-11-16},
	journal = {arXiv:2002.07394 [cs]},
	author = {Li, Junnan and Socher, Richard and Hoi, Steven C. H.},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.07394},
	file = {arXiv Fulltext PDF:/home/red0orange/Zotero/storage/3AFL7G62/Li 等。 - 2020 - DivideMix Learning with Noisy Labels as Semi-supe.pdf:application/pdf;arXiv.org Snapshot:/home/red0orange/Zotero/storage/NL9E5YYN/2002.html:text/html},
}

@article{song_learning_2021,
	series = {综述，提供了分类树},
	title = {Learning from {Noisy} {Labels} with {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Learning from {Noisy} {Labels} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.08199},
	abstract = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we ﬁrst describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into ﬁve groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies. All the contents will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:2007.08199 [cs, stat]},
	author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
	month = nov,
	year = {2021},
	note = {arXiv: 2007.08199},
	file = {Song 等。 - 2021 - Learning from Noisy Labels with Deep Neural Networ.pdf:/home/red0orange/Zotero/storage/GLVSWITM/Song 等。 - 2021 - Learning from Noisy Labels with Deep Neural Networ.pdf:application/pdf},
}

@article{algan_image_2021,
	series = {综述，提供了表格},
	title = {Image {Classification} with {Deep} {Learning} in the {Presence} of {Noisy} {Labels}: {A} {Survey}},
	volume = {215},
	issn = {09507051},
	shorttitle = {Image {Classification} with {Deep} {Learning} in the {Presence} of {Noisy} {Labels}},
	url = {http://arxiv.org/abs/1912.05170},
	doi = {10.1016/j.knosys.2021.106771},
	abstract = {Image classiﬁcation systems recently made a giant leap with the advancement of deep neural networks. However, these systems require an excessive amount of labeled data to be adequately trained. Gathering a correctly annotated dataset is not always feasible due to several factors, such as the expensiveness of the labeling process or diﬃculty of correctly classifying data, even for the experts. Because of these practical challenges, label noise is a common problem in realworld datasets, and numerous methods to train deep neural networks with label noise are proposed in the literature. Although deep neural networks are known to be relatively robust to label noise, their tendency to overﬁt data makes them vulnerable to memorizing even random noise. Therefore, it is crucial to consider the existence of label noise and develop counter algorithms to fade away its adverse eﬀects to train deep neural networks eﬃciently. Even though an extensive survey of machine learning techniques under label noise exists, the literature lacks a comprehensive survey of methodologies centered explicitly around deep learning in the presence of noisy labels. This paper aims to present these algorithms while categorizing them into one of the two subgroups: noise model based and noise model free methods. Algorithms in the ﬁrst group aim to estimate the noise structure and use this information to avoid the adverse eﬀects of noisy labels. Diﬀerently, methods in the second group try to come up with inherently noise robust algorithms by using approaches like robust losses, regularizers or other learning paradigms.},
	language = {en},
	urldate = {2021-11-17},
	journal = {Knowledge-Based Systems},
	author = {Algan, Görkem and Ulusoy, Ilkay},
	month = mar,
	year = {2021},
	note = {arXiv: 1912.05170},
	keywords = {综述},
	pages = {106771},
	file = {Algan 和 Ulusoy - 2021 - Image Classification with Deep Learning in the Pre.pdf:/home/red0orange/Zotero/storage/2BA69IZB/Algan 和 Ulusoy - 2021 - Image Classification with Deep Learning in the Pre.pdf:application/pdf},
}

@inproceedings{tanaka_joint_2018,
	series = {比较普通的思路，训练的过程中预测伪标签替换真实标签},
	title = {Joint {Optimization} {Framework} for {Learning} {With} {Noisy} {Labels}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Tanaka_Joint_Optimization_Framework_CVPR_2018_paper.html},
	urldate = {2021-11-16},
	author = {Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
	year = {2018},
	pages = {5552--5560},
	file = {Full Text PDF:/home/red0orange/Zotero/storage/8UNZDKQ9/Tanaka 等。 - 2018 - Joint Optimization Framework for Learning With Noi.pdf:application/pdf;Snapshot:/home/red0orange/Zotero/storage/ZVB4N3IP/Tanaka_Joint_Optimization_Framework_CVPR_2018_paper.html:text/html},
}

@article{jin_developing_2020-1,
	series = {智微信科 - 骨髓自动检测系统产品},
	title = {Developing and {Preliminary} {Validating} an {Automatic} {Cell} {Classification} {System} for {Bone} {Marrow} {Smears}: a {Pilot} {Study}},
	volume = {44},
	issn = {0148-5598, 1573-689X},
	shorttitle = {Developing and {Preliminary} {Validating} an {Automatic} {Cell} {Classification} {System} for {Bone} {Marrow} {Smears}},
	url = {https://link.springer.com/10.1007/s10916-020-01654-y},
	doi = {10.1007/s10916-020-01654-y},
	abstract = {Bone marrow smear examination is an indispensable diagnostic tool in the evaluation of hematological diseases, but the process of manual differential count is labor extensive. In this study, we developed an automatic system with integrated scanning hardware and machine learning-based software to perform differential cell count on bone marrow smears to assist diagnosis. The initial development of the artificial neural network was based on 3000 marrow smear samples retrospectively archived from Sir Run Run Shaw Hospital affiliated to Zhejiang University School of Medicine between June 2016 and December 2018. The preliminary field validating test of the system was based on 124 marrow smears newly collected from the Second Affiliated Hospital of Harbin Medical University between April 2019 and November 2019. The study was performed in parallel of machine automatic recognition with conventional manual differential count by pathologists using the microscope. We selected representative 600,000 marrow cell images as training set of the algorithm, followed by random captured 30,867 cell images for validation. In validation, the overall accuracy of automatic cell classification was 90.1\% (95\% CI, 89.8–90.5\%). In a preliminary field validating test, the reliability coefficient (ICC) of cell series proportion between the two analysis methods were high (ICC ≥ 0.883, P {\textless} 0.0001) and the results by the two analysis methods were consistent for granulocytes and erythrocytes. The system was effective in cell classification and differential cell count on marrow smears. It provides a useful digital tool in the screening and evaluation of various hematological disorders.},
	language = {en},
	number = {10},
	urldate = {2021-12-06},
	journal = {Journal of Medical Systems},
	author = {Jin, Hong and Fu, Xinyan and Cao, Xinyi and Sun, Mingxia and Wang, Xiaofen and Zhong, Yuhong and Yang, Suwen and Qi, Chao and Peng, Bo and He, Xin and He, Fei and Jiang, Yongfang and Gao, Haiyan and Li, Shun and Huang, Zhen and Li, Qiang and Fang, Fengqi and Zhang, Jun},
	month = oct,
	year = {2020},
	pages = {184},
	file = {Jin 等。 - 2020 - Developing and Preliminary Validating an Automatic.pdf:/home/red0orange/Zotero/storage/GQSAW5V3/Jin 等。 - 2020 - Developing and Preliminary Validating an Automatic.pdf:application/pdf},
}

@article{fu_morphogo_2020,
	series = {智微信科 - 2},
	title = {Morphogo: {An} {Automatic} {Bone} {Marrow} {Cell} {Classification} {System} on {Digital} {Images} {Analyzed} by {Artificial} {Intelligence}},
	volume = {64},
	issn = {0001-5547, 1938-2650},
	shorttitle = {Morphogo},
	url = {https://www.karger.com/Article/FullText/509524},
	doi = {10.1159/000509524},
	abstract = {\textbf{\textit{Introduction:}} The nucleated-cell differential count on the bone marrow aspirate smears is required for the clinical diagnosis of hematological malignancy. Manual bone marrow differential count is time consuming and lacks consistency. In this study, a novel artificial intelligence (AI)-based system was developed to perform cell automatic classification of bone marrow cells and determine its potential clinical applications. \textbf{\textit{Materials and Methods:}} Bone marrow aspirate smears were collected from the Xinqiao Hospital of Army Medical University. First, an automated analysis system (\textit{Morphogo}) scanned and generated whole digital images of bone marrow smears. Then, the nucleated marrow cells in the selected areas of the smears at a magnification of ×1,000 were analyzed by the software utilizing an AI-based platform. The cell classification results were further reviewed and confirmed independently by 2 experienced pathologists. The automatic cell classification performance of the system was evaluated using 3 categories: accuracy, sensitivity, and specificity. Correlation coefficients and linear regression equations between automatic cell classification by the AI-based system and concurrent manual differential count were calculated. \textbf{\textit{Results:}} In 230 cases, the classification accuracy was above 85.7\% for hematopoietic lineage cells. Averages of sensitivity and specificity of the system were found to be 69.4 and 97.2\%, respectively. The differential cell percentage of the automated count based on 200–500 cell counts was correlated with differential cell percentage provided by the pathologists for granulocytes, erythrocytes, and lymphocytes (\textit{r} ≥ 0.762, \textit{p} \&\#x3c; 0.001). \textbf{\textit{Discussion/Conclusion:}} This pilot study confirmed that the \textit{Morphogo} system is a reliable tool for automatic bone marrow cell differential count analysis and has potential for clinical applications. Current ongoing large-scale multicenter validation studies will provide more information to further confirm the clinical utility of the system.},
	language = {en},
	number = {6},
	urldate = {2021-12-06},
	journal = {Acta Cytologica},
	author = {Fu, Xinyan and Fu, May and Li, Qiang and Peng, Xiangui and Lu, Ju and Fang, Fengqi and Chen, Mingyi},
	year = {2020},
	pages = {588--596},
	file = {Fu 等。 - 2020 - Morphogo An Automatic Bone Marrow Cell Classifica.pdf:/home/red0orange/Zotero/storage/CHUFYEYA/Fu 等。 - 2020 - Morphogo An Automatic Bone Marrow Cell Classifica.pdf:application/pdf},
}

@article{shen_interpretable_2021,
	series = {属性分类参考文章},
	title = {Interpretable {Compositional} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2107.04474},
	abstract = {The reasonable deﬁnition of semantic interpretability presents the core challenge in explainable AI. This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable compositional CNN, in order to learn ﬁlters that encode meaningful visual patterns in intermediate convolutional layers. In a compositional CNN, each ﬁlter is supposed to consistently represent a speciﬁc compositional object part or image region with a clear meaning. The compositional CNN learns from image labels for classiﬁcation without any annotations of parts or regions for supervision. Our method can be broadly applied to different types of CNNs. Experiments have demonstrated the effectiveness of our method. The code will be released when the paper is accepted.},
	language = {en},
	urldate = {2021-11-28},
	journal = {arXiv:2107.04474 [cs]},
	author = {Shen, Wen and Wei, Zhihua and Huang, Shikun and Zhang, Binbin and Fan, Jiaqi and Zhao, Ping and Zhang, Quanshi},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.04474},
	file = {Shen 等。 - 2021 - Interpretable Compositional Convolutional Neural N.pdf:/home/red0orange/Zotero/storage/4PXLWB83/Shen 等。 - 2021 - Interpretable Compositional Convolutional Neural N.pdf:application/pdf},
}

@article{zhou_brief_2018,
	title = {A brief introduction to weakly supervised learning},
	volume = {5},
	issn = {2095-5138},
	url = {https://doi.org/10.1093/nsr/nwx106},
	doi = {10.1093/nsr/nwx106},
	abstract = {Supervised learning techniques construct predictive models by learning from a large number of training examples, where each training example has a label indicating its ground-truth output. Though current techniques have achieved great success, it is noteworthy that in many tasks it is difficult to get strong supervision information like fully ground-truth labels due to the high cost of the data-labeling process. Thus, it is desirable for machine-learning techniques to work with weak supervision. This article reviews some research progress of weakly supervised learning, focusing on three typical types of weak supervision: incomplete supervision, where only a subset of training data is given with labels; inexact supervision, where the training data are given with only coarse-grained labels; and inaccurate supervision, where the given labels are not always ground-truth.},
	number = {1},
	urldate = {2021-11-21},
	journal = {National Science Review},
	author = {Zhou, Zhi-Hua},
	month = jan,
	year = {2018},
	keywords = {综述},
	pages = {44--53},
	file = {2018-A brief introduction to weakly supervised learning.pdf:/home/red0orange/Zotero/storage/5PA8BD5L/2018-A brief introduction to weakly supervised learning.pdf:application/pdf},
}

@article{liu_co-correcting_2021-1,
	title = {Co-{Correcting}: {Noise}-tolerant {Medical} {Image} {Classification} via mutual {Label} {Correction}},
	shorttitle = {Co-{Correcting}},
	url = {https://arxiv.org/abs/2109.05159v1},
	doi = {10.1109/TMI.2021.3091178},
	abstract = {With the development of deep learning, medical image classification has been significantly improved. However, deep learning requires massive data with labels. While labeling the samples by human experts is expensive and time-consuming, collecting labels from crowd-sourcing suffers from the noises which may degenerate the accuracy of classifiers. Therefore, approaches that can effectively handle label noises are highly desired. Unfortunately, recent progress on handling label noise in deep learning has gone largely unnoticed by the medical image. To fill the gap, this paper proposes a noise-tolerant medical image classification framework named Co-Correcting, which significantly improves classification accuracy and obtains more accurate labels through dual-network mutual learning, label probability estimation, and curriculum label correcting. On two representative medical image datasets and the MNIST dataset, we test six latest Learning-with-Noisy-Labels methods and conduct comparative studies. The experiments show that Co-Correcting achieves the best accuracy and generalization under different noise ratios in various tasks. Our project can be found at: https://github.com/JiarunLiu/Co-Correcting.},
	language = {en},
	urldate = {2021-11-21},
	author = {Liu, Jiarun and Li, Ruirui and Sun, Chuan},
	month = sep,
	year = {2021},
	keywords = {一区},
	file = {2021-Co-Correcting.pdf:/home/red0orange/Zotero/storage/2DRCL938/2021-Co-Correcting.pdf:application/pdf;Snapshot:/home/red0orange/Zotero/storage/STJLAZZG/2109.html:text/html},
}

@article{han_co-teaching_2018,
	title = {Co-teaching: {Robust} {Training} of {Deep} {Neural} {Networks} with {Extremely} {Noisy} {Labels}},
	shorttitle = {Co-teaching},
	url = {http://arxiv.org/abs/1804.06872},
	abstract = {Deep learning with noisy labels is practically challenging, as the capacity of deep models is so high that they can totally memorize these noisy labels sooner or later during training. Nonetheless, recent studies on the memorization effects of deep neural networks show that they would first memorize training data of clean labels and then those of noisy labels. Therefore in this paper, we propose a new deep learning paradigm called Co-teaching for combating with noisy labels. Namely, we train two deep neural networks simultaneously, and let them teach each other given every mini-batch: firstly, each network feeds forward all data and selects some data of possibly clean labels; secondly, two networks communicate with each other what data in this mini-batch should be used for training; finally, each network back propagates the data selected by its peer network and updates itself. Empirical results on noisy versions of MNIST, CIFAR-10 and CIFAR-100 demonstrate that Co-teaching is much superior to the state-of-the-art methods in the robustness of trained deep models.},
	urldate = {2021-11-16},
	journal = {arXiv:1804.06872 [cs, stat]},
	author = {Han, Bo and Yao, Quanming and Yu, Xingrui and Niu, Gang and Xu, Miao and Hu, Weihua and Tsang, Ivor and Sugiyama, Masashi},
	month = oct,
	year = {2018},
	note = {arXiv: 1804.06872},
	file = {arXiv Fulltext PDF:/home/red0orange/Zotero/storage/4HE7QIGN/Han 等。 - 2018 - Co-teaching Robust Training of Deep Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/red0orange/Zotero/storage/WE3FZP58/1804.html:text/html;Han 等。 - 2018 - Co-teaching Robust Training of Deep Neural Networ.pdf:/home/red0orange/Zotero/storage/X24TKNK3/Han 等。 - 2018 - Co-teaching Robust Training of Deep Neural Networ.pdf:application/pdf},
}

@article{wei_understanding_2021,
	title = {Understanding {Generalized} {Label} {Smoothing} when {Learning} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2106.04149},
	abstract = {Label smoothing (LS) is an arising learning paradigm that uses the positively weighted average of both the hard training labels and uniformly distributed soft labels. It was shown that LS serves as a regularizer for training data with hard labels and therefore improves the generalization of the model. Later it was reported LS even helps with improving robustness when learning with noisy labels. However, we observe that the advantage of LS vanishes when we operate in a high label noise regime. Puzzled by the observation, we proceeded to discover that several proposed learning-with-noisy-labels solutions in the literature instead relate more closely to negative label smoothing (NLS), which defines as using a negative weight to combine the hard and soft labels! We show that NLS differs substantially from LS in their achieved model confidence. To differentiate the two cases, we will call LS the positive label smoothing (PLS), and this paper unifies PLS and NLS into generalized label smoothing (GLS). We provide understandings for the properties of GLS when learning with noisy labels. Among other established properties, we theoretically show NLS is considered more beneficial when the label noise rates are high. We provide extensive experimental results on multiple benchmarks to support our findings too.},
	urldate = {2021-11-16},
	journal = {arXiv:2106.04149 [cs]},
	author = {Wei, Jiaheng and Liu, Hangyu and Liu, Tongliang and Niu, Gang and Liu, Yang},
	month = oct,
	year = {2021},
	note = {arXiv: 2106.04149},
	file = {arXiv Fulltext PDF:/home/red0orange/Zotero/storage/65SS6Q2Q/Wei 等。 - 2021 - Understanding Generalized Label Smoothing when Lea.pdf:application/pdf;arXiv.org Snapshot:/home/red0orange/Zotero/storage/CHU9G7JG/2106.html:text/html},
}

@article{karimi_deep_2020,
	title = {Deep learning with noisy labels: {Exploring} techniques and remedies in medical image analysis},
	volume = {65},
	issn = {1361-8415},
	shorttitle = {Deep learning with noisy labels},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841520301237},
	doi = {10.1016/j.media.2020.101759},
	abstract = {Supervised training of deep learning models requires large labeled datasets. There is a growing interest in obtaining such datasets for medical image analysis applications. However, the impact of label noise has not received sufficient attention. Recent studies have shown that label noise can significantly impact the performance of deep learning models in many machine learning and computer vision applications. This is especially concerning for medical applications, where datasets are typically small, labeling requires domain expertise and suffers from high inter- and intra-observer variability, and erroneous predictions may influence decisions that directly impact human health. In this paper, we first review the state-of-the-art in handling label noise in deep learning. Then, we review studies that have dealt with label noise in deep learning for medical image analysis. Our review shows that recent progress on handling label noise in deep learning has gone largely unnoticed by the medical image analysis community. To help achieve a better understanding of the extent of the problem and its potential remedies, we conducted experiments with three medical imaging datasets with different types of label noise, where we investigated several existing strategies and developed new methods to combat the negative effect of label noise. Based on the results of these experiments and our review of the literature, we have made recommendations on methods that can be used to alleviate the effects of different types of label noise on deep models trained for medical image analysis. We hope that this article helps the medical image analysis researchers and developers in choosing and devising new techniques that effectively handle label noise in deep learning.},
	language = {en},
	urldate = {2021-11-17},
	journal = {Medical Image Analysis},
	author = {Karimi, Davood and Dou, Haoran and Warfield, Simon K. and Gholipour, Ali},
	month = oct,
	year = {2020},
	keywords = {综述},
	pages = {101759},
	file = {已接受版本:/home/red0orange/Zotero/storage/EPKBX5B4/Karimi 等。 - 2020 - Deep learning with noisy labels Exploring techniq.pdf:application/pdf},
}

@inproceedings{yu_how_2019,
	series = {Co-teaching+},
	title = {How does {Disagreement} {Help} {Generalization} against {Label} {Corruption}?},
	url = {https://proceedings.mlr.press/v97/yu19b.html},
	abstract = {Learning with noisy labels is one of the hottest problems in weakly-supervised learning. Based on memorization effects of deep neural networks, training on small-loss instances becomes very promising for handling noisy labels. This fosters the state-of-the-art approach "Co-teaching" that cross-trains two deep neural networks using the small-loss trick. However, with the increase of epochs, two networks converge to a consensus and Co-teaching reduces to the self-training MentorNet. To tackle this issue, we propose a robust learning paradigm called Co-teaching+, which bridges the "Update by Disagreement” strategy with the original Co-teaching. First, two networks feed forward and predict all data, but keep prediction disagreement data only. Then, among such disagreement data, each network selects its small-loss data, but back propagates the small-loss data from its peer network and updates its own parameters. Empirical results on benchmark datasets demonstrate that Co-teaching+ is much superior to many state-of-the-art methods in the robustness of trained models.},
	language = {en},
	urldate = {2021-11-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yu, Xingrui and Han, Bo and Yao, Jiangchao and Niu, Gang and Tsang, Ivor and Sugiyama, Masashi},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {7164--7173},
	file = {Full Text PDF:/home/red0orange/Zotero/storage/HLWA6C65/Yu 等。 - 2019 - How does Disagreement Help Generalization against .pdf:application/pdf;Supplementary PDF:/home/red0orange/Zotero/storage/QSZ7YB6N/Yu 等。 - 2019 - How does Disagreement Help Generalization against .pdf:application/pdf},
}

@inproceedings{xu_l_dmi_2019,
	title = {L\_DMI: {A} {Novel} {Information}-theoretic {Loss} {Function} for {Training} {Deep} {Nets} {Robust} to {Label} {Noise}},
	volume = {32},
	shorttitle = {L\_DMI},
	url = {https://proceedings.neurips.cc/paper/2019/hash/8a1ee9f2b7abe6e88d1a479ab6a42c5e-Abstract.html},
	urldate = {2021-11-16},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xu, Yilun and Cao, Peng and Kong, Yuqing and Wang, Yizhou},
	year = {2019},
	keywords = {顶会},
	file = {Full Text PDF:/home/red0orange/Zotero/storage/GKQL49D3/Xu 等。 - 2019 - L_DMI A Novel Information-theoretic Loss Function.pdf:application/pdf},
}

@article{northcutt_confident_2021,
	title = {Confident {Learning}: {Estimating} {Uncertainty} in {Dataset} {Labels}},
	shorttitle = {Confident {Learning}},
	url = {http://arxiv.org/abs/1911.00068},
	abstract = {Learning exists in the context of data, yet notions of conﬁdence typically focus on model predictions, not label quality. Conﬁdent learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with conﬁdence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present suﬃcient conditions where CL exactly ﬁnds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a speciﬁc data modality or model (e.g., we use CL to ﬁnd several label errors in the presumed error-free MNIST dataset and improve sentiment classiﬁcation on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.},
	language = {en},
	urldate = {2021-11-16},
	journal = {arXiv:1911.00068 [cs, stat]},
	author = {Northcutt, Curtis G. and Jiang, Lu and Chuang, Isaac L.},
	month = apr,
	year = {2021},
	note = {arXiv: 1911.00068},
	keywords = {不确定水平, 三区, Good Ideas},
	file = {Northcutt 等。 - 2021 - Confident Learning Estimating Uncertainty in Data.pdf:/home/red0orange/Zotero/storage/P2JZFCPA/Northcutt 等。 - 2021 - Confident Learning Estimating Uncertainty in Data.pdf:application/pdf},
}

@article{xu_deep_2019,
	title = {Deep {Regionlets}: {Blended} {Representation} and {Deep} {Learning} for {Generic} {Object} {Detection}},
	shorttitle = {Deep {Regionlets}},
	url = {http://arxiv.org/abs/1811.11318},
	abstract = {In this paper, we propose a novel object detection algorithm named "Deep Regionlets" by integrating deep neural networks and a conventional detection schema for accurate generic object detection. Motivated by the effectiveness of regionlets for modeling object deformations and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Speciﬁcally, given a detection bounding box proposal, the region selection network provides guidance on where to select sub-regions from which features can be learned from. An object proposal typically contains 3-16 sub-regions. The regionlet learning module focuses on local feature selection and transformations to alleviate the effects of appearance variations. To this end, we ﬁrst realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a “gating network" within the regionlet leaning module to enable instance dependent soft feature selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We present ablation studies and extensive experiments on the PASCAL VOC dataset and the Microsoft COCO dataset. The proposed method yields competitive performance over state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.},
	language = {en},
	urldate = {2021-10-30},
	journal = {arXiv:1811.11318 [cs]},
	author = {Xu, Hongyu and Lv, Xutao and Wang, Xiaoyu and Ren, Zhou and Bodla, Navaneeth and Chellappa, Rama},
	month = dec,
	year = {2019},
	note = {arXiv: 1811.11318},
	keywords = {顶会, Good Ideas},
	file = {Xu 等。 - 2019 - Deep Regionlets Blended Representation and Deep L.pdf:/home/red0orange/Zotero/storage/QR4HTSFZ/Xu 等。 - 2019 - Deep Regionlets Blended Representation and Deep L.pdf:application/pdf},
}

@article{kouzehkanan_raabin-wbc_2021,
	series = {数据集：},
	title = {Raabin-{WBC}: a large free access dataset of white blood cells from normal peripheral blood},
	shorttitle = {Raabin-{WBC}},
	doi = {10.1101/2021.05.02.442287},
	abstract = {The Raabin-WBC dataset can be used for different machine learning tasks such as classification, detection, segmentation, and localization, and it is shown how the generalization power of machine learning methods, especially deep neural networks, was affected by the mentioned diversity. Accurate and early detection of peripheral white blood cell anomalies plays a crucial role in the evaluation of an individual’s well-being. The emergence of new technologies such as artificial intelligence can be very effective in achieving this. In this regard, most of the state-of-the-art methods use deep neural networks. Data can significantly influence the performance and generalization power of machine learning approaches, especially deep neural networks. To that end, we collected a large free available dataset of white blood cells from normal peripheral blood samples called Raabin-WBC. Our dataset contains about 40000 white blood cells and artifacts (color spots). To reassure correct data, a significant number of cells were labeled by two experts, and the ground truth of nucleus and cytoplasm were extracted by experts for some cells (about 1145), as well. To provide the necessary diversity, various smears have been imaged. Hence, two different cameras and two different microscopes were used. The Raabin-WBC dataset can be used for different machine learning tasks such as classification, detection, segmentation, and localization. We also did some primary deep learning experiments on Raabin-WBC, and we showed how the generalization power of machine learning methods, especially deep neural networks, was affected by the mentioned diversity.},
	author = {Kouzehkanan, Zahra Mousavi and Saghari, Sepehr and Tavakoli, Eslam and Rostami, Peyman and Abaszadeh, Mohammadjavad and Mirzadeh, Farzaneh and Satlsar, Esmaeil Shahabi and Gheidishahran, Maryam and Gorgi, F. and Mohammadi, Saeed and Hosseini, Reshad},
	year = {2021},
	keywords = {dataset},
	file = {已提交版本:/home/red0orange/Zotero/storage/IBRBL37A/Kouzehkanan 等。 - 2021 - Raabin-WBC a large free access dataset of white b.pdf:application/pdf},
}

@article{huang_bone_2021,
	series = {最终论文},
	title = {Bone {Marrow} {Cell} {Recognition}: {Training} {Deep} {Object} {Detection} with {A} {New} {Loss} {Function}},
	shorttitle = {Bone {Marrow} {Cell} {Recognition}},
	url = {http://arxiv.org/abs/2110.12647},
	abstract = {For a long time, bone marrow cell morphology examination has been an essential tool for diagnosing blood diseases. However, it is still mainly dependent on the subjective diagnosis of experienced doctors, and there is no objective quantitative standard. Therefore, it is crucial to study a robust bone marrow cell detection algorithm for a quantitative automatic analysis system. Currently, due to the dense distribution of cells in the bone marrow smear and the diverse cell classes, the detection of bone marrow cells is difﬁcult. The existing bone marrow cell detection algorithms are still insufﬁcient for the automatic analysis system of bone marrow smears. This paper proposes a bone marrow cell detection algorithm based on the YOLOv5 network, trained by minimizing a novel loss function. The classiﬁcation method of bone marrow cell detection tasks is the basis of the proposed novel loss function. Since bone marrow cells are classiﬁed according to series and stages, part of the classes in adjacent stages are similar. The proposed novel loss function considers the similarity between bone marrow cell classes, increases the penalty for prediction errors between dissimilar classes, and reduces the penalty for prediction errors between similar classes. The results show that the proposed loss function effectively improves the algorithm’s performance, and the proposed bone marrow cell detection algorithm has achieved better performance than other cell detection algorithms.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:2110.12647 [cs]},
	author = {Huang, Dehao and Cheng, Jintao and Fan, Rui and Su, Zhihao and Ma, Qiongxiong and Li, Jie},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.12647},
	file = {Huang 等。 - 2021 - Bone Marrow Cell Recognition Training Deep Object.pdf:/home/red0orange/Zotero/storage/VBGIN885/Huang 等。 - 2021 - Bone Marrow Cell Recognition Training Deep Object.pdf:application/pdf},
}

@article{wang_salient_2021,
	series = {显著物体检测},
	title = {Salient {Object} {Detection} in the {Deep} {Learning} {Era}: {An} {In}-depth {Survey}},
	issn = {1939-3539},
	shorttitle = {Salient {Object} {Detection} in the {Deep} {Learning} {Era}},
	doi = {10.1109/TPAMI.2021.3051099},
	abstract = {As an essential problem in computer vision, salient object detection (SOD) has attracted an increasing amount of research attention over the years. Recent advances in SOD are predominantly led by deep learning-based solutions (named deep SOD). To enable an in-depth understanding of deep SOD, in this paper, we provide a comprehensive survey covering various aspects, ranging from algorithm taxonomy to unsolved issues. In particular, we first review deep SOD algorithms from different perspectives, including network architecture, level of supervision, learning paradigm, and object-/instance-level detection. Following that, we summarize and analyze existing SOD datasets and evaluation metrics. Then, we benchmark a large group of representative SOD models, and provide detailed analyses of the comparison results. Moreover, we study the performance of SOD algorithms under different attribute settings, which has not been thoroughly explored previously, by constructing a novel SOD dataset with rich attribute annotations covering various salient object types, challenging factors, and scene categories. We further analyze, for the first time in the field, the robustness of SOD models to random input perturbations and adversarial attacks. We also look into the generalization and difficulty of existing SOD datasets. Finally, we discuss several open issues of SOD and outline future research directions. All the saliency prediction maps, our constructed dataset with annotations, and codes for evaluation are publicly available at https://github.com/wenguanwang/SODsurvey.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Wenguan and Lai, Qiuxia and Fu, Huazhu and Shen, Jianbing and Ling, Haibin and Yang, Ruigang},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {1--1},
	file = {已提交版本:/home/red0orange/Zotero/storage/QTZ3AXEX/Wang 等。 - 2021 - Salient Object Detection in the Deep Learning Era.pdf:application/pdf;IEEE Xplore Abstract Record:/home/red0orange/Zotero/storage/HCDHAQBW/9320524.html:text/html},
}

@article{sharif_recognition_2020,
	title = {Recognition of {Different} {Types} of {Leukocytes} {Using} {YOLOv2} and {Optimized} {Bag}-of-{Features}},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2020.3021660},
	abstract = {White blood cells (WBCs) protect human body against different types of infections including fungal, parasitic, viral, and bacterial. The detection of abnormal regions in WBCs is a difficult task. Therefore a method is proposed for the localization of WBCs based on YOLOv2-Nucleus-Cytoplasm, which contains darkNet-19 as a basenetwork of the YOLOv2 model. In this model features are extracted from LeakyReLU-18 of darkNet-19 and supplied as an input to the YOLOv2 model. The YOLOv2-Nucleus-Cytoplasm model localizes and classifies the WBCs with maximum score labels. It also localize the WBCs into the blast and non-blast cells. After localization, the bag-of-features are extracted and optimized by using particle swarm optimization(PSO). The improved feature vector is fed to classifiers i.e., optimized naïve Bayes (O-NB) \& optimized discriminant analysis (O-DA) for WBCs classification. The experiments are performed on LISC, ALL-IDB1, and ALL-IDB2 datasets.},
	journal = {IEEE Access},
	author = {Sharif, Muhammad and Amin, Javaria and Siddiqa, Ayesha and Khan, Habib Ullah and Arshad Malik, Muhammad Sheraz and Anjum, Muhammad Almas and Kadry, Seifedine},
	year = {2020},
	note = {Conference Name: IEEE Access},
	keywords = {Good Ideas},
	pages = {167448--167459},
	file = {IEEE Xplore Abstract Record:/home/red0orange/Zotero/storage/CSNQY9CN/9186086.html:text/html;IEEE Xplore Full Text PDF:/home/red0orange/Zotero/storage/BX6FD3HE/Sharif 等。 - 2020 - Recognition of Different Types of Leukocytes Using.pdf:application/pdf},
}

@article{he_cyclegan_2020,
	series = {修复错标、漏标的细胞标注},
	title = {{CycleGAN} {With} an {Improved} {Loss} {Function} for {Cell} {Detection} {Using} {Partly} {Labeled} {Images}},
	volume = {24},
	issn = {2168-2194, 2168-2208},
	url = {https://ieeexplore.ieee.org/document/8972414/},
	doi = {10.1109/JBHI.2020.2970091},
	abstract = {The object detection, which has been widely applied in the biomedical ﬁeld already, is of real signiﬁcance but technically challenging. In practice, the object detection accuracy is vulnerable to labeling quality, which is usually not a big headache for simple algorithm or model veriﬁcation since there are a bunch of ideal public available datasets whose classes and tags are all well-marked. However, in real scenarios, image data is often partially or even incorrectly labeled. Particularly, in cell detection, this becomes a thorny issue since the labelling of the dataset is incomplete and inaccurate. To address this issue, we propose a data-augmentation algorithm that can generate full labeled cell image data from incomplete labeled ones. First of all, we randomly extract the labeled objects from raw cell images, and meanwhile, keep their corresponding position information. Next, we employ the framework of cycle-consistent adversarial network, but signiﬁcantly distinguished from the original one, to generate fully labeled data including both objects and backgrounds. We conduct extensive experiments on a blood cell classiﬁcation dataset called BCCD to evaluate our model, and experimental results show that our proposed method can successfully address the weak annotation problem and improve the performance of object detection.},
	language = {en},
	number = {9},
	urldate = {2021-10-28},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {He, Jin and Wang, Cong and Jiang, Dan and Li, Zhuo and Liu, Yangyi and Zhang, Tao},
	month = sep,
	year = {2020},
	keywords = {Good Ideas},
	pages = {2473--2480},
	file = {He 等。 - 2020 - CycleGAN With an Improved Loss Function for Cell D.pdf:/home/red0orange/Zotero/storage/5E9ZRGD4/He 等。 - 2020 - CycleGAN With an Improved Loss Function for Cell D.pdf:application/pdf},
}

@article{shakarami_fast_2021,
	series = {{优化YOLOv3的血细胞检测算法}},
	title = {A fast and yet efficient {YOLOv3} for blood cell detection},
	volume = {66},
	issn = {1746-8094},
	url = {https://www.sciencedirect.com/science/article/pii/S1746809421000926},
	doi = {10.1016/j.bspc.2021.102495},
	abstract = {These days, blood cell detection in microscopic images plays a vital role in cognition, the health of a patient. Since disease detection based on manual checking of blood cells is mostly time-consuming and full of errors, analysis of blood cells using object detectors can be considered as an effective tool. Hence, in this study, an object detector has been proposed which is used for detecting blood objects such as white blood cells, red blood cells, and platelets. This detector is called FED (Fast and Efficient YOLOv3) and it is a One-Stage detector, which is similar to YOLOv3, performs detection in three scales. For the purpose of increasing efficiency and flexibility, the proposed object detector utilizes the EfficientNet Convolutional Neural Network as the backbone effectiveness. Furthermore, the Dilated Convolution is indeed applied in order to increase receptive view of the backbone. In addition, the Depthwise Separable Convolution method is utilized to minimize the detector’s parametersand the Distance Intersection over Union is further used for bounding box regression. Besides, for increasing the performance, the Swish activation function is employed. The experiments are run on the BCCD dataset that the average precision of platelets, red blood cells, and white blood cells become 90.25\%, 80.41\%, and 98.92\%, respectively. The results of experiments and comparisons demonstrate that the proposed FED detector is more efficient than other existing studies for blood cell detection.},
	language = {en},
	urldate = {2021-10-28},
	journal = {Biomedical Signal Processing and Control},
	author = {Shakarami, Ashkan and Menhaj, Mohammad Bagher and Mahdavi-Hormat, Ali and Tarrah, Hadis},
	month = apr,
	year = {2021},
	pages = {102495},
	file = {Shakarami 等。 - 2021 - A fast and yet efficient YOLOv3 for blood cell det.pdf:/home/red0orange/Zotero/storage/8L25RZ6Q/Shakarami 等。 - 2021 - A fast and yet efficient YOLOv3 for blood cell det.pdf:application/pdf},
}

@inproceedings{chen_you_2021,
	title = {You {Only} {Look} {One}-{Level} {Feature}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Chen_You_Only_Look_One-Level_Feature_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-10-28},
	author = {Chen, Qiang and Wang, Yingming and Yang, Tong and Zhang, Xiangyu and Cheng, Jian and Sun, Jian},
	year = {2021},
	pages = {13039--13048},
	file = {Full Text PDF:/home/red0orange/Zotero/storage/Z6BDQ4D7/Chen 等。 - 2021 - You Only Look One-Level Feature.pdf:application/pdf;Snapshot:/home/red0orange/Zotero/storage/4IN2JGK6/Chen_You_Only_Look_One-Level_Feature_CVPR_2021_paper.html:text/html},
}

@article{wang_deep_2022,
	series = {用于整图的骨髓细胞检测算法},
	title = {Deep learning for bone marrow cell detection and classification on whole-slide images},
	volume = {75},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521003157},
	doi = {10.1016/j.media.2021.102270},
	abstract = {Bone marrow (BM) examination is an essential step in both diagnosing and managing numerous hematologic disorders. BM nucleated differential count (NDC) analysis, as part of BM examination, holds the most fundamental and crucial information. However, there are many challenges to perform automated BM NDC analysis on whole-slide images (WSIs), including large dimensions of data to process, complicated cell types with subtle differences. To the authors best knowledge, this is the first study on fully automatic BM NDC using WSIs with 40x objective magnification, which can replace traditional manual counting relying on light microscopy via oil-immersion 100x objective lens with a total 1000x magnification. In this study, we develop an efficient and fully automatic hierarchical deep learning framework for BM NDC WSI analysis in seconds. The proposed hierarchical framework consists of (1) a deep learning model for rapid localization of BM particles and cellular trails generating regions of interest (ROI) for further analysis, (2) a patch-based deep learning model for cell identification of 16 cell types, including megakaryocytes, mitotic cells, and four stages of erythroblasts which have not been demonstrated in previous studies before, and (3) a fast stitching model for integrating patch-based results and producing final outputs. In evaluation, the proposed method is firstly tested on a dataset with a total of 12,426 annotated cells using cross validation, achieving high recall and accuracy of 0.905 ± 0.078 and 0.989 ± 0.006, respectively, and taking only 44 seconds to perform BM NDC analysis for a WSI. To further examine the generalizability of our model, we conduct an evaluation on the second independent dataset with a total of 3005 cells, and the results show that the proposed method also obtains high recall and accuracy of 0.842 and 0.988, respectively. In comparison with the existing small-image-based benchmark methods, the proposed method demonstrates superior performance in recall, accuracy and computational time.},
	language = {en},
	urldate = {2021-10-28},
	journal = {Medical Image Analysis},
	author = {Wang, Ching-Wei and Huang, Sheng-Chuan and Lee, Yu-Ching and Shen, Yu-Jie and Meng, Shwu-Ing and Gaol, Jeff L.},
	month = jan,
	year = {2022},
	keywords = {一区},
	pages = {102270},
}

@inproceedings{woo_cbam_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{CBAM}: {Convolutional} {Block} {Attention} {Module}},
	isbn = {978-3-030-01234-2},
	shorttitle = {{CBAM}},
	doi = {10.1007/978-3-030-01234-2_1},
	abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {待探索, 模块},
	pages = {3--19},
	file = {已提交版本:/home/red0orange/Zotero/storage/3PE4R58Q/Woo 等。 - 2018 - CBAM Convolutional Block Attention Module.pdf:application/pdf},
}

@article{asgari_taghanaki_deep_2021,
	series = {医学图像的语义分割综述},
	title = {Deep semantic segmentation of natural and medical images: a review},
	volume = {54},
	issn = {1573-7462},
	shorttitle = {Deep semantic segmentation of natural and medical images},
	url = {https://doi.org/10.1007/s10462-020-09854-1},
	doi = {10.1007/s10462-020-09854-1},
	abstract = {The semantic image segmentation task consists of classifying each pixel of an image into an instance, where each instance corresponds to a class. This task is a part of the concept of scene understanding or better explaining the global context of an image. In the medical image analysis domain, image segmentation can be used for image-guided interventions, radiotherapy, or improved radiological diagnostics. In this review, we categorize the leading deep learning-based medical and non-medical image segmentation solutions into six main groups of deep architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, and multi-task methods and provide a comprehensive review of the contributions in each of these groups. Further, for each group, we analyze each variant of these groups and discuss the limitations of the current approaches and present potential future research directions for semantic image segmentation.},
	language = {en},
	number = {1},
	urldate = {2021-10-28},
	journal = {Artificial Intelligence Review},
	author = {Asgari Taghanaki, Saeid and Abhishek, Kumar and Cohen, Joseph Paul and Cohen-Adad, Julien and Hamarneh, Ghassan},
	month = jan,
	year = {2021},
	keywords = {一区, 待探索},
	pages = {137--178},
	file = {已提交版本:/home/red0orange/Zotero/storage/42XBVXRN/Asgari Taghanaki 等。 - 2021 - Deep semantic segmentation of natural and medical .pdf:application/pdf},
}

@incollection{lu_survey_2020,
	address = {Cham},
	series = {2020年外周血细胞检测综述},
	title = {A {Survey} on {Peripheral} {Blood} {Smear} {Analysis} {Using} {Deep} {Learning}},
	volume = {12068},
	isbn = {978-3-030-59829-7 978-3-030-59830-3},
	url = {http://link.springer.com/10.1007/978-3-030-59830-3_63},
	abstract = {Peripheral Blood Smear (PBS) analysis is a routine test carried out in specialized medical laboratories by specialists to assess some aspects of health status that are measured and assessed through blood. PBS analysis is prone to human errors and the usage of computerbased analysis can greatly enhance this process in terms of accuracy and cost. Despite the challenges, Deep Learning neural networks have shown impressive performance in this context. In this study the recent contributions are summarized along with the main challenges and future directions in this context.},
	language = {en},
	urldate = {2021-10-28},
	booktitle = {Pattern {Recognition} and {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Al-qudah, Rabiah and Suen, Ching Y.},
	editor = {Lu, Yue and Vincent, Nicole and Yuen, Pong Chi and Zheng, Wei-Shi and Cheriet, Farida and Suen, Ching Y.},
	year = {2020},
	doi = {10.1007/978-3-030-59830-3_63},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {725--738},
	file = {Al-qudah 和 Suen - 2020 - A Survey on Peripheral Blood Smear Analysis Using .pdf:/home/red0orange/Zotero/storage/FKAPINHT/Al-qudah 和 Suen - 2020 - A Survey on Peripheral Blood Smear Analysis Using .pdf:application/pdf},
}

@article{yan_real-time_2021,
	series = {借鉴写作思路},
	title = {A {Real}-{Time} {Apple} {Targets} {Detection} {Method} for {Picking} {Robot} {Based} on {Improved} {YOLOv5}},
	volume = {13},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/9/1619},
	doi = {10.3390/rs13091619},
	abstract = {The apple target recognition algorithm is one of the core technologies of the apple picking robot. However, most of the existing apple detection algorithms cannot distinguish between the apples that are occluded by tree branches and occluded by other apples. The apples, grasping end-effector and mechanical picking arm of the robot are very likely to be damaged if the algorithm is directly applied to the picking robot. Based on this practical problem, in order to automatically recognize the graspable and ungraspable apples in an apple tree image, a light-weight apple targets detection method was proposed for picking robot using improved YOLOv5s. Firstly, BottleneckCSP module was improved designed to BottleneckCSP-2 module which was used to replace the BottleneckCSP module in backbone architecture of original YOLOv5s network. Secondly, SE module, which belonged to the visual attention mechanism network, was inserted to the proposed improved backbone network. Thirdly, the bonding fusion mode of feature maps, which were inputs to the target detection layer of medium size in the original YOLOv5s network, were improved. Finally, the initial anchor box size of the original network was improved. The experimental results indicated that the graspable apples, which were unoccluded or only occluded by tree leaves, and the ungraspable apples, which were occluded by tree branches or occluded by other fruits, could be identiﬁed effectively using the proposed improved network model in this study. Speciﬁcally, the recognition recall, precision, mAP and F1 were 91.48\%, 83.83\%, 86.75\% and 87.49\%, respectively. The average recognition time was 0.015 s per image. Contrasted with original YOLOv5s, YOLOv3, YOLOv4 and EfﬁcientDet-D0 model, the mAP of the proposed improved YOLOv5s model increased by 5.05\%, 14.95\%, 4.74\% and 6.75\% respectively, the size of the model compressed by 9.29\%, 94.6\%, 94.8\% and 15.3\% respectively. The average recognition speeds per image of the proposed improved YOLOv5s model were 2.53, 1.13 and 3.53 times of EfﬁcientDet-D0, YOLOv4 and YOLOv3 and model, respectively. The proposed method can provide technical support for the real-time accurate detection of multiple fruit targets for the apple picking robot.},
	language = {en},
	number = {9},
	urldate = {2021-10-28},
	journal = {Remote Sensing},
	author = {Yan, Bin and Fan, Pan and Lei, Xiaoyan and Liu, Zhijie and Yang, Fuzeng},
	month = apr,
	year = {2021},
	pages = {1619},
	file = {Yan 等。 - 2021 - A Real-Time Apple Targets Detection Method for Pic.pdf:/home/red0orange/Zotero/storage/6CHQIP2L/Yan 等。 - 2021 - A Real-Time Apple Targets Detection Method for Pic.pdf:application/pdf},
}

@article{redmon_yolov3_2018,
	title = {{YOLOv3}: {An} {Incremental} {Improvement}},
	shorttitle = {{YOLOv3}},
	url = {http://arxiv.org/abs/1804.02767},
	abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that’s pretty swell. It’s a little bigger than last time but more accurate. It’s still fast though, don’t worry. At 320 × 320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 AP50 in 51 ms on a Titan X, compared to 57.5 AP50 in 198 ms by RetinaNet, similar performance but 3.8× faster. As always, all the code is online at https://pjreddie.com/yolo/.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1804.02767 [cs]},
	author = {Redmon, Joseph and Farhadi, Ali},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02767},
	file = {Redmon 和 Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:/home/red0orange/Zotero/storage/6BZ23WPB/Redmon 和 Farhadi - 2018 - YOLOv3 An Incremental Improvement.pdf:application/pdf},
}

@article{bochkovskiy_yolov4_2020,
	title = {{YOLOv4}: {Optimal} {Speed} and {Accuracy} of {Object} {Detection}},
	shorttitle = {{YOLOv4}},
	url = {http://arxiv.org/abs/2004.10934},
	abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justiﬁcation of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of ∼65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:2004.10934 [cs, eess]},
	author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10934},
	file = {Bochkovskiy 等。 - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:/home/red0orange/Zotero/storage/9KYCWFNV/Bochkovskiy 等。 - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf:application/pdf},
}

@article{tian_apple_2019,
	series = {参考写作思路},
	title = {Apple detection during different growth stages in orchards using the improved {YOLO}-{V3} model},
	volume = {157},
	issn = {01681699},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016816991831528X},
	doi = {10.1016/j.compag.2019.01.012},
	abstract = {Real-time detection of apples in orchards is one of the most important methods for judging growth stages of apples and estimating yield. The size, colour, cluster density, and other growth characteristics of apples change as they grow. Traditional detection methods can only detect apples during a particular growth stage, but these methods cannot be adapted to diﬀerent growth stages using the same model. We propose an improved YOLO-V3 model for detecting apples during diﬀerent growth stages in orchards with ﬂuctuating illumination, complex backgrounds, overlapping apples, and branches and leaves. Images of young apples, expanding apples, and ripe apples are initially collected. These images are subsequently augmented using rotation transformation, colour balance transformation, brightness transformation, and blur processing. The augmented images are used to create training sets. The DenseNet method is used to process feature layers with low resolution in the YOLO-V3 network. This eﬀectively enhances feature propagation, promotes feature reuse, and improves network performance. After training the model, the performance of the trained model is tested on a test dataset. The test results show that the proposed YOLOV3-dense model is superior to the original YOLO-V3 model and the Faster R-CNN with VGG16 net model, which is the state-of-art fruit detection model. The average detection time of the model is 0.304 s per frame at 3000 × 3000 resolution, which can provide real-time detection of apples in orchards. Moreover, the YOLOV3-dense model can eﬀectively provide apple detection under overlapping apples and occlusion conditions, and can be applied in the actual environment of orchards.},
	language = {en},
	urldate = {2021-10-28},
	journal = {Computers and Electronics in Agriculture},
	author = {Tian, Yunong and Yang, Guodong and Wang, Zhe and Wang, Hao and Li, En and Liang, Zize},
	month = feb,
	year = {2019},
	pages = {417--426},
	file = {Tian 等。 - 2019 - Apple detection during different growth stages in .pdf:/home/red0orange/Zotero/storage/APXFUB8B/Tian 等。 - 2019 - Apple detection during different growth stages in .pdf:application/pdf},
}

@article{chen_research_2019,
	series = {参考绘图思路},
	title = {Research on {Recognition} {Method} of {Electrical} {Components} {Based} on {YOLO} {V3}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8886369/},
	doi = {10.1109/ACCESS.2019.2950053},
	abstract = {The reliability of electrical components affects the stable operation of the power system. Electrical components inspection has long been important issues in the intelligent power system. The main problems of traditional recognition methods of electrical components are low detection accuracy and poor real-time performance, which are challenging to extract necessary features from the inspection images. This paper proposes a way to detect the electrical components in the Unmanned Aerial Vehicle (UAV) inspection image based on You Only Look Once (YOLO) V3 algorithm. Due to some of the inspection images are not clear, which result in the reduction of the available dataset. On this basis, we adopt Super-Resolution Convolutional Neural Network (SRCNN) to realize super-resolution reconstruction on the blurred image, which achieves the expansion of the dataset. We compare the performance of the proposed method with other popular recognition methods. The results of experiment verify the effectiveness of the proposed method, and the technique reaches high recognition accuracy, good robustness, and strong real-time performance for UAV power inspection system.},
	language = {en},
	urldate = {2021-10-28},
	journal = {IEEE Access},
	author = {Chen, Haipeng and He, Zhentao and Shi, Bowen and Zhong, Tie},
	year = {2019},
	pages = {157818--157829},
	file = {Chen 等。 - 2019 - Research on Recognition Method of Electrical Compo.pdf:/home/red0orange/Zotero/storage/WSKG2GSM/Chen 等。 - 2019 - Research on Recognition Method of Electrical Compo.pdf:application/pdf},
}

@inproceedings{abdeltawab_automatic_2019,
	address = {Abu Dhabi, United Arab Emirates},
	series = {参考绘图、 写作思路},
	title = {Automatic {Segmentation} and {Functional} {Assessment} of the {Left} {Ventricle} using {U}-net {Fully} {Convolutional} {Network}},
	isbn = {978-1-72813-868-8},
	url = {https://ieeexplore.ieee.org/document/9010123/},
	doi = {10.1109/IST48021.2019.9010123},
	abstract = {A new method for the automatic segmentation and quantitative assessment of the left ventricle (LV) is proposed in this paper. The method is composed of two steps. First, a fully convolutional U-net is used for the segmentation of the epi- and endo-cardial boundaries of the LV from cine MR images. This step incorporates a novel loss function that accounts for the class imbalance problem caused by the binary cross entropy (BCE) loss function. Our novel loss function maximizes the segmentation accuracy and penalizes the effect of the class-imbalance caused by BCE. In the second step, the ventricular volume curves are constructed from which LV function parameter is estimated (i.e., ejection fraction). Our method demonstrated a statistical signiﬁcance in the segmentation of the epi- and endo-cardial boundaries (Dice score of 0.94 and 0.96, respectively) compared with the BCE loss (Dice score of 0.89 and 0.86, respectively). Furthermore, a high positive correlation of 0.97 between the estimated ejection fraction and the gold standard was obtained.},
	language = {en},
	urldate = {2021-10-28},
	booktitle = {2019 {IEEE} {International} {Conference} on {Imaging} {Systems} and {Techniques} ({IST})},
	publisher = {IEEE},
	author = {Abdeltawab, Hisham and Khalifa, Fahmi and Taher, Fatma and Beache, Garth and Mohamed, Tamer and Elmaghraby, Adel and Ghazal, Mohammed and Keynton, Robert and El-Baz, Ayman},
	month = dec,
	year = {2019},
	pages = {1--6},
	file = {Abdeltawab 等。 - 2019 - Automatic Segmentation and Functional Assessment o.pdf:/home/red0orange/Zotero/storage/4EHGBCM9/Abdeltawab 等。 - 2019 - Automatic Segmentation and Functional Assessment o.pdf:application/pdf},
}

@article{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	language = {en},
	urldate = {2021-10-27},
	journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2016},
	pages = {779--788},
	file = {Redmon 等。 - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:/home/red0orange/Zotero/storage/MEEW6AZJ/Redmon 等。 - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf},
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {60},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3065386 http://dx.doi.org/10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	number = {6},
	journal = {Commun. ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
}

@article{xie_unsupervised_2016,
	series = {{DEC}},
	title = {Unsupervised {Deep} {Embedding} for {Clustering} {Analysis}},
	url = {http://arxiv.org/abs/1511.06335},
	abstract = {Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show signiﬁcant improvement over state-of-the-art methods.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:1511.06335 [cs]},
	author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
	month = may,
	year = {2016},
	note = {00000 
arXiv: 1511.06335},
	file = {Xie 等。 - 2016 - Unsupervised Deep Embedding for Clustering Analysi.pdf:/home/red0orange/Zotero/storage/MBZVYVL3/Xie 等。 - 2016 - Unsupervised Deep Embedding for Clustering Analysi.pdf:application/pdf},
}

@article{shu_modeling_2021,
	series = {{第一版看的GRN}},
	title = {Modeling gene regulatory networks using neural network architectures},
	volume = {1},
	issn = {2662-8457},
	url = {http://www.nature.com/articles/s43588-021-00099-8},
	doi = {10.1038/s43588-021-00099-8},
	language = {en},
	number = {7},
	urldate = {2021-09-17},
	journal = {Nature Computational Science},
	author = {Shu, Hantao and Zhou, Jingtian and Lian, Qiuyu and Li, Han and Zhao, Dan and Zeng, Jianyang and Ma, Jianzhu},
	month = jul,
	year = {2021},
	note = {00000},
	pages = {491--501},
	file = {Shu 等。 - 2021 - Modeling gene regulatory networks using neural net.pdf:/home/red0orange/Zotero/storage/T2BAZ565/Shu 等。 - 2021 - Modeling gene regulatory networks using neural net.pdf:application/pdf},
}

@article{aibar_scenic_2017,
	series = {{SCENIC}：基因调控网络},
	title = {{SCENIC}: single-cell regulatory network inference and clustering},
	volume = {14},
	issn = {1548-7091, 1548-7105},
	shorttitle = {{SCENIC}},
	url = {http://www.nature.com/articles/nmeth.4463},
	doi = {10.1038/nmeth.4463},
	abstract = {Although single-cell RNA-seq is revolutionizing biology, data interpretation remains a challenge. We present SCENIC for the simultaneous reconstruction of gene regulatory networks and identification of cell states. We apply SCENIC to a compendium of single-cell data from tumors and brain, and demonstrate that the genomic regulatory code can be exploited to guide the identification of transcription factors and cell states. SCENIC provides critical biological insights into the mechanisms driving cellular heterogeneity.},
	language = {en},
	number = {11},
	urldate = {2021-09-27},
	journal = {Nature Methods},
	author = {Aibar, Sara and González-Blas, Carmen Bravo and Moerman, Thomas and Huynh-Thu, Vân Anh and Imrichova, Hana and Hulselmans, Gert and Rambow, Florian and Marine, Jean-Christophe and Geurts, Pierre and Aerts, Jan and van den Oord, Joost and Atak, Zeynep Kalender and Wouters, Jasper and Aerts, Stein},
	month = nov,
	year = {2017},
	note = {00000},
	pages = {1083--1086},
	file = {Aibar 等。 - 2017 - SCENIC single-cell regulatory network inference a.pdf:/home/red0orange/Zotero/storage/JZ7ZZDM9/Aibar 等。 - 2017 - SCENIC single-cell regulatory network inference a.pdf:application/pdf},
}

@article{van_de_sande_scalable_2020,
	series = {{pySCENIC}：基因调控网络},
	title = {A scalable {SCENIC} workflow for single-cell gene regulatory network analysis},
	volume = {15},
	issn = {1754-2189, 1750-2799},
	url = {http://www.nature.com/articles/s41596-020-0336-2},
	doi = {10.1038/s41596-020-0336-2},
	language = {en},
	number = {7},
	urldate = {2021-09-27},
	journal = {Nature Protocols},
	author = {Van de Sande, Bram and Flerin, Christopher and Davie, Kristofer and De Waegeneer, Maxime and Hulselmans, Gert and Aibar, Sara and Seurinck, Ruth and Saelens, Wouter and Cannoodt, Robrecht and Rouchon, Quentin and Verbeiren, Toni and De Maeyer, Dries and Reumers, Joke and Saeys, Yvan and Aerts, Stein},
	month = jul,
	year = {2020},
	note = {00000},
	pages = {2247--2276},
	file = {Van de Sande 等。 - 2020 - A scalable SCENIC workflow for single-cell gene re.pdf:/home/red0orange/Zotero/storage/IRZZKCSS/Van de Sande 等。 - 2020 - A scalable SCENIC workflow for single-cell gene re.pdf:application/pdf},
}

@article{noauthor_deepdrim_nodate,
	series = {{DeepDRIM}：使用单细胞 {RNA}-seq 数据重建细胞类型特异性基因调控网络的深度神经网络},
	title = {{DeepDRIM}: a deep neural network to reconstruct cell-type-specific gene regulatory network using single-cell {RNA}-{Seq} {Data}},
	language = {en},
	note = {00001},
	pages = {21},
	file = {DeepDRIM a deep neural network to reconstruct cel.pdf:/home/red0orange/Zotero/storage/RMF7D8M8/DeepDRIM a deep neural network to reconstruct cel.pdf:application/pdf},
}

@article{zhao_comprehensive_2021,
	series = {基因调控网络方法综述},
	title = {A comprehensive overview and critical evaluation of gene regulatory network inference technologies},
	volume = {22},
	issn = {1467-5463, 1477-4054},
	url = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbab009/6128842},
	doi = {10.1093/bib/bbab009},
	abstract = {Gene regulatory network (GRN) is the important mechanism of maintaining life process, controlling biochemical reaction and regulating compound level, which plays an important role in various organisms and systems. Reconstructing GRN can help us to understand the molecular mechanism of organisms and to reveal the essential rules of a large number of biological processes and reactions in organisms. Various outstanding network reconstruction algorithms use specific assumptions that affect prediction accuracy, in order to deal with the uncertainty of processing. In order to study why a certain method is more suitable for specific research problem or experimental data, we conduct research from model-based, information-based and machine learning-based method classifications. There are obviously different types of computational tools that can be generated to distinguish GRNs. Furthermore, we discuss several classical, representative and latest methods in each category to analyze core ideas, general steps, characteristics, etc. We compare the performance of state-of-the-art GRN reconstruction technologies on simulated networks and real networks under different scaling conditions. Through standardized performance metrics and common benchmarks, we quantitatively evaluate the stability of various methods and the sensitivity of the same algorithm applying to different scaling networks. The aim of this study is to explore the most appropriate method for a specific GRN, which helps biologists and medical scientists in discovering potential drug targets and identifying cancer biomarkers.},
	language = {en},
	number = {5},
	urldate = {2021-09-27},
	journal = {Briefings in Bioinformatics},
	author = {Zhao, Mengyuan and He, Wenying and Tang, Jijun and Zou, Quan and Guo, Fei},
	month = sep,
	year = {2021},
	note = {00000},
	pages = {bbab009},
	file = {Zhao 等。 - 2021 - A comprehensive overview and critical evaluation o.pdf:/home/red0orange/Zotero/storage/3UE9BCNP/Zhao 等。 - 2021 - A comprehensive overview and critical evaluation o.pdf:application/pdf},
}

@article{chen_deep_2020,
	title = {Deep soft {K}-means clustering with self-training for single-cell {RNA} sequence data},
	volume = {2},
	issn = {2631-9268},
	url = {https://academic.oup.com/nargab/article/doi/10.1093/nargab/lqaa039/5843803},
	doi = {10.1093/nargab/lqaa039},
	abstract = {Single-cell RNA sequencing (scRNA-seq) allows researchers to study cell heterogeneity at the cellular level. A crucial step in analyzing scRNA-seq data is to cluster cells into subpopulations to facilitate subsequent downstream analysis. However, frequent dropout events and increasing size of scRNA-seq data make clustering such high-dimensional, sparse and massive transcriptional expression proﬁles challenging. Although some existing deep learningbased clustering algorithms for single cells combine dimensionality reduction with clustering, they either ignore the distance and afﬁnity constraints between similar cells or make some additional latent space assumptions like mixture Gaussian distribution, failing to learn cluster-friendly low-dimensional space. Therefore, in this paper, we combine the deep learning technique with the use of a denoising autoencoder to characterize scRNA-seq data while propose a soft self-training K-means algorithm to cluster the cell population in the learned latent space. The self-training procedure can effectively aggregate the similar cells and pursue more cluster-friendly latent space. Our method, called ‘scziDesk’, alternately performs data compression, data reconstruction and soft clustering iteratively, and the results exhibit excellent compatibility and robustness in both simulated and real data. Moreover, our proposed method has perfect scalability in line with cell size on largescale datasets.},
	language = {en},
	number = {2},
	urldate = {2021-09-18},
	journal = {NAR Genomics and Bioinformatics},
	author = {Chen, Liang and Wang, Weinan and Zhai, Yuyao and Deng, Minghua},
	month = jun,
	year = {2020},
	note = {00000},
	pages = {lqaa039},
	file = {Chen 等。 - 2020 - Deep soft K-means clustering with self-training fo.pdf:/home/red0orange/Zotero/storage/VXXS5KNX/Chen 等。 - 2020 - Deep soft K-means clustering with self-training fo.pdf:application/pdf},
}

@article{smith_super-convergence_2018,
	series = {{OneCycleLR}},
	title = {Super-{Convergence}: {Very} {Fast} {Training} of {Neural} {Networks} {Using} {Large} {Learning} {Rates}},
	shorttitle = {Super-{Convergence}},
	url = {http://arxiv.org/abs/1708.07120},
	abstract = {In this paper, we describe a phenomenon, which we named “super-convergence”, where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simpliﬁcation of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures to replicate this work will be made available upon publication.},
	language = {en},
	urldate = {2021-09-16},
	journal = {arXiv:1708.07120 [cs, stat]},
	author = {Smith, Leslie N. and Topin, Nicholay},
	month = may,
	year = {2018},
	note = {00000 
arXiv: 1708.07120},
	keywords = {Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Smith 和 Topin - 2018 - Super-Convergence Very Fast Training of Neural Ne.pdf:/home/red0orange/Zotero/storage/X75DPFYJ/Smith 和 Topin - 2018 - Super-Convergence Very Fast Training of Neural Ne.pdf:application/pdf},
}

@article{loshchilov_sgdr_2017,
	series = {{SGDR}},
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {http://arxiv.org/abs/1608.03983},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	language = {en},
	urldate = {2021-09-16},
	journal = {arXiv:1608.03983 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = may,
	year = {2017},
	note = {00000 
arXiv: 1608.03983},
	keywords = {Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {Loshchilov 和 Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:/home/red0orange/Zotero/storage/35AF5RI5/Loshchilov 和 Hutter - 2017 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf},
}

@article{smith_cyclical_2017,
	series = {{CycleLR}},
	title = {Cyclical {Learning} {Rates} for {Training} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.01186},
	abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally ﬁnd the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of ﬁxed values achieves improved classiﬁcation accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
	language = {en},
	urldate = {2021-09-16},
	journal = {arXiv:1506.01186 [cs]},
	author = {Smith, Leslie N.},
	month = apr,
	year = {2017},
	note = {00000 
arXiv: 1506.01186},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf:/home/red0orange/Zotero/storage/WY4EHL4C/Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf:application/pdf},
}

@article{srivastava_dropout_nodate,
	series = {dropout},
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overﬁtting}},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overﬁtting is a serious problem in such networks. Large networks are also slow to use, making it diﬃcult to deal with overﬁtting by combining the predictions of many diﬀerent large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of diﬀerent “thinned” networks. At test time, it is easy to approximate the eﬀect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This signiﬁcantly reduces overﬁtting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classiﬁcation and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoﬀrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	note = {00000},
	pages = {30},
	file = {Srivastava 等。 - Dropout A Simple Way to Prevent Neural Networks f.pdf:/home/red0orange/Zotero/storage/JU7XDJTD/Srivastava 等。 - Dropout A Simple Way to Prevent Neural Networks f.pdf:application/pdf},
}

@article{deisenroth_mathematics_nodate,
	title = {Mathematics for {Machine} {Learning}},
	language = {en},
	author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
	pages = {417},
	file = {Deisenroth 等。 - Mathematics for Machine Learning.pdf:/home/red0orange/Zotero/storage/5B7TSZN3/Deisenroth 等。 - Mathematics for Machine Learning.pdf:application/pdf},
}

@article{li_deep_2020-1,
	series = {批次效应评价指标},
	title = {Deep learning enables accurate clustering with batch effect removal in single-cell {RNA}-seq analysis},
	volume = {11},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-020-15851-3},
	doi = {10.1038/s41467-020-15851-3},
	abstract = {Abstract
            Single-cell RNA sequencing (scRNA-seq) can characterize cell types and states through unsupervised clustering, but the ever increasing number of cells and batch effect impose computational challenges. We present DESC, an unsupervised deep embedding algorithm that clusters scRNA-seq data by iteratively optimizing a clustering objective function. Through iterative self-learning, DESC gradually removes batch effects, as long as technical differences across batches are smaller than true biological variations. As a soft clustering algorithm, cluster assignment probabilities from DESC are biologically interpretable and can reveal both discrete and pseudotemporal structure of cells. Comprehensive evaluations show that DESC offers a proper balance of clustering accuracy and stability, has a small footprint on memory, does not explicitly require batch information for batch effect removal, and can utilize GPU when available. As the scale of single-cell studies continues to grow, we believe DESC will offer a valuable tool for biomedical researchers to disentangle complex cellular heterogeneity.},
	language = {en},
	number = {1},
	urldate = {2021-09-06},
	journal = {Nature Communications},
	author = {Li, Xiangjie and Wang, Kui and Lyu, Yafei and Pan, Huize and Zhang, Jingxiao and Stambolian, Dwight and Susztak, Katalin and Reilly, Muredach P. and Hu, Gang and Li, Mingyao},
	month = dec,
	year = {2020},
	note = {00000},
	pages = {2338},
	file = {Li 等。 - 2020 - Deep learning enables accurate clustering with bat.pdf:/home/red0orange/Zotero/storage/SHGCU2DY/Li 等。 - 2020 - Deep learning enables accurate clustering with bat.pdf:application/pdf},
}

@article{szegedy_rethinking_2015,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	url = {http://arxiv.org/abs/1512.00567},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error.},
	language = {en},
	urldate = {2021-09-02},
	journal = {arXiv:1512.00567 [cs]},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
	month = dec,
	year = {2015},
	note = {00000 
arXiv: 1512.00567},
	file = {Szegedy 等。 - 2015 - Rethinking the Inception Architecture for Computer.pdf:/home/red0orange/Zotero/storage/2N5VBE5S/Szegedy 等。 - 2015 - Rethinking the Inception Architecture for Computer.pdf:application/pdf},
}

@article{chen_semantic_2014,
	series = {W-{Net}，{CRF后处理}},
	title = {Semantic image segmentation with deep convolutional nets and fully connected crfs},
	journal = {arXiv preprint arXiv:1412.7062},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
	year = {2014},
	file = {1606.00915.pdf:/home/red0orange/Zotero/storage/FBY2RWCC/1606.00915.pdf:application/pdf;2014-Semantic image segmentation with deep convolutional nets and fully connected.pdf:/home/red0orange/Zotero/storage/7VMQ6N48/2014-Semantic image segmentation with deep convolutional nets and fully connected.pdf:application/pdf},
}

@article{arbelaez_contour_2011,
	series = {W-{Net后处理用的hei方法来源}},
	title = {Contour {Detection} and {Hierarchical} {Image} {Segmentation}},
	volume = {33},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/5557884/},
	doi = {10.1109/TPAMI.2010.161},
	abstract = {This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by userspecified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.},
	language = {en},
	number = {5},
	urldate = {2021-08-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Arbeláez, P and Maire, M and Fowlkes, C and Malik, J},
	month = may,
	year = {2011},
	note = {Number: 5
00000},
	pages = {898--916},
	file = {2011-Contour Detection and Hierarchical Image Segmentation.pdf:/home/red0orange/Zotero/storage/F4E7FJ7M/Arbeláez 等。 - 2011 - Contour Detection and Hierarchical Image Segmentat.pdf:application/pdf},
}

@article{jianbo_shi_normalized_2000,
	series = {N-cut},
	title = {Normalized cuts and image segmentation},
	volume = {22},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/868688/},
	doi = {10.1109/34.868688},
	abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
	language = {en},
	number = {8},
	urldate = {2021-08-30},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Jianbo Shi} and Malik, J.},
	month = aug,
	year = {2000},
	note = {Number: 8
00000},
	pages = {888--905},
	file = {2000-Normalized cuts and image segmentation.pdf:/home/red0orange/Zotero/storage/3X6NTG45/2000-Normalized cuts and image segmentation.pdf:application/pdf},
}

@incollection{ronneberger_u-net_2015,
	address = {Cham},
	series = {U-{Net}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	shorttitle = {U-{Net}},
	url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2021-08-30},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	doi = {10.1007/978-3-319-24574-4_28},
	note = {00000 
Series Title: Lecture Notes in Computer Science},
	pages = {234--241},
	file = {2015-U-Net.pdf:/home/red0orange/Zotero/storage/VF5X379L/2015-U-Net.pdf:application/pdf},
}

@inproceedings{chollet_xception_2017,
	address = {Honolulu, HI},
	series = {Depthwise {Separable} {Convolutions}},
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Xception},
	url = {http://ieeexplore.ieee.org/document/8099678/},
	doi = {10.1109/CVPR.2017.195},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and signiﬁcantly outperforms Inception V3 on a larger image classiﬁcation dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efﬁcient use of model parameters.},
	language = {en},
	urldate = {2021-08-27},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chollet, Francois},
	month = jul,
	year = {2017},
	note = {05942},
	pages = {1800--1807},
	file = {2017-Xception.pdf:/home/red0orange/Zotero/storage/Y4FJAAIU/2017-Xception.pdf:application/pdf},
}

@article{xia_w-net_2017,
	series = {W-{Net}，损失函数设计},
	title = {W-{Net}: {A} {Deep} {Model} for {Fully} {Unsupervised} {Image} {Segmentation}},
	shorttitle = {W-{Net}},
	url = {http://arxiv.org/abs/1711.08506},
	abstract = {While signiﬁcant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufﬁcient supervised pixel-level labels are difﬁcult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder—one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random ﬁeld smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.},
	language = {en},
	urldate = {2021-08-27},
	journal = {arXiv:1711.08506 [cs]},
	author = {Xia, Xide and Kulis, Brian},
	month = nov,
	year = {2017},
	note = {00124 
arXiv: 1711.08506},
	file = {2017-W-Net.pdf:/home/red0orange/Zotero/storage/UCBGFL87/2017-W-Net.pdf:application/pdf},
}

@inproceedings{uhrig_sparsity_2017-2,
	address = {Qingdao},
	title = {Sparsity {Invariant} {CNNs}},
	isbn = {978-1-5386-2610-8},
	url = {https://ieeexplore.ieee.org/document/8374553/},
	doi = {10.1109/3DV.2017.00012},
	abstract = {In this paper, we consider convolutional neural networks operating on sparse inputs with an application to depth completion from sparse laser scan data. First, we show that traditional convolutional networks perform poorly when applied to sparse data even when the location of missing data is provided to the network. To overcome this problem, we propose a simple yet effective sparse convolution layer which explicitly considers the location of missing data during the convolution operation. We demonstrate the beneﬁts of the proposed network architecture in synthetic and real experiments with respect to various baseline approaches. Compared to dense baselines, the proposed sparse convolution network generalizes well to novel datasets and is invariant to the level of sparsity in the data. For our evaluation, we derive a novel dataset from the KITTI benchmark, comprising over 94k depth annotated RGB images. Our dataset allows for training and evaluating depth completion and depth prediction techniques in challenging real-world settings and is available online at: www.cvlibs.net/datasets/kitti.},
	language = {en},
	urldate = {2021-08-27},
	booktitle = {2017 {International} {Conference} on {3D} {Vision} ({3DV})},
	publisher = {IEEE},
	author = {Uhrig, Jonas and Schneider, Nick and Schneider, Lukas and Franke, Uwe and Brox, Thomas and Geiger, Andreas},
	month = oct,
	year = {2017},
	note = {00376},
	pages = {11--20},
	file = {2017-Sparsity Invariant CNNs.pdf:/home/red0orange/Zotero/storage/ETDH3HII/2017-Sparsity Invariant CNNs.pdf:application/pdf},
}

@article{noauthor_deepcluster_nodate,
	series = {{DeepCluster}，伪标签},
	title = {{DeepCluster}},
	file = {DeepCluster.pdf:/home/red0orange/Zotero/storage/6U7XGWTY/DeepCluster.pdf:application/pdf},
}

@article{noauthor_rethinking_2021,
	series = {{BatchNorm的问题}},
	title = {Rethinking “{Batch}” in {BatchNorm}},
	month = may,
	year = {2021},
}

@article{noauthor_batch_2015,
	title = {Batch {Normalization}},
	year = {2015},
	file = {2015-Batch Normalization.pdf:/home/red0orange/Zotero/storage/N6BPCC9G/2015-Batch Normalization.pdf:application/pdf},
}

@inproceedings{goldman_learn_2019-1,
	address = {Long Beach, CA, USA},
	series = {{KITTI榜}},
	title = {Learn {Stereo}, {Infer} {Mono}: {Siamese} {Networks} for {Self}-{Supervised}, {Monocular}, {Depth} {Estimation}},
	isbn = {978-1-72812-506-0},
	shorttitle = {Learn {Stereo}, {Infer} {Mono}},
	url = {https://ieeexplore.ieee.org/document/9025636/},
	doi = {10.1109/CVPRW.2019.00348},
	abstract = {The ﬁeld of self-supervised monocular depth estimation has seen huge advancements in recent years. Most methods assume stereo data is available during training but usually under-utilize it and only treat it as a reference signal. We propose a novel self-supervised approach which uses both left and right images equally during training, but can still be used with a single input image at test time, for monocular depth estimation. Our Siamese network architecture consists of two, twin networks, each learns to predict a disparity map from a single image. At test time, however, only one of these networks is used in order to infer depth. We show state-of-the-art results on the standard KITTI Eigen split benchmark as well as being the highest scoring selfsupervised method on the new KITTI single view benchmark. To demonstrate the ability of our method to generalize to new data sets, we further provide results on the Make3D benchmark, which was not used during training.},
	language = {en},
	urldate = {2021-08-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Goldman, Matan and Hassner, Tal and Avidan, Shai},
	month = jun,
	year = {2019},
	note = {00013},
	pages = {2886--2895},
	file = {2019-Learn Stereo, Infer Mono.pdf:/home/red0orange/Zotero/storage/6FYARR53/2019-Learn Stereo, Infer Mono.pdf:application/pdf},
}

@article{chawla_multimodal_2021,
	series = {{KITTI榜}},
	title = {Multimodal {Scale} {Consistency} and {Awareness} for {Monocular} {Self}-{Supervised} {Depth} {Estimation}},
	abstract = {Dense depth estimation is essential to sceneunderstanding for autonomous driving. However, recent selfsupervised approaches on monocular videos suffer from scaleinconsistency across long sequences. Utilizing data from the ubiquitously copresent global positioning systems (GPS), we tackle this challenge by proposing a dynamically-weighted GPS-to-Scale (g2s) loss to complement the appearance-based losses. We emphasize that the GPS is needed only during the multimodal training, and not at inference. The relative distance between frames captured through the GPS provides a scale signal that is independent of the camera setup and scene distribution, resulting in richer learned feature representations. Through extensive evaluation on multiple datasets, we demonstrate scale-consistent and -aware depth estimation during inference, improving the performance even when training with low-frequency GPS data.},
	language = {en},
	author = {Chawla, Hemang and Varma, Arnav and Arani, Elahe and Zonooz, Bahram},
	month = jun,
	year = {2021},
	note = {00000},
	pages = {8},
	file = {2021-Multimodal Scale Consistency and Awareness for Monocular Self-Supervised Depth.pdf:/home/red0orange/Zotero/storage/BH9T8M5J/2021-Multimodal Scale Consistency and Awareness for Monocular Self-Supervised Depth.pdf:application/pdf},
}

@article{jiang_dipe_2020,
	series = {{KITTI榜}},
	title = {{DiPE}: {Deeper} into {Photometric} {Errors} for {Unsupervised} {Learning} of {Depth} and {Ego}-motion from {Monocular} {Videos}},
	shorttitle = {{DiPE}},
	url = {http://arxiv.org/abs/2003.01360},
	abstract = {Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn great attention, which avoids the use of expensive ground truth in the supervised one. It achieves this by using the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Despite signiﬁcant progress, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difﬁculties better. The primary improvement is achieved by a statistical technique that can mask out the invisible or nonstationary pixels in the photometric error map and thus prevents misleading the networks. With this outlier masking approach, the depth of objects moving in the opposite direction to the camera can be estimated more accurately. To the best of our knowledge, such scenarios have not been seriously considered in the previous works, even though they pose a higher risk in applications like autonomous driving. We also propose an efﬁcient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-oftheart performance on both depth and ego-motion estimation.},
	language = {en},
	urldate = {2021-08-21},
	journal = {arXiv:2003.01360 [cs]},
	author = {Jiang, Hualie and Ding, Laiyan and Sun, Zhenglong and Huang, Rui},
	month = nov,
	year = {2020},
	note = {00002 
arXiv: 2003.01360},
	file = {2020-DiPE.pdf:/home/red0orange/Zotero/storage/Q85XRSKT/2020-DiPE.pdf:application/pdf},
}

@article{klingner_self-supervised_2020,
	series = {{KITTI榜}},
	title = {Self-{Supervised} {Monocular} {Depth} {Estimation}: {Solving} the {Dynamic} {Object} {Problem} by {Semantic} {Guidance}},
	shorttitle = {Self-{Supervised} {Monocular} {Depth} {Estimation}},
	url = {http://arxiv.org/abs/2007.06936},
	abstract = {Self-supervised monocular depth estimation presents a powerful method to obtain 3D scene information from single camera images, which is trainable on arbitrary image sequences without requiring depth labels, e.g., from a LiDAR sensor. In this work we present a new selfsupervised semantically-guided depth estimation (SGDepth) method to deal with moving dynamic-class (DC) objects, such as moving cars and pedestrians, which violate the static-world assumptions typically made during training of such models. Speciﬁcally, we propose (i) mutually beneﬁcial cross-domain training of (supervised) semantic segmentation and self-supervised depth estimation with task-speciﬁc network heads, (ii) a semantic masking scheme providing guidance to prevent moving DC objects from contaminating the photometric loss, and (iii) a detection method for frames with non-moving DC objects, from which the depth of DC objects can be learned. We demonstrate the performance of our method on several benchmarks, in particular on the Eigen split, where we exceed all baselines without test-time reﬁnement.},
	language = {en},
	urldate = {2021-08-21},
	journal = {arXiv:2007.06936 [cs]},
	author = {Klingner, Marvin and Termöhlen, Jan-Aike and Mikolajczyk, Jonas and Fingscheidt, Tim},
	month = jul,
	year = {2020},
	note = {00043 
arXiv: 2007.06936},
	file = {2020-Self-Supervised Monocular Depth Estimation.pdf:/home/red0orange/Zotero/storage/8TW4X8KY/2020-Self-Supervised Monocular Depth Estimation.pdf:application/pdf},
}

@inproceedings{qi_geonet_2018,
	address = {Salt Lake City, UT, USA},
	title = {{GeoNet}: {Geometric} {Neural} {Network} for {Joint} {Depth} and {Surface} {Normal} {Estimation}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{GeoNet}},
	url = {https://ieeexplore.ieee.org/document/8578135/},
	doi = {10.1109/CVPR.2018.00037},
	abstract = {In this paper, we propose Geometric Neural Network (GeoNet) to jointly predict depth and surface normal maps from a single image. Building on top of two-stream CNNs, our GeoNet incorporates geometric relation between depth and surface normal via the new depth-to-normal and normalto-depth networks. Depth-to-normal network exploits the least square solution of surface normal from depth and improves its quality with a residual module. Normal-to-depth network, contrarily, reﬁnes the depth map based on the constraints from the surface normal through a kernel regression module, which has no parameter to learn. These two networks enforce the underlying model to efﬁciently predict depth and surface normal for high consistency and corresponding accuracy. Our experiments on NYU v2 dataset verify that our GeoNet is able to predict geometrically consistent depth and normal maps. It achieves top performance on surface normal estimation and is on par with state-of-theart depth estimation methods.},
	language = {en},
	urldate = {2021-08-20},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Qi, Xiaojuan and Liao, Renjie and Liu, Zhengzhe and Urtasun, Raquel and Jia, Jiaya},
	month = jun,
	year = {2018},
	note = {00177},
	keywords = {在看},
	pages = {283--291},
	file = {2018-GeoNet.pdf:/home/red0orange/Zotero/storage/RPVSWJGI/2018-GeoNet.pdf:application/pdf},
}

@inproceedings{guizilini_3d_2020,
	address = {Seattle, WA, USA},
	series = {{KITTI榜}：{PackNet}},
	title = {{3D} {Packing} for {Self}-{Supervised} {Monocular} {Depth} {Estimation}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156708/},
	doi = {10.1109/CVPR42600.2020.00256},
	language = {en},
	urldate = {2021-08-21},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Raventos, Allan and Gaidon, Adrien},
	month = jun,
	year = {2020},
	pages = {2482--2491},
	file = {2020-3D Packing for Self-Supervised Monocular Depth Estimation.pdf:/home/red0orange/Zotero/storage/8TAFKTS6/2020-3D Packing for Self-Supervised Monocular Depth Estimation.pdf:application/pdf},
}

@inproceedings{xu_pad-net_2018,
	address = {Salt Lake City, UT},
	series = {结合深度预测和场景理解},
	title = {{PAD}-{Net}: {Multi}-tasks {Guided} {Prediction}-and-{Distillation} {Network} for {Simultaneous} {Depth} {Estimation} and {Scene} {Parsing}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{PAD}-{Net}},
	url = {https://ieeexplore.ieee.org/document/8578175/},
	doi = {10.1109/CVPR.2018.00077},
	abstract = {Depth estimation and scene parsing are two particularly important tasks in visual scene understanding. In this paper we tackle the problem of simultaneous depth estimation and scene parsing in a joint CNN. The task can be typically treated as a deep multi-task learning problem [42]. Different from previous methods directly optimizing multiple tasks given the input training data, this paper proposes a novel multi-task guided prediction-and-distillation network (PAD-Net), which ﬁrst predicts a set of intermediate auxiliary tasks ranging from low level to high level, and then the predictions from these intermediate auxiliary tasks are utilized as multi-modal input via our proposed multi-modal distillation modules for the ﬁnal tasks. During the joint learning, the intermediate tasks not only act as supervision for learning more robust deep representations but also provide rich multi-modal information for improving the ﬁnal tasks. Extensive experiments are conducted on two challenging datasets (i.e. NYUD-v2 and Cityscapes) for both the depth estimation and scene parsing tasks, demonstrating the effectiveness of the proposed approach.},
	language = {en},
	urldate = {2021-08-20},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Xu, Dan and Ouyang, Wanli and Wang, Xiaogang and Sebe, Nicu},
	month = jun,
	year = {2018},
	note = {00184},
	keywords = {在看},
	pages = {675--684},
	file = {2018-PAD-Net.pdf:/home/red0orange/Zotero/storage/LSGHENWA/2018-PAD-Net.pdf:application/pdf},
}

@inproceedings{huang_framenet_2019,
	address = {Seoul, Korea (South)},
	title = {{FrameNet}: {Learning} {Local} {Canonical} {Frames} of {3D} {Surfaces} {From} a {Single} {RGB} {Image}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{FrameNet}},
	url = {https://ieeexplore.ieee.org/document/9010031/},
	doi = {10.1109/ICCV.2019.00873},
	abstract = {In this work, we introduce the novel problem of identifying dense canonical 3D coordinate frames from a single RGB image. We observe that each pixel in an image is the projection of a small surface region in the underlying 3D geometry, where a canonical frame can be identiﬁed as represented by three orthogonal axes, one along its normal direction and two in its tangent plane. We propose an algorithm to predict these axes from RGB data. Our ﬁrst insight is that canonical frames computed automatically with recently introduced direction ﬁeld synthesis methods can provide training data for the task. Our second insight is that networks designed for surface normal prediction provide better results when trained jointly to predict canonical frames, and even better when trained to also predict 2D projections of canonical frames. We conjecture this is because projections of canonical tangent directions often align with local gradients in images, and because those directions are tightly linked to 3D canonical frames through projective geometry and orthogonality constraints. In our experiments, we ﬁnd that our method predicts 3D canonical frames that can be used in applications ranging from surface normal estimation, feature matching, and augmented reality.},
	language = {en},
	urldate = {2021-08-20},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Huang, Jingwei and Zhou, Yichao and Funkhouser, Thomas and Guibas, Leonidas},
	month = oct,
	year = {2019},
	note = {00019},
	keywords = {在看},
	pages = {8637--8646},
	file = {2019-FrameNet.pdf:/home/red0orange/Zotero/storage/B2HVQHUW/2019-FrameNet.pdf:application/pdf},
}

@article{yang_unsupervised_2017,
	series = {无监督：用法线的一篇},
	title = {Unsupervised {Learning} of {Geometry} with {Edge}-aware {Depth}-{Normal} {Consistency}},
	url = {http://arxiv.org/abs/1711.03665},
	abstract = {Learning to reconstruct depths in a single image by watching unlabeled videos via deep convolutional network (DCN) is attracting signiﬁcant attention in recent years, e.g.(Zhou et al. 2017). In this paper, we introduce a surface normal representation for unsupervised depth estimation framework. Our estimated depths are constrained to be compatible with predicted normals, yielding more robust geometry results. Speciﬁcally, we formulate an edge-aware depth-normal consistency term, and solve it by constructing a depth-to-normal layer and a normal-to-depth layer inside of the DCN. The depth-to-normal layer takes estimated depths as input, and computes normal directions using cross production based on neighboring pixels. Then given the estimated normals, the normal-to-depth layer outputs a regularized depth map through local planar smoothness. Both layers are computed with awareness of edges inside the image to help address the issue of depth/normal discontinuity and preserve sharp edges. Finally, to train the network, we apply the photometric error and gradient smoothness for both depth and normal predictions. We conducted experiments on both outdoor (KITTI) and indoor (NYUv2) datasets, and show that our algorithm vastly outperforms state of the art, which demonstrates the beneﬁts from our approach.},
	language = {en},
	urldate = {2021-08-19},
	journal = {arXiv:1711.03665 [cs]},
	author = {Yang, Zhenheng and Wang, Peng and Xu, Wei and Zhao, Liang and Nevatia, Ramakant},
	month = nov,
	year = {2017},
	note = {00000 
arXiv: 1711.03665},
	keywords = {在看},
	file = {2017-Unsupervised Learning of Geometry with Edge-aware Depth-Normal Consistency.pdf:/home/red0orange/Zotero/storage/B5XDTR8R/2017-Unsupervised Learning of Geometry with Edge-aware Depth-Normal Consistency.pdf:application/pdf},
}

@article{qiao_vip-deeplab_2021,
	series = {最新的单目深度预测进展},
	title = {{VIP}-{DeepLab}: {Learning} {Visual} {Perception} {With} {Depth}-{Aware} {Video} {Panoptic} {Segmentation}},
	abstract = {In this paper, we present ViP-DeepLab, a uniﬁed model attempting to tackle the long-standing and challenging inverse projection problem in vision, which we model as restoring the point clouds from perspective image sequences while providing each point with instance-level semantic interpretations. Solving this problem requires the vision models to predict the spatial location, semantic class, and temporally consistent instance label for each 3D point. ViP-DeepLab approaches it by jointly performing monocular depth estimation and video panoptic segmentation. We name this joint task as Depth-aware Video Panoptic Segmentation, and propose a new evaluation metric along with two derived datasets for it, which will be made available to the public. On the individual sub-tasks, ViP-DeepLab also achieves state-of-the-art results, outperforming previous methods by 5.1\% VPQ on Cityscapes-VPS, ranking 1st on the KITTI monocular depth estimation benchmark, and 1st on KITTI MOTS pedestrian. The datasets and the evaluation codes are made publicly available1.},
	language = {en},
	author = {Qiao, Siyuan and Zhu, Yukun and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
	year = {2021},
	note = {00000},
	keywords = {在看},
	pages = {12},
	file = {2021-VIP-DeepLab.pdf:/home/red0orange/Zotero/storage/7V5EMKVQ/2021-VIP-DeepLab.pdf:application/pdf},
}

@inproceedings{zhou_unsupervised_2017,
	address = {Honolulu, HI},
	series = {无监督：单目深度预测+ego-motion},
	title = {Unsupervised {Learning} of {Depth} and {Ego}-{Motion} from {Video}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100183/},
	doi = {10.1109/CVPR.2017.700},
	abstract = {We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings.},
	language = {en},
	urldate = {2021-08-18},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
	month = jul,
	year = {2017},
	note = {00000},
	keywords = {基础性文章},
	pages = {6612--6619},
	file = {2017-Unsupervised Learning of Depth and Ego-Motion from Video.pdf:/home/red0orange/Zotero/storage/4PWVTZ8Z/2017-Unsupervised Learning of Depth and Ego-Motion from Video.pdf:application/pdf},
}

@inproceedings{wang_learning_2018,
	address = {Salt Lake City, UT, USA},
	series = {无监督：利用{DVO做的}},
	title = {Learning {Depth} from {Monocular} {Videos} {Using} {Direct} {Methods}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578314/},
	doi = {10.1109/CVPR.2018.00216},
	abstract = {The ability to predict depth from a single image - using recent advances in CNNs - is of increasing interest to the vision community. Unsupervised strategies to learning are particularly appealing as they can utilize much larger and varied monocular video datasets during learning without the need for ground truth depth or stereo. In previous works, separate pose and depth CNN predictors had to be determined such that their joint outputs minimized the photometric error. Inspired by recent advances in direct visual odometry (DVO), we argue that the depth CNN predictor can be learned without a pose CNN predictor. Further, we demonstrate empirically that incorporation of a differentiable implementation of DVO, along with a novel depth normalization strategy - substantially improves performance over state of the art that use monocular videos for training.},
	language = {en},
	urldate = {2021-08-18},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Chaoyang and Buenaposada, Jose Miguel and Zhu, Rui and Lucey, Simon},
	month = jun,
	year = {2018},
	note = {00000},
	pages = {2022--2030},
	file = {2018-Learning Depth from Monocular Videos Using Direct Methods.pdf:/home/red0orange/Zotero/storage/2V96UTHC/2018-Learning Depth from Monocular Videos Using Direct Methods.pdf:application/pdf},
}

@article{casser_depth_2019-1,
	series = {无监督：结合实例分割的方法},
	title = {Depth {Prediction} without the {Sensors}: {Leveraging} {Structure} for {Unsupervised} {Learning} from {Monocular} {Videos}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Depth {Prediction} without the {Sensors}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4801},
	doi = {10.1609/aaai.v33i01.33018001},
	abstract = {Learning to predict scene depth from RGB inputs is a challenging task both for indoor and outdoor robot navigation. In this work we address unsupervised learning of scene depth and robot ego-motion where supervision is provided by monocular videos, as cameras are the cheapest, least restrictive and most ubiquitous sensor for robotics.},
	language = {en},
	urldate = {2021-08-18},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Casser, Vincent and Pirk, Soeren and Mahjourian, Reza and Angelova, Anelia},
	month = jul,
	year = {2019},
	note = {00000},
	pages = {8001--8008},
	file = {2019-Depth Prediction without the Sensors.pdf:/home/red0orange/Zotero/storage/2PFWSH69/2019-Depth Prediction without the Sensors.pdf:application/pdf},
}

@article{lopez_deep_2018,
	series = {sciv-tools},
	title = {Deep generative modeling for single-cell transcriptomics},
	volume = {15},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/s41592-018-0229-2},
	doi = {10.1038/s41592-018-0229-2},
	language = {en},
	number = {12},
	urldate = {2021-08-18},
	journal = {Nature Methods},
	author = {Lopez, Romain and Regier, Jeffrey and Cole, Michael B. and Jordan, Michael I. and Yosef, Nir},
	month = dec,
	year = {2018},
	note = {Number: 12
00000},
	pages = {1053--1058},
	file = {2018-Deep generative modeling for single-cell transcriptomics.pdf:/home/red0orange/Zotero/storage/RP7NG5SU/2018-Deep generative modeling for single-cell transcriptomics.pdf:application/pdf},
}

@article{zhu_identification_2018,
	series = {{Giotto提到的HMRF}(马尔可夫随机场模型)},
	title = {Identification of spatially associated subpopulations by combining {scRNAseq} and sequential fluorescence in situ hybridization data},
	volume = {36},
	issn = {1087-0156, 1546-1696},
	url = {http://www.nature.com/articles/nbt.4260},
	doi = {10.1038/nbt.4260},
	language = {en},
	number = {12},
	urldate = {2021-08-17},
	journal = {Nature Biotechnology},
	author = {Zhu, Qian and Shah, Sheel and Dries, Ruben and Cai, Long and Yuan, Guo-Cheng},
	month = dec,
	year = {2018},
	note = {Number: 12},
	pages = {1183--1190},
	file = {2018-Identification of spatially associated subpopulations by combining scRNAseq and.pdf:/home/red0orange/Zotero/storage/4DZ3KQDQ/2018-Identification of spatially associated subpopulations by combining scRNAseq and.pdf:application/pdf},
}

@article{dries_giotto_2021,
	series = {Giotto},
	title = {Giotto: a toolbox for integrative analysis and visualization of spatial expression data},
	volume = {22},
	issn = {1474-760X},
	shorttitle = {Giotto},
	url = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02286-2},
	doi = {10.1186/s13059-021-02286-2},
	abstract = {Spatial transcriptomic and proteomic technologies have provided new opportunities to investigate cells in their native microenvironment. Here we present Giotto, a comprehensive and open-source toolbox for spatial data analysis and visualization. The analysis module provides end-to-end analysis by implementing a wide range of algorithms for characterizing tissue composition, spatial expression patterns, and cellular interactions. Furthermore, single-cell RNAseq data can be integrated for spatial cell-type enrichment analysis. The visualization module allows users to interactively visualize analysis outputs and imaging features. To demonstrate its general applicability, we apply Giotto to a wide range of datasets encompassing diverse technologies and platforms.},
	language = {en},
	number = {1},
	urldate = {2021-08-17},
	journal = {Genome Biology},
	author = {Dries, Ruben and Zhu, Qian and Dong, Rui and Eng, Chee-Huat Linus and Li, Huipeng and Liu, Kan and Fu, Yuntian and Zhao, Tianxiao and Sarkar, Arpan and Bao, Feng and George, Rani E. and Pierson, Nico and Cai, Long and Yuan, Guo-Cheng},
	month = dec,
	year = {2021},
	note = {Number: 1},
	keywords = {Toolbox},
	pages = {78},
	file = {2021-Giotto.pdf:/home/red0orange/Zotero/storage/VBCHL5RH/2021-Giotto.pdf:application/pdf},
}

@article{brennecke_accounting_2013,
	series = {{SpatialDE的单细胞版本}：{HVGs}},
	title = {Accounting for technical noise in single-cell {RNA}-seq experiments},
	volume = {10},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/nmeth.2645},
	doi = {10.1038/nmeth.2645},
	language = {en},
	number = {11},
	urldate = {2021-08-16},
	journal = {Nature Methods},
	author = {Brennecke, Philip and Anders, Simon and Kim, Jong Kyoung and Kołodziejczyk, Aleksandra A and Zhang, Xiuwei and Proserpio, Valentina and Baying, Bianka and Benes, Vladimir and Teichmann, Sarah A and Marioni, John C and Heisler, Marcus G},
	month = nov,
	year = {2013},
	note = {Number: 11},
	pages = {1093--1095},
	file = {2013-Accounting for technical noise in single-cell RNA-seq experiments.pdf:/home/red0orange/Zotero/storage/8KNDTS7Z/2013-Accounting for technical noise in single-cell RNA-seq experiments.pdf:application/pdf},
}

@article{svensson_spatialde_2018,
	series = {{SpatialDE}},
	title = {{SpatialDE}: identification of spatially variable genes},
	volume = {15},
	issn = {1548-7091, 1548-7105},
	shorttitle = {{SpatialDE}},
	url = {http://www.nature.com/articles/nmeth.4636},
	doi = {10.1038/nmeth.4636},
	language = {en},
	number = {5},
	urldate = {2021-08-16},
	journal = {Nature Methods},
	author = {Svensson, Valentine and Teichmann, Sarah A and Stegle, Oliver},
	month = may,
	year = {2018},
	note = {Number: 5},
	pages = {343--346},
	file = {2018-SpatialDE.pdf:/home/red0orange/Zotero/storage/ERKXLIKG/2018-SpatialDE.pdf:application/pdf},
}

@article{edsgard_identification_2018,
	series = {与{SpatialDE类似的}：{Trendsceek}},
	title = {Identification of spatial expression trends in single-cell gene expression data},
	volume = {15},
	issn = {1548-7091, 1548-7105},
	url = {http://www.nature.com/articles/nmeth.4634},
	doi = {10.1038/nmeth.4634},
	abstract = {Methods for spatial gene expression analyses at single-cell resolution are becoming available, whereas computational strategies for spatial gene expression analyses are lacking. We present a computational method (trendsceek) based on marked point processes that identifies genes with significant spatial expression trends. Trendsceek identifies significant genes in spatial transcriptomics and sequential FISH data and also reveal significant gene expression gradients and hotspots in low-dimensional projections of dissociated single-cell RNA-seq data.},
	language = {en},
	number = {5},
	urldate = {2021-08-16},
	journal = {Nature Methods},
	author = {Edsgärd, Daniel and Johnsson, Per and Sandberg, Rickard},
	month = may,
	year = {2018},
	note = {Number: 5},
	pages = {339--342},
	file = {2018-Identification of spatial expression trends in single-cell gene expression data.pdf:/home/red0orange/Zotero/storage/RQ8QUXYZ/2018-Identification of spatial expression trends in single-cell gene expression data.pdf:application/pdf},
}

@article{tran_benchmark_2020,
	title = {A benchmark of batch-effect correction methods for single-cell {RNA} sequencing data},
	volume = {21},
	issn = {1474-760X},
	url = {https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1850-9},
	doi = {10.1186/s13059-019-1850-9},
	abstract = {Background: Large-scale single-cell transcriptomic datasets generated using different technologies contain batchspecific systematic variations that present a challenge to batch-effect removal and data integration. With continued growth expected in scRNA-seq data, achieving effective batch integration with available computational resources is crucial. Here, we perform an in-depth benchmark study on available batch correction methods to determine the most suitable method for batch-effect removal.
Results: We compare 14 methods in terms of computational runtime, the ability to handle large datasets, and batch-effect correction efficacy while preserving cell type purity. Five scenarios are designed for the study: identical cell types with different technologies, non-identical cell types, multiple batches, big data, and simulated data. Performance is evaluated using four benchmarking metrics including kBET, LISI, ASW, and ARI. We also investigate the use of batch-corrected data to study differential gene expression.
Conclusion: Based on our results, Harmony, LIGER, and Seurat 3 are the recommended methods for batch integration. Due to its significantly shorter runtime, Harmony is recommended as the first method to try, with the other methods as viable alternatives.},
	language = {en},
	number = {1},
	urldate = {2021-08-16},
	journal = {Genome Biology},
	author = {Tran, Hoa Thi Nhu and Ang, Kok Siong and Chevrier, Marion and Zhang, Xiaomeng and Lee, Nicole Yee Shin and Goh, Michelle and Chen, Jinmiao},
	month = dec,
	year = {2020},
	note = {Number: 1},
	pages = {12},
	file = {2020-A benchmark of batch-effect correction methods for single-cell RNA sequencing.pdf:/home/red0orange/Zotero/storage/JQ38IZBN/2020-A benchmark of batch-effect correction methods for single-cell RNA sequencing.pdf:application/pdf},
}

@inproceedings{park_changesim_2021,
	address = {Prague, Czech Republic},
	title = {{ChangeSim}: {Towards} {End}-to-{End} {Online} {Scene} {Change} {Detection} in {Industrial} {Indoor} {Environments}},
	isbn = {978-1-66541-714-3},
	shorttitle = {{ChangeSim}},
	url = {https://ieeexplore.ieee.org/document/9636350/},
	doi = {10.1109/IROS51168.2021.9636350},
	language = {en},
	urldate = {2022-05-23},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Park, Jin-Man and Jang, Jae-Hyuk and Yoo, Sahng-Min and Lee, Sun-Kyung and Kim, Ue-Hwan and Kim, Jong-Hwan},
	month = sep,
	year = {2021},
	pages = {8578--8585},
	file = {Park 等。 - 2021 - ChangeSim Towards End-to-End Online Scene Change .pdf:/home/red0orange/Zotero/storage/5BZ6QGAX/Park 等。 - 2021 - ChangeSim Towards End-to-End Online Scene Change .pdf:application/pdf},
}

@inproceedings{oleynikova_voxblox_2017,
	address = {Vancouver, BC},
	title = {Voxblox: {Incremental} {3D} {Euclidean} {Signed} {Distance} {Fields} for on-board {MAV} planning},
	isbn = {978-1-5386-2682-5},
	shorttitle = {Voxblox},
	url = {http://ieeexplore.ieee.org/document/8202315/},
	doi = {10.1109/IROS.2017.8202315},
	abstract = {Micro Aerial Vehicles (MAVs) that operate in unstructured, unexplored environments require fast and ﬂexible local planning, which can replan when new parts of the map are explored. Trajectory optimization methods fulﬁll these needs, but require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs).},
	language = {en},
	urldate = {2022-05-24},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Oleynikova, Helen and Taylor, Zachary and Fehr, Marius and Siegwart, Roland and Nieto, Juan},
	month = sep,
	year = {2017},
	pages = {1366--1373},
	file = {Oleynikova 等。 - 2017 - Voxblox Incremental 3D Euclidean Signed Distance .pdf:/home/red0orange/Zotero/storage/KUKLQLG5/Oleynikova 等。 - 2017 - Voxblox Incremental 3D Euclidean Signed Distance .pdf:application/pdf},
}

@misc{liu_structformer_2021,
	title = {{StructFormer}: {Learning} {Spatial} {Structure} for {Language}-{Guided} {Semantic} {Rearrangement} of {Novel} {Objects}},
	shorttitle = {{StructFormer}},
	url = {http://arxiv.org/abs/2110.10189},
	abstract = {Geometric organization of objects into semanti- searches on the comprehensive chore list from [1] sugcally meaningful arrangements pervades the built world. As such, assistive robots operating in warehouses, ofﬁces, and homes would greatly beneﬁt from the ability to recognize and rearrange objects into these semantically meaningful structures. To be useful, these robots must contend with previously unseen objects and receive instructions without signiﬁcant programgesting its practical importance for future domestic robots.},
	language = {en},
	urldate = {2022-05-26},
	publisher = {arXiv},
	author = {Liu, Weiyu and Paxton, Chris and Hermans, Tucker and Fox, Dieter},
	month = oct,
	year = {2021},
	note = {Number: arXiv:2110.10189
arXiv:2110.10189 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Liu 等。 - 2021 - StructFormer Learning Spatial Structure for Langu.pdf:/home/red0orange/Zotero/storage/KWXXBKPQ/Liu 等。 - 2021 - StructFormer Learning Spatial Structure for Langu.pdf:application/pdf},
}

@article{szot_habitat_nodate,
	series = {Intel google合作开发的室内服务机器人仿真平台},
	title = {Habitat 2.0: {Training} {Home} {Assistants} to {Rearrange} their {Habitat}},
	abstract = {We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all levels of the embodied AI stack – data, simulation, and benchmark tasks. Speciﬁcally, we present: (i) ReplicaCAD: an artist-authored, annotated, reconﬁgurable 3D dataset of apartments (matching real spaces) with articulated objects (e.g. cabinets and drawers that can open/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with speeds exceeding 25,000 simulation steps per second (850⇥ real-time) on an 8-GPU node, representing 100⇥ speed-ups over prior work; and, (iii) Home Assistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy the house, stock groceries, set the table) that test a range of mobile manipulation capabilities. These large-scale engineering contributions allow us to systematically compare deep reinforcement learning (RL) at scale and classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with an emphasis on generalization to new objects, receptacles, and layouts. We ﬁnd that (1) ﬂat RL policies struggle on HAB compared to hierarchical ones; (2) a hierarchy with independent skills suffers from ‘hand-off problems’, and (3) SPA pipelines are more brittle than RL policies.},
	language = {en},
	author = {Szot, Andrew and Clegg, Alex and Undersander, Eric and Wijmans, Erik and Zhao, Yili and Turner, John and Maestre, Noah and Mukadam, Mustafa and Chaplot, Devendra and Maksymets, Oleksandr and Gokaslan, Aaron and Vondrus, Vladimir and Dharur, Sameer and Meier, Franziska and Galuba, Wojciech and Chang, Angel and Kira, Zsolt and Koltun, Vladlen and Malik, Jitendra and Savva, Manolis and Batra, Dhruv},
	pages = {16},
	file = {NeurIPS-2021-habitat-20-training-home-assistants-to-rearrange-their-habitat-Supplemental.pdf:/home/red0orange/Zotero/storage/ZGR972BW/NeurIPS-2021-habitat-20-training-home-assistants-to-rearrange-their-habitat-Supplemental.pdf:application/pdf;Szot 等。 - Habitat 2.0 Training Home Assistants to Rearrange.pdf:/home/red0orange/Zotero/storage/PJM3V7GQ/Szot 等。 - Habitat 2.0 Training Home Assistants to Rearrange.pdf:application/pdf},
}

@inproceedings{weihs_visual_2021,
	series = {ai2thor 2022 {CVPR} chanllenge论文，见笔记},
	title = {Visual room rearrangement},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Weihs, Luca and Deitke, Matt and Kembhavi, Aniruddha and Mottaghi, Roozbeh},
	year = {2021},
	pages = {5922--5931},
	file = {Weihs_Visual_Room_Rearrangement_CVPR_2021_paper.pdf:/home/red0orange/Zotero/storage/LNA9AM2E/Weihs_Visual_Room_Rearrangement_CVPR_2021_paper.pdf:application/pdf},
}

@misc{goodwin_semantically_2021,
	series = {相关实例文章},
	title = {Semantically {Grounded} {Object} {Matching} for {Robust} {Robotic} {Scene} {Rearrangement}},
	url = {http://arxiv.org/abs/2111.07975},
	abstract = {Object rearrangement has recently emerged as a key competency in robot manipulation, with practical solutions generally involving object detection, recognition, grasping and high-level planning. Goal-images describing a desired scene conﬁguration are a promising and increasingly used mode of instruction. A key outstanding challenge is the accurate inference of matches between objects in front of a robot, and those seen in a provided goal image, where recent works have struggled in the absence of object-speciﬁc training data. In this work, we explore the deterioration of existing methods’ ability to infer matches between objects as the visual shift between observed and goal scenes increases. We ﬁnd that a fundamental limitation of the current setting is that source and target images must contain the same instance of every object, which restricts practical deployment. We present a novel approach to object matching that uses a large pre-trained vision-language model to match objects in a cross-instance setting by leveraging semantics together with visual features as a more robust, and much more general, measure of similarity. We demonstrate that this provides considerably improved matching performance in cross-instance settings, and can be used to guide multi-object rearrangement with a robot manipulator from an image that shares no object instances with the robot’s scene. Our code is available at https://github.com/applied-ai-lab/ object\_matching.},
	language = {en},
	urldate = {2022-05-31},
	publisher = {arXiv},
	author = {Goodwin, Walter and Vaze, Sagar and Havoutis, Ioannis and Posner, Ingmar},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2111.07975
arXiv:2111.07975 [cs]},
	file = {Goodwin 等。 - 2021 - Semantically Grounded Object Matching for Robust R.pdf:/home/red0orange/Zotero/storage/Z226CXZN/Goodwin 等。 - 2021 - Semantically Grounded Object Matching for Robust R.pdf:application/pdf},
}

@misc{gadre_continuous_2022,
	series = {Roozbeh {Mottaghi}},
	title = {Continuous {Scene} {Representations} for {Embodied} {AI}},
	url = {http://arxiv.org/abs/2203.17251},
	abstract = {We propose Continuous Scene Representations (CSR), a scene representation constructed by an embodied agent navigating within a space, where objects and their relationships are modeled by continuous valued embeddings. Our method captures feature relationships between objects, composes them into a graph structure on-the-ﬂy, and situates an embodied agent within the representation. Our key insight is to embed pair-wise relationships between objects in a latent space. This allows for a richer representation compared to discrete relations (e.g., [SUPPORT], [NEXTTO]) commonly used for building scene representations. CSR can track objects as the agent moves in a scene, update the representation accordingly, and detect changes in room conﬁgurations. Using CSR, we outperform state-ofthe-art approaches for the challenging downstream task of visual room rearrangement, without any task speciﬁc training. Moreover, we show the learned embeddings capture salient spatial details of the scene and show applicability to real world data. A summery video and code is available at prior.allenai.org/projects/csr.},
	language = {en},
	urldate = {2022-05-31},
	publisher = {arXiv},
	author = {Gadre, Samir Yitzhak and Ehsani, Kiana and Song, Shuran and Mottaghi, Roozbeh},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.17251
arXiv:2203.17251 [cs]},
	file = {Gadre 等。 - 2022 - Continuous Scene Representations for Embodied AI.pdf:/home/red0orange/Zotero/storage/N7BZDK86/Gadre 等。 - 2022 - Continuous Scene Representations for Embodied AI.pdf:application/pdf},
}

@misc{khandelwal_simple_2022,
	series = {Roozbeh {Mottaghi}},
	title = {Simple but {Effective}: {CLIP} {Embeddings} for {Embodied} {AI}},
	shorttitle = {Simple but {Effective}},
	url = {http://arxiv.org/abs/2111.09888},
	abstract = {Contrastive language image pretraining (CLIP) encoders have been shown to be beneﬁcial for a range of visual tasks from classiﬁcation and detection to captioning and image manipulation. We investigate the effectiveness of CLIP visual backbones for Embodied AI tasks. We build incredibly simple baselines, named EmbCLIP, with no task speciﬁc architectures, inductive biases (such as the use of semantic maps), auxiliary tasks during training, or depth maps—yet we ﬁnd that our improved baselines perform very well across a range of tasks and simulators. EmbCLIP tops the RoboTHOR ObjectNav leaderboard by a huge margin of 20 pts (Success Rate). It tops the iTHOR 1-Phase Rearrangement leaderboard, beating the next best submission, which employs Active Neural Mapping, and more than doubling the \% Fixed Strict metric (0.08 to 0.17). It also beats the winners of the 2021 Habitat ObjectNav Challenge, which employ auxiliary tasks, depth maps, and human demonstrations, and those of the 2019 Habitat PointNav Challenge. We evaluate the ability of CLIP’s visual representations at capturing semantic information about input observations—primitives that are useful for navigation-heavy embodied tasks—and ﬁnd that CLIP’s representations encode these primitives more effectively than ImageNet-pretrained backbones. Finally, we extend one of our baselines, producing an agent capable of zero-shot object navigation that can navigate to objects that were not used as targets during training. Our code and models are available at https://github.com/ allenai/embodied-clip.},
	language = {en},
	urldate = {2022-05-31},
	publisher = {arXiv},
	author = {Khandelwal, Apoorv and Weihs, Luca and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
	month = apr,
	year = {2022},
	note = {Number: arXiv:2111.09888
arXiv:2111.09888 [cs]},
	file = {Khandelwal 等。 - 2022 - Simple but Effective CLIP Embeddings for Embodied.pdf:/home/red0orange/Zotero/storage/BR9QUWZI/Khandelwal 等。 - 2022 - Simple but Effective CLIP Embeddings for Embodied.pdf:application/pdf},
}

@misc{duan_survey_2022,
	title = {A {Survey} of {Embodied} {AI}: {From} {Simulators} to {Research} {Tasks}},
	shorttitle = {A {Survey} of {Embodied} {AI}},
	url = {http://arxiv.org/abs/2103.04918},
	abstract = {There has been an emerging paradigm shift from the era of “internet AI” to “embodied AI”, where AI algorithms and agents no longer learn from datasets of images, videos or text curated primarily from the internet. Instead, they learn through interactions with their environments from an egocentric perception similar to humans. Consequently, there has been substantial growth in the demand for embodied AI simulators to support various embodied AI research tasks. This growing interest in embodied AI is beneﬁcial to the greater pursuit of Artiﬁcial General Intelligence (AGI), but there has not been a contemporary and comprehensive survey of this ﬁeld. This paper aims to provide an encyclopedic survey for the ﬁeld of embodied AI, from its simulators to its research. By evaluating nine current embodied AI simulators with our proposed seven features, this paper aims to understand the simulators in their provision for use in embodied AI research and their limitations. Lastly, this paper surveys the three main research tasks in embodied AI – visual exploration, visual navigation and embodied question answering (QA), covering the state-of-the-art approaches, evaluation metrics and datasets. Finally, with the new insights revealed through surveying the ﬁeld, the paper will provide suggestions for simulator-for-task selections and recommendations for the future directions of the ﬁeld.},
	language = {en},
	urldate = {2022-06-02},
	publisher = {arXiv},
	author = {Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
	month = jan,
	year = {2022},
	note = {Number: arXiv:2103.04918
arXiv:2103.04918 [cs]},
	file = {Duan 等。 - 2022 - A Survey of Embodied AI From Simulators to Resear.pdf:/home/red0orange/Zotero/storage/DESUW4EJ/Duan 等。 - 2022 - A Survey of Embodied AI From Simulators to Resear.pdf:application/pdf},
}

@book{coumans_pybullet_nodate,
	title = {{PyBullet} {Quickstart} {Guide}},
	language = {en},
	author = {Coumans, Erwin and Bai, Yunfei},
	file = {Coumans 和 Bai - PyBullet Quickstart Guide.pdf:/home/red0orange/Zotero/storage/RRSMG93Z/Coumans 和 Bai - PyBullet Quickstart Guide.pdf:application/pdf},
}

@misc{bobu_learning_2022,
	title = {Learning {Perceptual} {Concepts} by {Bootstrapping} from {Human} {Queries}},
	url = {http://arxiv.org/abs/2111.05251},
	abstract = {Most robot tasks can be thought of as relating one or more objects, and learning new tasks by necessity involves teaching the robot new concepts relating objects to one another. However, learning new concepts that operate on highdimensional data – like that coming from a robot’s sensors – is impractical, because it requires an unrealistic amount of labeled human input. In this work, we observe that by using a simulator at training time we can get access to signiﬁcant privileged information – things like object poses and bounding boxes –that allows for learning a low-dimensional variant of the concept with much less human input. The robot can then use this lowdimensional concept to automatically label large amounts of high-dimensional data in the simulator. This enables learning perceptual concepts that work with real sensor input where no privileged information is available. We evaluate our Perceptual Concept Bootstrapping (PCB) approach by learning spatial concepts that describe object state or multi-object relationships. We show that our approach improves sample complexity when compared to learning concepts directly in the high-dimensional space. We also demonstrate the utility of the learned concepts in motion planning tasks on a 7-DoF Franka Panda robot.},
	language = {en},
	urldate = {2022-06-16},
	publisher = {arXiv},
	author = {Bobu, Andreea and Paxton, Chris and Yang, Wei and Sundaralingam, Balakumar and Chao, Yu-Wei and Cakmak, Maya and Fox, Dieter},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2111.05251
arXiv:2111.05251 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {Bobu 等。 - 2022 - Learning Perceptual Concepts by Bootstrapping from.pdf:/home/red0orange/Zotero/storage/W3II7ITQ/Bobu 等。 - 2022 - Learning Perceptual Concepts by Bootstrapping from.pdf:application/pdf},
}

@article{kant_housekeep_nodate,
	title = {Housekeep: {Tidying} {Virtual} {Households} using {Commonsense} {Reasoning}},
	abstract = {We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We show that our baseline agent generalizes to rearranging unseen objects in unknown environments.},
	language = {en},
	author = {Kant, Yash and Ramachandran, Arun and Yenamandra, Sriram},
	pages = {4},
	file = {Kant 等。 - Housekeep Tidying Virtual Households using Common.pdf:/home/red0orange/Zotero/storage/Z3Y8GN27/Kant 等。 - Housekeep Tidying Virtual Households using Common.pdf:application/pdf},
}

@article{shridhar_cliport_nodate,
	title = {{CLIPORT}: {What} and {Where} {Pathways} for {Robotic} {Manipulation}},
	abstract = {How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for ﬁne-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Speciﬁcally, we present CLIPORT, a language-conditioned imitationlearning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-speciﬁed tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efﬁcient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.},
	language = {en},
	author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
	pages = {13},
	file = {Shridhar 等。 - CLIPORT What and Where Pathways for Robotic Manip.pdf:/home/red0orange/Zotero/storage/NPCNRAPU/Shridhar 等。 - CLIPORT What and Where Pathways for Robotic Manip.pdf:application/pdf},
}

@misc{liu_structformer_2021-1,
	series = {{StructFormer}},
	title = {{StructFormer}: {Learning} {Spatial} {Structure} for {Language}-{Guided} {Semantic} {Rearrangement} of {Novel} {Objects}},
	shorttitle = {{StructFormer}},
	url = {http://arxiv.org/abs/2110.10189},
	abstract = {Geometric organization of objects into semanti- searches on the comprehensive chore list from [1] sugcally meaningful arrangements pervades the built world. As such, assistive robots operating in warehouses, ofﬁces, and homes would greatly beneﬁt from the ability to recognize and rearrange objects into these semantically meaningful structures. To be useful, these robots must contend with previously unseen objects and receive instructions without signiﬁcant programgesting its practical importance for future domestic robots.},
	language = {en},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Liu, Weiyu and Paxton, Chris and Hermans, Tucker and Fox, Dieter},
	month = oct,
	year = {2021},
	note = {arXiv:2110.10189 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Liu 等。 - 2021 - StructFormer Learning Spatial Structure for Langu.pdf:/home/red0orange/Zotero/storage/I38VNL23/Liu 等。 - 2021 - StructFormer Learning Spatial Structure for Langu.pdf:application/pdf},
}

@misc{zeng_transporter_2022,
	series = {爆款},
	title = {Transporter {Networks}: {Rearranging} the {Visual} {World} for {Robotic} {Manipulation}},
	shorttitle = {Transporter {Networks}},
	url = {http://arxiv.org/abs/2010.14406},
	abstract = {Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io},
	language = {en},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Wahid, Ayzaan and Sindhwani, Vikas and Lee, Johnny},
	month = jan,
	year = {2022},
	note = {arXiv:2010.14406 [cs]},
	file = {Zeng 等。 - 2022 - Transporter Networks Rearranging the Visual World.pdf:/home/red0orange/Zotero/storage/XF7KI86X/Zeng 等。 - 2022 - Transporter Networks Rearranging the Visual World.pdf:application/pdf},
}

@misc{garrett_integrated_2020,
	series = {task and motion planning ({TAMP})},
	title = {Integrated {Task} and {Motion} {Planning}},
	url = {http://arxiv.org/abs/2010.01083},
	abstract = {The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (tamp). tamp problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be eﬀectively addressed by any of these ﬁelds directly. In this paper, we deﬁne a class of tamp problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.},
	language = {en},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Garrett, Caelan Reed and Chitnis, Rohan and Holladay, Rachel and Kim, Beomjoon and Silver, Tom and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
	month = oct,
	year = {2020},
	note = {arXiv:2010.01083 [cs]},
	file = {Garrett 等。 - 2020 - Integrated Task and Motion Planning.pdf:/home/red0orange/Zotero/storage/F355NSSE/Garrett 等。 - 2020 - Integrated Task and Motion Planning.pdf:application/pdf},
}

@inproceedings{danielczuk_object_2021,
	address = {Xi'an, China},
	title = {Object {Rearrangement} {Using} {Learned} {Implicit} {Collision} {Functions}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561516/},
	doi = {10.1109/ICRA48506.2021.9561516},
	abstract = {Robotic object rearrangement combines the skills of picking and placing objects. When object models are unavailable, typical collision-checking models may be unable to predict collisions in partial point clouds with occlusions, making generation of collision-free grasping or placement trajectories challenging. We propose a learned collision model that accepts scene and query object point clouds and predicts collisions for 6DOF object poses within the scene. We train the model on a synthetic set of 1 million scene/object point cloud pairs and 2 billion collision queries. We leverage the learned collision model as part of a model predictive path integral (MPPI) policy in a tabletop rearrangement task and show that the policy can plan collision-free grasps and placements for objects unseen in training in both simulated and physical cluttered scenes with a Franka Panda robot. The learned model outperforms both traditional pipelines and learned ablations by 9.8\% in accuracy on a dataset of simulated collision queries and is 75x faster than the best-performing baseline. Videos and supplementary material are available at https://research.nvidia.com/publication/ 2021-03\_Object-Rearrangement-Using.},
	language = {en},
	urldate = {2022-07-09},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Danielczuk, Michael and Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},
	month = may,
	year = {2021},
	pages = {6010--6017},
	file = {Danielczuk 等。 - 2021 - Object Rearrangement Using Learned Implicit Collis.pdf:/home/red0orange/Zotero/storage/PNHS75ZI/Danielczuk 等。 - 2021 - Object Rearrangement Using Learned Implicit Collis.pdf:application/pdf},
}

@misc{labbe_monte-carlo_2020,
	title = {Monte-{Carlo} {Tree} {Search} for {Efficient} {Visually} {Guided} {Rearrangement} {Planning}},
	url = {http://arxiv.org/abs/1904.10348},
	abstract = {We address the problem of visually guided rearrangement planning with many movable objects, i.e., ﬁnding a sequence of actions to move a set of objects from an initial arrangement to a desired one, while relying on visual inputs coming from an RGB camera. To do so, we introduce a complete pipeline relying on two key contributions. First, we introduce an efﬁcient and scalable rearrangement planning method, based on a Monte-Carlo Tree Search exploration strategy. We demonstrate that because of its good trade-off between exploration and exploitation our method (i) scales well with the number of objects while (ii) ﬁnding solutions which require a smaller number of moves compared to the other state-of-the-art approaches. Note that on the contrary to many approaches, we do not require any buffer space to be available. Second, to precisely localize movable objects in the scene, we develop an integrated approach for robust multi-object workspace state estimation from a single uncalibrated RGB camera using a deep neural network trained only with synthetic data. We validate our multi-object visually guided manipulation pipeline with several experiments on a real UR-5 robotic arm by solving various rearrangement planning instances, requiring only 60 ms to compute the plan to rearrange 25 objects. In addition, we show that our system is insensitive to camera movements and can successfully recover from external perturbations. Supplementary video, source code and pre-trained models are available at https://ylabbe.github.io/rearrangement-planning.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Labbé, Yann and Zagoruyko, Sergey and Kalevatykh, Igor and Laptev, Ivan and Carpentier, Justin and Aubry, Mathieu and Sivic, Josef},
	month = apr,
	year = {2020},
	note = {arXiv:1904.10348 [cs]},
	file = {Labbé 等。 - 2020 - Monte-Carlo Tree Search for Efficient Visually Gui.pdf:/home/red0orange/Zotero/storage/6SPE4HGB/Labbé 等。 - 2020 - Monte-Carlo Tree Search for Efficient Visually Gui.pdf:application/pdf},
}

@misc{qureshi_nerp_2021,
	series = {{NeRP}},
	title = {{NeRP}: {Neural} {Rearrangement} {Planning} for {Unknown} {Objects}},
	shorttitle = {{NeRP}},
	url = {http://arxiv.org/abs/2106.01352},
	abstract = {Robots will be expected to manipulate a wide variety of objects in complex and arbitrary ways as they become more widely used in human environments. As such, the rearrangement of objects has been noted to be an important benchmark for AI capabilities in recent years. We propose NeRP (Neural Rearrangement Planning), a deep learning based approach for multi-step neural object rearrangement planning which works with never-before-seen objects, that is trained on simulation data, and generalizes to the real world. We compare NeRP to several naive and model-based baselines, demonstrating that our approach is measurably better and can efﬁciently arrange unseen objects in fewer steps and with less planning time. Finally, we demonstrate it on several challenging rearrangement problems in the real world1.},
	language = {en},
	urldate = {2022-07-08},
	publisher = {arXiv},
	author = {Qureshi, Ahmed H. and Mousavian, Arsalan and Paxton, Chris and Yip, Michael C. and Fox, Dieter},
	month = jun,
	year = {2021},
	note = {arXiv:2106.01352 [cs]},
	file = {Qureshi 等。 - 2021 - NeRP Neural Rearrangement Planning for Unknown Ob.pdf:/home/red0orange/Zotero/storage/897IFHAK/Qureshi 等。 - 2021 - NeRP Neural Rearrangement Planning for Unknown Ob.pdf:application/pdf},
}

@misc{goyal_ifor_2022,
	series = {{IFOR}},
	title = {{IFOR}: {Iterative} {Flow} {Minimization} for {Robotic} {Object} {Rearrangement}},
	shorttitle = {{IFOR}},
	url = {http://arxiv.org/abs/2202.00732},
	abstract = {Accurate object rearrangement from vision is a crucial problem for a wide variety of real-world robotics applications in unstructured environments. We propose IFOR, Iterative Flow Minimization for Robotic Object Rearrangement, an end-to-end method for the challenging problem of object rearrangement for unknown objects given an RGBD image of the original and ﬁnal scenes. First, we learn an optical ﬂow model based on RAFT to estimate the relative transformation of the objects purely from synthetic data. This ﬂow is then used in an iterative minimization algorithm to achieve accurate positioning of previously unseen objects. Crucially, we show that our method applies to cluttered scenes, and in the real world, while training only on synthetic data. Videos are available at https: //imankgoyal.github.io/ifor.html.},
	language = {en},
	urldate = {2022-06-26},
	publisher = {arXiv},
	author = {Goyal, Ankit and Mousavian, Arsalan and Paxton, Chris and Chao, Yu-Wei and Okorn, Brian and Deng, Jia and Fox, Dieter},
	month = feb,
	year = {2022},
	note = {arXiv:2202.00732 [cs]},
	file = {Goyal 等。 - 2022 - IFOR Iterative Flow Minimization for Robotic Obje.pdf:/home/red0orange/Zotero/storage/8GSN4PH8/Goyal 等。 - 2022 - IFOR Iterative Flow Minimization for Robotic Obje.pdf:application/pdf},
}

@inproceedings{doersch_unsupervised_2015,
	title = {Unsupervised visual representation learning by context prediction},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
	year = {2015},
	pages = {1422--1430},
}

@inproceedings{ahsan_video_2019,
	title = {Video jigsaw: {Unsupervised} learning of spatiotemporal context for video action recognition},
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Ahsan, Unaiza and Madhok, Rishi and Essa, Irfan},
	year = {2019},
	pages = {179--189},
}

@inproceedings{salakhutdinov_efficient_2010,
	title = {Efficient learning of deep {Boltzmann} machines},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Salakhutdinov, Ruslan and Larochelle, Hugo},
	year = {2010},
	pages = {693--700},
}

@inproceedings{masci_stacked_2011,
	title = {Stacked convolutional auto-encoders for hierarchical feature extraction},
	booktitle = {International conference on artificial neural networks},
	publisher = {Springer},
	author = {Masci, Jonathan and Meier, Ueli and Cireşan, Dan and Schmidhuber, Jürgen},
	year = {2011},
	pages = {52--59},
}

@article{donahue_large_2019,
	title = {Large scale adversarial representation learning},
	volume = {32},
	journal = {Advances in neural information processing systems},
	author = {Donahue, Jeff and Simonyan, Karen},
	year = {2019},
}

@article{donahue_adversarial_2016,
	title = {Adversarial feature learning},
	journal = {arXiv preprint arXiv:1605.09782},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	year = {2016},
}

@inproceedings{ranzato_unsupervised_2007,
	title = {Unsupervised learning of invariant feature hierarchies with applications to object recognition},
	booktitle = {2007 {IEEE} conference on computer vision and pattern recognition},
	publisher = {IEEE},
	author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann},
	year = {2007},
	pages = {1--8},
}

@article{olshausen_emergence_1996,
	title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
	volume = {381},
	number = {6583},
	journal = {Nature},
	author = {Olshausen, Bruno A and Field, David J},
	year = {1996},
	note = {Publisher: Nature Publishing Group},
	pages = {607--609},
}

@article{krizhevsky_imagenet_2012-1,
	title = {Imagenet classification with deep convolutional neural networks},
	volume = {25},
	journal = {Advances in neural information processing systems},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	volume = {1},
	booktitle = {2005 {IEEE} computer society conference on computer vision and pattern recognition ({CVPR}'05)},
	publisher = {Ieee},
	author = {Dalal, Navneet and Triggs, Bill},
	year = {2005},
	pages = {886--893},
}

@inproceedings{lowe_object_1999,
	title = {Object recognition from local scale-invariant features},
	volume = {2},
	booktitle = {Proceedings of the seventh {IEEE} international conference on computer vision},
	publisher = {Ieee},
	author = {Lowe, David G},
	year = {1999},
	pages = {1150--1157},
}

@inproceedings{misra_self-supervised_2020,
	title = {Self-supervised learning of pretext-invariant representations},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Misra, Ishan and Maaten, Laurens van der},
	year = {2020},
	pages = {6707--6717},
}

@inproceedings{brafman_one_2008,
	title = {From {One} to {Many}: {Planning} for {Loosely} {Coupled} {Multi}-{Agent} {Systems}.},
	volume = {8},
	booktitle = {{ICAPS}},
	author = {Brafman, Ronen I and Domshlak, Carmel},
	year = {2008},
	pages = {28--35},
}

@article{jannach_modeling_2011,
	title = {Modeling and solving distributed configuration problems: {A} {CSP}-based approach},
	volume = {25},
	number = {3},
	journal = {IEEE transactions on knowledge and data engineering},
	author = {Jannach, Dietmar and Zanker, Markus},
	year = {2011},
	note = {Publisher: IEEE},
	pages = {603--618},
}

@inproceedings{nissim_general_2010,
	title = {A general, fully distributed multi-agent planning algorithm},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}: volume 1-{Volume} 1},
	author = {Nissim, Raz and Brafman, Ronen I and Domshlak, Carmel},
	year = {2010},
	pages = {1323--1330},
}

@article{dimopoulos_-satplan_2012,
	title = {μ-{SATPLAN}: {Multi}-agent planning as satisfiability},
	volume = {29},
	journal = {Knowledge-Based Systems},
	author = {Dimopoulos, Yannis and Hashmi, Muhammad Adnan and Moraitis, Pavlos},
	year = {2012},
	note = {Publisher: Elsevier},
	pages = {54--62},
}

@article{kala_dynamic_2014,
	title = {Dynamic distributed lanes: motion planning for multiple autonomous vehicles},
	volume = {41},
	number = {1},
	journal = {Applied intelligence},
	author = {Kala, Rahul and Warwick, Kevin},
	year = {2014},
	note = {Publisher: Springer},
	pages = {260--281},
}

@article{cox_efficient_2009,
	title = {Efficient and distributable methods for solving the multiagent plan coordination problem},
	volume = {5},
	number = {4},
	journal = {Multiagent and Grid Systems},
	author = {Cox, Jeffrey and Durfee, Edmund},
	year = {2009},
	note = {Publisher: IOS Press},
	pages = {373--408},
}

@article{wang_cooperative_2011,
	title = {Cooperative target tracking control of multiple robots},
	volume = {59},
	number = {8},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Wang, Zongyao and Gu, Dongbing},
	year = {2011},
	note = {Publisher: IEEE},
	pages = {3232--3240},
}

@inproceedings{ong_decentralised_2006,
	title = {A decentralised particle filtering algorithm for multi-target tracking across multiple flight vehicles},
	booktitle = {2006 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	publisher = {IEEE},
	author = {Ong, Lee-Ling and Upcroft, Ben and Bailey, Tim and Ridley, Matthew and Sukkarieh, Salah and Durrant-Whyte, Hugh},
	year = {2006},
	pages = {4539--4544},
}

@article{lima_formation_2015,
	title = {Formation control driven by cooperative object tracking},
	volume = {63},
	journal = {Robotics and Autonomous Systems},
	author = {Lima, Pedro U and Ahmad, Aamir and Dias, André and Conceição, André GS and Moreira, António Paulo and Silva, Eduardo and Almeida, Luis and Oliveira, Luis and Nascimento, Tiago P},
	year = {2015},
	note = {Publisher: Elsevier},
	pages = {68--79},
}

@inproceedings{van_der_krogt_plan_2005,
	title = {Plan {Repair} as an {Extension} of {Planning}.},
	volume = {5},
	booktitle = {{ICAPS}},
	author = {Van Der Krogt, Roman and De Weerdt, Mathijs},
	year = {2005},
	pages = {161--170},
}

@article{tonino_plan_2002,
	title = {Plan coordination by revision in collective agent based systems},
	volume = {142},
	number = {2},
	journal = {Artificial Intelligence},
	author = {Tonino, Hans and Bos, André and de Weerdt, Mathijs and Witteveen, Cees},
	year = {2002},
	note = {Publisher: Elsevier},
	pages = {121--145},
}

@incollection{jung_cooperative_2006,
	title = {Cooperative multi-robot target tracking},
	booktitle = {Distributed {Autonomous} {Robotic} {Systems} 7},
	publisher = {Springer},
	author = {Jung, Boyoon and Sukhatme, Gaurav S},
	year = {2006},
	pages = {81--90},
}

@article{jung_tracking_2002,
	title = {Tracking targets using multiple robots: {The} effect of environment occlusion},
	volume = {13},
	number = {3},
	journal = {Autonomous robots},
	author = {Jung, Boyoon and Sukhatme, Gaurav S},
	year = {2002},
	note = {Publisher: Springer},
	pages = {191--205},
}

@inproceedings{mottaghi_integrated_2006,
	title = {An integrated particle filter and potential field method for cooperative robot target tracking},
	booktitle = {Proceedings 2006 {IEEE} {International} {Conference} on {Robotics} and {Automation}, 2006. {ICRA} 2006.},
	publisher = {IEEE},
	author = {Mottaghi, Roozbeh and Vaughan, Richard},
	year = {2006},
	pages = {1342--1347},
}

@inproceedings{adamey_decentralized_2012,
	title = {A decentralized approach for multi-{UAV} multitarget tracking and surveillance},
	volume = {8389},
	booktitle = {Ground/{Air} {Multisensor} {Interoperability}, {Integration}, and {Networking} for {Persistent} {ISR} {III}},
	publisher = {SPIE},
	author = {Adamey, Emrah and Ozguner, Umit},
	year = {2012},
	pages = {307--312},
}

@inproceedings{fink_optimal_2010,
	title = {Optimal robust multihop routing for wireless networks of mobile micro autonomous systems},
	booktitle = {2010-{MILCOM} 2010 {MILITARY} {COMMUNICATIONS} {CONFERENCE}},
	publisher = {IEEE},
	author = {Fink, Jonathan and Ribeiro, Alejandro and Kumar, Vijay and Brian, M Sadler},
	year = {2010},
	pages = {1268--1273},
}

@inproceedings{valenti_indoor_2006,
	title = {Indoor multi-vehicle flight testbed for fault detection, isolation, and recovery},
	booktitle = {{AIAA} guidance, navigation, and control conference and exhibit},
	author = {Valenti, Mario and Bethke, Brett and Fiore, Gaston and How, Jonathan and Feron, Eric},
	year = {2006},
	pages = {6200},
}

@article{michael_grasp_2010,
	title = {The grasp multiple micro-uav testbed},
	volume = {17},
	number = {3},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Michael, Nathan and Mellinger, Daniel and Lindsey, Quentin and Kumar, Vijay},
	year = {2010},
	note = {Publisher: IEEE},
	pages = {56--65},
}

@inproceedings{lupashin_simple_2010,
	title = {A simple learning strategy for high-speed quadrocopter multi-flips},
	booktitle = {2010 {IEEE} international conference on robotics and automation},
	publisher = {IEEE},
	author = {Lupashin, Sergei and Schöllig, Angela and Sherback, Michael and D'Andrea, Raffaello},
	year = {2010},
	pages = {1642--1648},
}

@article{nagatani_multirobot_2011,
	title = {Multirobot exploration for search and rescue missions: {A} report on map building in {RoboCupRescue} 2009},
	volume = {28},
	number = {3},
	journal = {Journal of Field Robotics},
	author = {Nagatani, Keiji and Okada, Yoshito and Tokunaga, Naoki and Kiribayashi, Seiga and Yoshida, Kazuya and Ohno, Kazunori and Takeuchi, Eijiro and Tadokoro, Satoshi and Akiyama, Hidehisa and Noda, Itsuki and {others}},
	year = {2011},
	note = {Publisher: Wiley Online Library},
	pages = {373--387},
}

@article{mur-artal_orb-slam_2015-1,
	title = {{ORB}-{SLAM}: a versatile and accurate monocular {SLAM} system},
	volume = {31},
	number = {5},
	journal = {IEEE transactions on robotics},
	author = {Mur-Artal, Raul and Montiel, Jose Maria Martinez and Tardos, Juan D},
	year = {2015},
	note = {Publisher: IEEE},
	pages = {1147--1163},
}

@article{charrow_approximate_2014,
	title = {Approximate representations for multi-robot control policies that maximize mutual information},
	volume = {37},
	number = {4},
	journal = {Autonomous Robots},
	author = {Charrow, Benjamin and Kumar, Vijay and Michael, Nathan},
	year = {2014},
	note = {Publisher: Springer},
	pages = {383--400},
}

@article{kurazume_automatic_2017,
	title = {Automatic large-scale three dimensional modeling using cooperative multiple robots},
	volume = {157},
	journal = {Computer Vision and Image Understanding},
	author = {Kurazume, Ryo and Oshima, Souichiro and Nagakura, Shingo and Jeong, Yongjin and Iwashita, Yumi},
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {25--42},
}

@inproceedings{michael_collaborative_2014,
	title = {Collaborative mapping of an earthquake damaged building via ground and aerial robots},
	booktitle = {Field and service robotics},
	publisher = {Springer},
	author = {Michael, Nathan and Shen, Shaojie and Mohta, Kartik and Kumar, Vijay and Nagatani, Keiji and Okada, Yoshito and Kiribayashi, Seiga and Otake, Kazuki and Yoshida, Kazuya and Ohno, Kazunori and {others}},
	year = {2014},
	pages = {33--47},
}

@article{davison_monoslam_2007,
	title = {{MonoSLAM}: {Real}-time single camera {SLAM}},
	volume = {29},
	number = {6},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Davison, Andrew J and Reid, Ian D and Molton, Nicholas D and Stasse, Olivier},
	year = {2007},
	note = {Publisher: IEEE},
	pages = {1052--1067},
}

@inproceedings{sodhi_leo_2022,
	title = {Leo: {Learning} energy-based models in factor graph optimization},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Sodhi, Paloma and Dexheimer, Eric and Mukadam, Mustafa and Anderson, Stuart and Kaess, Michael},
	year = {2022},
	pages = {234--244},
}

@inproceedings{zhang_visual-lidar_2015,
	title = {Visual-lidar odometry and mapping: {Low}-drift, robust, and fast},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zhang, Ji and Singh, Sanjiv},
	year = {2015},
	pages = {2174--2181},
}

@inproceedings{dutoit_consistent_2017,
	title = {Consistent map-based {3D} localization on mobile devices},
	booktitle = {2017 {IEEE} international conference on robotics and automation ({ICRA})},
	publisher = {IEEE},
	author = {DuToit, Ryan C and Hesch, Joel A and Nerurkar, Esha D and Roumeliotis, Stergios I},
	year = {2017},
	pages = {6253--6260},
}

@inproceedings{lee_intermittent_2020,
	title = {Intermittent gps-aided vio: {Online} initialization and calibration},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lee, Woosik and Eckenhoff, Kevin and Geneva, Patrick and Huang, Guoquan},
	year = {2020},
	pages = {5724--5731},
}

@book{barfoot_state_2017,
	title = {State estimation for robotics},
	publisher = {Cambridge University Press},
	author = {Barfoot, Timothy D},
	year = {2017},
}

@article{ding_persistent_2019,
	title = {Persistent stereo visual localization on cross-modal invariant map},
	volume = {21},
	number = {11},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Ding, Xiaqing and Wang, Yue and Xiong, Rong and Li, Dongxuan and Tang, Li and Yin, Huan and Zhao, Liang},
	year = {2019},
	note = {Publisher: IEEE},
	pages = {4646--4658},
}

@inproceedings{cioffi_tightly-coupled_2020,
	title = {Tightly-coupled fusion of global positional measurements in optimization-based visual-inertial odometry},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Cioffi, Giovanni and Scaramuzza, Davide},
	year = {2020},
	pages = {5089--5095},
}

@article{qin_general_2019,
	title = {A general optimization-based framework for global pose estimation with multiple sensors},
	journal = {arXiv preprint arXiv:1901.03642},
	author = {Qin, Tong and Cao, Shaozu and Pan, Jie and Shen, Shaojie},
	year = {2019},
}

@article{campos_orb-slam3_2021-1,
	title = {Orb-slam3: {An} accurate open-source library for visual, visual–inertial, and multimap slam},
	volume = {37},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J Gómez and Montiel, José MM and Tardós, Juan D},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {1874--1890},
}

@article{schneider_maplab_2018,
	title = {maplab: {An} open framework for research in visual-inertial mapping and localization},
	volume = {3},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Schneider, Thomas and Dymczyk, Marcin and Fehr, Marius and Egger, Kevin and Lynen, Simon and Gilitschenski, Igor and Siegwart, Roland},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {1418--1425},
}

@inproceedings{lynen_robust_2013,
	title = {A robust and modular multi-sensor fusion approach applied to mav navigation},
	booktitle = {2013 {IEEE}/{RSJ} international conference on intelligent robots and systems},
	publisher = {IEEE},
	author = {Lynen, Simon and Achtelik, Markus W and Weiss, Stephan and Chli, Margarita and Siegwart, Roland},
	year = {2013},
	pages = {3923--3929},
}

@article{__2018,
	title = {改进的北斗三频 {RTK} 整周模糊度固定方法},
	volume = {37},
	number = {6},
	journal = {传感器与微系统},
	author = {{张冠显} and {王玲} and {黄文德}},
	year = {2018},
	pages = {48--51},
}

@article{frei_rapid_1990,
	title = {Rapid static positioning based on the fast ambiguity resolution {approachFARA}': theory and first results},
	volume = {15},
	journal = {Manuscripta geodaetica},
	author = {Frei, Erwin},
	year = {1990},
	pages = {325--356},
}

@inproceedings{kelly_combined_2008,
	title = {Combined visual and inertial navigation for an unmanned aerial vehicle},
	booktitle = {Field and {Service} {Robotics}},
	publisher = {Springer},
	author = {Kelly, Jonathan and Saripalli, Srikanth and Sukhatme, Gaurav S},
	year = {2008},
	pages = {255--264},
}

@inproceedings{enge_high_1999,
	title = {High integrity carrier phase navigation for future {LAAS} using multiple civilian {GPS} signals},
	volume = {5},
	booktitle = {Proceedings of the 1999 {American} {Control} {Conference} ({Cat}. {No}. {99CH36251})},
	publisher = {IEEE},
	author = {Enge, Per and Jung, Jaewoo and Pervan, Boris},
	year = {1999},
	pages = {3650--3654},
}

@inproceedings{forssell_carrier_1997,
	title = {Carrier phase ambiguity resolution in {GNSS}-2},
	booktitle = {Proceedings of the 10th {International} {Technical} {Meeting} of the {Satellite} {Division} of the {Institute} of {Navigation} ({ION} {GPS} 1997)},
	author = {Forssell, B and Martin-Neira, M and Harrisz, RA},
	year = {1997},
	pages = {1727--1736},
}

@article{temiissen_least-squares_1995,
	title = {The least-squares ambiguity decorrelation adjustment: a method for fast {GPS} integer ambiguity estimation},
	volume = {70},
	journal = {Journal of Geodesy},
	author = {Temiissen, JG},
	year = {1995},
	pages = {65--82},
}

@article{chen_comparison_1995,
	title = {A {Comparison} of the {FASF} and {Least}-{Squares} {Search} {Algorithms} for on-the-{Fly} {Ambiguity} {Resolution}},
	volume = {42},
	number = {2},
	journal = {Navigation},
	author = {Chen, D and Lachapelle, Gérard},
	year = {1995},
	note = {Publisher: Wiley Online Library},
	pages = {371--390},
}

@incollection{hatch_instantaneous_1991,
	title = {Instantaneous ambiguity resolution},
	booktitle = {Kinematic systems in geodesy, surveying, and remote sensing},
	publisher = {Springer},
	author = {Hatch, Ron},
	year = {1991},
	pages = {299--308},
}

@misc{newbury_deep_2022,
	title = {Deep {Learning} {Approaches} to {Grasp} {Synthesis}: {A} {Review}},
	shorttitle = {Deep {Learning} {Approaches} to {Grasp} {Synthesis}},
	url = {http://arxiv.org/abs/2207.02556},
	abstract = {Grasping is the process of picking an object by applying forces and torques at a set of contacts. Recent advances in deep-learning methods have allowed rapid progress in robotic object grasping. We systematically surveyed the publications over the last decade, with a particular interest in grasping an object using all 6 degrees of freedom of the end-effector pose. Our review found four common methodologies for robotic grasping: sampling-based approaches, direct regression, reinforcement learning, and exemplar approaches. Furthermore, we found two 'supporting methods' around grasping that use deep-learning to support the grasping process, shape approximation, and affordances. We have distilled the publications found in this systematic review (85 papers) into ten key takeaways we consider crucial for future robotic grasping and manipulation research. An online version of the survey is available at https://rhys-newbury.github.io/projects/6dof/},
	language = {en},
	urldate = {2022-07-20},
	publisher = {arXiv},
	author = {Newbury, Rhys and Gu, Morris and Chumbley, Lachlan and Mousavian, Arsalan and Eppner, Clemens and Leitner, Jürgen and Bohg, Jeannette and Morales, Antonio and Asfour, Tamim and Kragic, Danica and Fox, Dieter and Cosgun, Akansel},
	month = jul,
	year = {2022},
	note = {arXiv:2207.02556 [cs]},
	file = {Newbury 等。 - 2022 - Deep Learning Approaches to Grasp Synthesis A Rev.pdf:/home/red0orange/Zotero/storage/L9RH4LVY/Newbury 等。 - 2022 - Deep Learning Approaches to Grasp Synthesis A Rev.pdf:application/pdf},
}

@misc{lipson_coupled_2022,
	series = {Single-shot，但是是迭代求解优化},
	title = {Coupled {Iterative} {Refinement} for {6D} {Multi}-{Object} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2204.12516},
	abstract = {We address the task of 6D multi-object pose: given a set of known 3D objects and an RGB or RGB-D input image, we detect and estimate the 6D pose of each object. We propose a new approach to 6D object pose estimation which consists of an end-to-end differentiable architecture that makes use of geometric knowledge. Our approach iteratively reﬁnes both pose and correspondence in a tightly coupled manner, allowing us to dynamically remove outliers to improve accuracy. We use a novel differentiable layer to perform pose reﬁnement by solving an optimization problem we refer to as Bidirectional Depth-Augmented PerspectiveN-Point (BD-PnP). Our method achieves state-of-the-art accuracy on standard 6D Object Pose benchmarks. Code is available at https://github.com/princetonvl/Coupled-Iterative-Refinement.},
	language = {en},
	urldate = {2022-07-22},
	publisher = {arXiv},
	author = {Lipson, Lahav and Teed, Zachary and Goyal, Ankit and Deng, Jia},
	month = apr,
	year = {2022},
	note = {arXiv:2204.12516 [cs]},
	file = {Lipson 等。 - 2022 - Coupled Iterative Refinement for 6D Multi-Object P.pdf:/home/red0orange/Zotero/storage/EK8MNULZ/Lipson 等。 - 2022 - Coupled Iterative Refinement for 6D Multi-Object P.pdf:application/pdf},
}

@misc{lin_multi-view_2021,
	title = {Multi-{View} {Fusion} for {Multi}-{Level} {Robotic} {Scene} {Understanding}},
	url = {http://arxiv.org/abs/2103.13539},
	abstract = {We present a system for multi-level scene awareness for robotic manipulation. Given a sequence of camera-inhand RGB images, the system calculates three types of information: 1) a point cloud representation of all the surfaces in the scene, for the purpose of obstacle avoidance; 2) the rough pose of unknown objects from categories corresponding to primitive shapes (e.g., cuboids and cylinders); and 3) full 6-DoF pose of known objects. By developing and fusing recent techniques in these domains, we provide a rich scene representation for robot awareness. We demonstrate the importance of each of these modules, their complementary nature, and the potential beneﬁts of the system in the context of robotic manipulation.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Lin, Yunzhi and Tremblay, Jonathan and Tyree, Stephen and Vela, Patricio A. and Birchfield, Stan},
	month = oct,
	year = {2021},
	note = {arXiv:2103.13539 [cs]},
	file = {Lin 等。 - 2021 - Multi-View Fusion for Multi-Level Robotic Scene Un.pdf:/home/red0orange/Zotero/storage/RJYQF85H/Lin 等。 - 2021 - Multi-View Fusion for Multi-Level Robotic Scene Un.pdf:application/pdf},
}

@misc{noauthor_six-dim_nodate,
	title = {Six-{Dim} {Object} {Pose} {Estimation}},
}

@misc{tyree_6-dof_2022,
	series = {{HOPE}},
	title = {6-{DoF} {Pose} {Estimation} of {Household} {Objects} for {Robotic} {Manipulation}: {An} {Accessible} {Dataset} and {Benchmark}},
	shorttitle = {6-{DoF} {Pose} {Estimation} of {Household} {Objects} for {Robotic} {Manipulation}},
	url = {http://arxiv.org/abs/2203.05701},
	abstract = {We present a new dataset for 6-DoF pose estimation of known objects, with a focus on robotic manipulation research. We propose a set of toy grocery objects, whose physical instantiations are readily available for purchase and are appropriately sized for robotic grasping and manipulation. We provide 3D scanned textured models of these objects, suitable for generating synthetic training data, as well as RGBD images of the objects in challenging, cluttered scenes exhibiting partial occlusion, extreme lighting variations, multiple instances per image, and a large variety of poses. Using semi-automated RGBD-to-model texture correspondences, the images are annotated with ground truth poses that were verified empirically to be accurate to within a few millimeters. We also propose a new pose evaluation metric called \{ADD-H\} based upon the Hungarian assignment algorithm that is robust to symmetries in object geometry without requiring their explicit enumeration. We share pre-trained pose estimators for all the toy grocery objects, along with their baseline performance on both validation and test sets. We offer this dataset to the community to help connect the efforts of computer vision researchers with the needs of roboticists.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Tyree, Stephen and Tremblay, Jonathan and To, Thang and Cheng, Jia and Mosier, Terry and Smith, Jeffrey and Birchfield, Stan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.05701 [cs]},
	file = {Tyree 等。 - 2022 - 6-DoF Pose Estimation of Household Objects for Rob.pdf:/home/red0orange/Zotero/storage/7KRYR8AF/Tyree 等。 - 2022 - 6-DoF Pose Estimation of Household Objects for Rob.pdf:application/pdf},
}

@misc{labbe_cosypose_2020,
	title = {{CosyPose}: {Consistent} multi-view multi-object {6D} pose estimation},
	shorttitle = {{CosyPose}},
	url = {http://arxiv.org/abs/2008.08465},
	abstract = {We introduce an approach for recovering the 6D pose of multiple known objects in a scene captured by a set of input images with unknown camera viewpoints. First, we present a single-view single-object 6D pose estimation method, which we use to generate 6D object pose hypotheses. Second, we develop a robust method for matching individual 6D object pose hypotheses across different input images in order to jointly estimate camera viewpoints and 6D poses of all objects in a single consistent scene. Our approach explicitly handles object symmetries, does not require depth measurements, is robust to missing or incorrect object hypotheses, and automatically recovers the number of objects in the scene. Third, we develop a method for global scene refinement given multiple object hypotheses and their correspondences across views. This is achieved by solving an object-level bundle adjustment problem that refines the poses of cameras and objects to minimize the reprojection error in all views. We demonstrate that the proposed method, dubbed CosyPose, outperforms current state-of-the-art results for single-view and multi-view 6D object pose estimation by a large margin on two challenging benchmarks: the YCB-Video and T-LESS datasets. Code and pre-trained models are available on the project webpage https://www.di.ens.fr/willow/research/cosypose/.},
	language = {en},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Labbé, Yann and Carpentier, Justin and Aubry, Mathieu and Sivic, Josef},
	month = aug,
	year = {2020},
	note = {arXiv:2008.08465 [cs]},
	file = {Labbé 等。 - 2020 - CosyPose Consistent multi-view multi-object 6D po.pdf:/home/red0orange/Zotero/storage/Z9T7EGA9/Labbé 等。 - 2020 - CosyPose Consistent multi-view multi-object 6D po.pdf:application/pdf},
}

@misc{james_q-attention_2022,
	title = {Q-attention: {Enabling} {Efficient} {Learning} for {Vision}-based {Robotic} {Manipulation}},
	shorttitle = {Q-attention},
	url = {http://arxiv.org/abs/2105.14829},
	abstract = {Despite the success of reinforcement learning methods, they have yet to have their breakthrough moment when applied to a broad range of robotic manipulation tasks. This is partly due to the fact that reinforcement learning algorithms are notoriously difficult and time consuming to train, which is exacerbated when training from images rather than full-state inputs. As humans perform manipulation tasks, our eyes closely monitor every step of the process with our gaze focusing sequentially on the objects being manipulated. With this in mind, we present our Attention-driven Robotic Manipulation (ARM) algorithm, which is a general manipulation algorithm that can be applied to a range of sparse-rewarded tasks, given only a small number of demonstrations. ARM splits the complex task of manipulation into a 3 stage pipeline: (1) a Q-attention agent extracts relevant pixel locations from RGB and point cloud inputs, (2) a next-best pose agent that accepts crops from the Q-attention agent and outputs poses, and (3) a control agent that takes the goal pose and outputs joint actions. We show that current learning algorithms fail on a range of RLBench tasks, whilst ARM is successful.},
	language = {en},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {James, Stephen and Davison, Andrew J.},
	month = feb,
	year = {2022},
	note = {arXiv:2105.14829 [cs]},
	file = {James 和 Davison - 2022 - Q-attention Enabling Efficient Learning for Visio.pdf:/home/red0orange/Zotero/storage/PIX3LPA2/James 和 Davison - 2022 - Q-attention Enabling Efficient Learning for Visio.pdf:application/pdf},
}

@article{james_coarse--fine_nodate,
	title = {Coarse-{To}-{Fine} {Q}-{Attention}: {Efficient} {Learning} for {Visual} {Robotic} {Manipulation} via {Discretisation}},
	abstract = {We present a coarse-to-fine discretisation method that enables the use of discrete reinforcement learning approaches in place of unstable and data-inefficient actor-critic methods in continuous robotics domains. This approach builds on the recently released ARM algorithm, which replaces the continuous next-best pose agent with a discrete one, with coarse-to-fine Q-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what part of the scene to ‘zoom’ into. When this ‘zooming’ behaviour is applied iteratively, it results in a near-lossless discretisation of the translation space, and allows the use of a discrete action, deep Q-learning method. We show that our new coarseto-fine algorithm achieves state-of-the-art performance on several difficult sparsely rewarded RLBench vision-based robotics tasks, and can train real-world policies, tabula rasa, in a matter of minutes, with as little as 3 demonstrations.},
	language = {en},
	author = {James, Stephen and Wada, Kentaro and Laidlow, Tristan and Davison, Andrew J},
	pages = {10},
	file = {James 等。 - Coarse-To-Fine Q-Attention Efficient Learning for.pdf:/home/red0orange/Zotero/storage/88C47QAR/James 等。 - Coarse-To-Fine Q-Attention Efficient Learning for.pdf:application/pdf},
}

@article{james_attention-driven_2020,
	title = {Attention-driven {Robotic} {Manipulation}},
	author = {James, Stephen and Davison, Andrew},
	year = {2020},
	file = {attention_driven_robotic_manip.pdf:/home/red0orange/Zotero/storage/DF85KF6K/attention_driven_robotic_manip.pdf:application/pdf},
}

@misc{irshad_shapo_2022,
	title = {{ShAPO}: {Implicit} {Representations} for {Multi}-{Object} {Shape}, {Appearance}, and {Pose} {Optimization}},
	shorttitle = {{ShAPO}},
	url = {http://arxiv.org/abs/2207.13691},
	abstract = {Our method studies the complex task of object-centric 3D understanding from a single RGB-D observation. As it is an ill-posed problem, existing methods suffer from low performance for both 3D shape and 6D pose and size estimation in complex multi-object scenarios with occlusions. We present ShAPO, a method for joint multi-object detection, 3D textured reconstruction, 6D object pose and size estimation. Key to ShAPO is a single-shot pipeline to regress shape, appearance and pose latent codes along with the masks of each object instance, which is then further refined in a sparse-to-dense fashion. A novel disentangled shape and appearance database of priors is first learned to embed objects in their respective shape and appearance space. We also propose a novel, octree-based differentiable optimization step, allowing us to further improve object shape, pose and appearance simultaneously under the learned latent space, in an analysis-by-synthesis fashion. Our novel joint implicit textured object representation allows us to accurately identify and reconstruct novel unseen objects without having access to their 3D meshes. Through extensive experiments, we show that our method, trained on simulated indoor scenes, accurately regresses the shape, appearance and pose of novel objects in the real-world with minimal fine-tuning. Our method significantly out-performs all baselines on the NOCS dataset with an 8\% absolute improvement in mAP for 6D pose estimation. Project page: https://zubair-irshad.github.io/projects/ShAPO.html},
	language = {en},
	urldate = {2022-07-29},
	publisher = {arXiv},
	author = {Irshad, Muhammad Zubair and Zakharov, Sergey and Ambrus, Rares and Kollar, Thomas and Kira, Zsolt and Gaidon, Adrien},
	month = jul,
	year = {2022},
	note = {arXiv:2207.13691 [cs]},
	file = {Irshad 等。 - 2022 - ShAPO Implicit Representations for Multi-Object S.pdf:/home/red0orange/Zotero/storage/3QUGZW27/Irshad 等。 - 2022 - ShAPO Implicit Representations for Multi-Object S.pdf:application/pdf},
}

@article{nagarajan_shaping_nodate,
	title = {Shaping embodied agent behavior with activity-context priors from egocentric video},
	language = {en},
	author = {Nagarajan, Tushar and Grauman, Kristen},
	pages = {12},
	file = {Nagarajan 和 Grauman - Shaping embodied agent behavior with activity-cont.pdf:/home/red0orange/Zotero/storage/KMQ76593/Nagarajan 和 Grauman - Shaping embodied agent behavior with activity-cont.pdf:application/pdf},
}

@article{collet_moped_2011,
	title = {The {MOPED} framework: {Object} recognition and pose estimation for manipulation},
	volume = {30},
	issn = {0278-3649, 1741-3176},
	shorttitle = {The {MOPED} framework},
	url = {http://journals.sagepub.com/doi/10.1177/0278364911401765},
	doi = {10.1177/0278364911401765},
	abstract = {We present MOPED, a framework for Multiple Object Pose Estimation and Detection that seamlessly integrates single-image and multi-image object recognition and pose estimation in one optimized, robust, and scalable framework. We address two main challenges in computer vision for robotics: robust performance in complex scenes, and low latency for real-time operation.},
	language = {en},
	number = {10},
	urldate = {2022-08-03},
	journal = {The International Journal of Robotics Research},
	author = {Collet, Alvaro and Martinez, Manuel and Srinivasa, Siddhartha S},
	month = sep,
	year = {2011},
	pages = {1284--1306},
	file = {Collet 等。 - 2011 - The MOPED framework Object recognition and pose e.pdf:/home/red0orange/Zotero/storage/7FUTCG62/Collet 等。 - 2011 - The MOPED framework Object recognition and pose e.pdf:application/pdf},
}

@article{cai_ove6d_nodate,
	title = {{OVE6D}: {Object} {Viewpoint} {Encoding} for {Depth}-{Based} {6D} {Object} {Pose} {Estimation}},
	abstract = {This paper proposes a universal framework, called OVE6D, for model-based 6D object pose estimation from a single depth image and a target object mask. Our model is trained using purely synthetic data rendered from ShapeNet, and, unlike most of the existing methods, it generalizes well on new real-world objects without any fine-tuning. We achieve this by decomposing the 6D pose into viewpoint, inplane rotation around the camera optical axis and translation, and introducing novel lightweight modules for estimating each component in a cascaded manner. The resulting network contains less than 4M parameters while demonstrating excellent performance on the challenging T-LESS and Occluded LINEMOD datasets without any datasetspecific training. We show that OVE6D outperforms some contemporary deep learning-based pose estimation methods specifically trained for individual objects or datasets with real-world training data. The implementation is available at https://github.com/dingdingcai/OVE6D-pose.},
	language = {en},
	author = {Cai, Dingding and Heikkila, Janne and Rahtu, Esa},
	keywords = {Ignore},
	pages = {11},
	file = {Cai 等。 - OVE6D Object Viewpoint Encoding for Depth-Based 6.pdf:/home/red0orange/Zotero/storage/LADW4R46/Cai 等。 - OVE6D Object Viewpoint Encoding for Depth-Based 6.pdf:application/pdf},
}

@article{deng_poserbpf_2021,
	title = {{PoseRBPF}: {A} {Rao}–{Blackwellized} {Particle} {Filter} for 6-{D} {Object} {Pose} {Tracking}},
	volume = {37},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{PoseRBPF}},
	url = {https://ieeexplore.ieee.org/document/9363455/},
	doi = {10.1109/TRO.2021.3056043},
	abstract = {Tracking 6-D poses of objects from videos provides rich information to a robot in performing different tasks such as manipulation and navigation. In this article, we formulate the 6-D object pose tracking problem in the Rao–Blackwellized particle ﬁltering framework, where the 3-D rotation and the 3-D translation of an object are decoupled. This factorization allows our approach, called PoseRBPF, to efﬁciently estimate the 3-D translation of an object along with the full distribution over the 3-D rotation. This is achieved by discretizing the rotation space in a ﬁne-grained manner and training an autoencoder network to construct a codebook of feature embeddings for the discretized rotations. As a result, PoseRBPF can track objects with arbitrary symmetries while still maintaining adequate posterior distributions. Our approach achieves state-of-the-art results on two 6-D pose estimation benchmarks. We open-source our implementation at https://github.com/NVlabs/PoseRBPF.},
	language = {en},
	number = {5},
	urldate = {2022-08-02},
	journal = {IEEE Transactions on Robotics},
	author = {Deng, Xinke and Mousavian, Arsalan and Xiang, Yu and Xia, Fei and Bretl, Timothy and Fox, Dieter},
	month = oct,
	year = {2021},
	pages = {1328--1342},
	file = {Deng 等。 - 2021 - PoseRBPF A Rao–Blackwellized Particle Filter for .pdf:/home/red0orange/Zotero/storage/8NADRZ58/Deng 等。 - 2021 - PoseRBPF A Rao–Blackwellized Particle Filter for .pdf:application/pdf},
}

@article{tang_show_nodate,
	title = {Show and {Tell}: {Learning} {Task}-{Oriented} {Grasping} {Strategy} from {Visual}-{Language} {Human} {Demonstration}},
	abstract = {How can we endow robots with the ability to manipulate objects guided by human language instruction? Previous works have shown that deep neural networks are capable of predicting high quality task-agnostic grasp configurations given only visual inputs. On top of that, it is demanded during human robot interaction that the user instructs robots to perform task-oriented object manipulation with natural language. Inspired by that, we propose GraspCLIP, an endto-end framework that learns task-oriented grasping strategy from visual-language human demonstration. While learning, demonstrated actions along with user preferences are encoded jointly and further conditioned on language inputs. Due to the sparsity of human demonstration, a novel ground truth synthesis pipeline is introduced in accompany for collecting and augmenting real-world demonstration data. Both simulation and real-robot experiments illustrate the superiority of GraspCLIP over established baselines. The code implementation will be publicly available.},
	language = {en},
	author = {Tang, Chao and Liu, Weiyu and Huang, Dehao and Meng, Lingxiao and Zhang, Hong},
	pages = {7},
	file = {Tang 等。 - Show and Tell Learning Task-Oriented Grasping Str.pdf:/home/red0orange/Zotero/storage/UMADFJ4Y/Tang 等。 - Show and Tell Learning Task-Oriented Grasping Str.pdf:application/pdf},
}

@misc{tian_shape_2020,
	title = {Shape {Prior} {Deformation} for {Categorical} {6D} {Object} {Pose} and {Size} {Estimation}},
	url = {http://arxiv.org/abs/2007.08454},
	abstract = {We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach signiﬁcantly outperforms the state of the art. Our code is available at https://github.com/mentian/object-deformnet.},
	language = {en},
	urldate = {2022-08-02},
	publisher = {arXiv},
	author = {Tian, Meng and Ang Jr, Marcelo H. and Lee, Gim Hee},
	month = jul,
	year = {2020},
	note = {arXiv:2007.08454 [cs]},
	file = {Tian 等。 - 2020 - Shape Prior Deformation for Categorical 6D Object .pdf:/home/red0orange/Zotero/storage/8YDXNJMF/Tian 等。 - 2020 - Shape Prior Deformation for Categorical 6D Object .pdf:application/pdf},
}

@inproceedings{wang_normalized_2019,
	address = {Long Beach, CA, USA},
	series = {{NCOS}},
	title = {Normalized {Object} {Coordinate} {Space} for {Category}-{Level} {6D} {Object} {Pose} and {Size} {Estimation}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953761/},
	doi = {10.1109/CVPR.2019.00275},
	abstract = {The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to “instance-level” 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce Normalized Object Coordinate Space (NOCS)—a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new contextaware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks.},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J.},
	month = jun,
	year = {2019},
	pages = {2637--2646},
	file = {Wang 等。 - 2019 - Normalized Object Coordinate Space for Category-Le.pdf:/home/red0orange/Zotero/storage/L59ADVTD/Wang 等。 - 2019 - Normalized Object Coordinate Space for Category-Le.pdf:application/pdf},
}

@inproceedings{wen_se3-tracknet_2020,
	title = {se(3)-{TrackNet}: {Data}-driven {6D} {Pose} {Tracking} by {Calibrating} {Image} {Residuals} in {Synthetic} {Domains}},
	shorttitle = {se(3)-{TrackNet}},
	url = {http://arxiv.org/abs/2007.13866},
	doi = {10.1109/IROS45743.2020.9341314},
	abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz.},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Wen, Bowen and Mitash, Chaitanya and Ren, Baozhang and Bekris, Kostas E.},
	month = oct,
	year = {2020},
	note = {arXiv:2007.13866 [cs, eess]},
	pages = {10367--10373},
	file = {Wen 等。 - 2020 - se(3)-TrackNet Data-driven 6D Pose Tracking by Ca.pdf:/home/red0orange/Zotero/storage/TXJNBAJI/Wen 等。 - 2020 - se(3)-TrackNet Data-driven 6D Pose Tracking by Ca.pdf:application/pdf},
}

@inproceedings{wang_6-pack_2020,
	address = {Paris, France},
	title = {6-{PACK}: {Category}-level {6D} {Pose} {Tracker} with {Anchor}-{Based} {Keypoints}},
	isbn = {978-1-72817-395-5},
	shorttitle = {6-{PACK}},
	url = {https://ieeexplore.ieee.org/document/9196679/},
	doi = {10.1109/ICRA40945.2020.9196679},
	abstract = {We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closedloop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.},
	language = {en},
	urldate = {2022-08-02},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Wang, Chen and Martin-Martin, Roberto and Xu, Danfei and Lv, Jun and Lu, Cewu and Fei-Fei, Li and Savarese, Silvio and Zhu, Yuke},
	month = may,
	year = {2020},
	pages = {10059--10066},
	file = {Wang 等。 - 2020 - 6-PACK Category-level 6D Pose Tracker with Anchor.pdf:/home/red0orange/Zotero/storage/KL6BD9UK/Wang 等。 - 2020 - 6-PACK Category-level 6D Pose Tracker with Anchor.pdf:application/pdf},
}

@misc{wen_bundletrack_2021,
	title = {{BundleTrack}: {6D} {Pose} {Tracking} for {Novel} {Objects} without {Instance} or {Category}-{Level} {3D} {Models}},
	shorttitle = {{BundleTrack}},
	url = {http://arxiv.org/abs/2108.00516},
	abstract = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method's reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack},
	language = {en},
	urldate = {2022-08-02},
	publisher = {arXiv},
	author = {Wen, Bowen and Bekris, Kostas},
	month = aug,
	year = {2021},
	note = {arXiv:2108.00516 [cs]},
	file = {2108.00516.pdf:/home/red0orange/Zotero/storage/9WLTDPUI/2108.00516.pdf:application/pdf},
}

@inproceedings{gouda_object_2021,
	address = {Krakow, Poland},
	title = {Object class-agnostic segmentation for practical {CNN} utilization in industry},
	isbn = {978-1-66540-642-0},
	url = {https://ieeexplore.ieee.org/document/9680821/},
	doi = {10.1109/ICMERR54363.2021.9680821},
	abstract = {The speed of adopting new technologies in industrial automation depends on two factors, reliability and ease of integration. CNN-based object segmentation is one of those technologies that are well developed in research and other industries but still not well established in industrial automation. It is an essential processing step for robotic grasping. Nevertheless, most of the grasping in the industry is still computed by classical nonlearning algorithms or based on simple manually programmed hypotheses. The traditional setup in most research related to the object segmentation problem is to have a ﬁnite number of objects/classes. While this is suitable for some other problems, it is the hurdle stopping the ease of integrating CNN object segmentation in the industry. A more practical approach is to use object class-agnostic segmentation, where a CNN is used to segment objects in an image without classifying them. Then classical feature extractors can be used for the classiﬁcation process. This method would avoid the need for manual tailoring of CNNs for each individual setup/environment.},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {2021 6th {International} {Conference} on {Mechanical} {Engineering} and {Robotics} {Research} ({ICMERR})},
	publisher = {IEEE},
	author = {Gouda, Anas and Ghanem, Abraham and Kaiser, Pascal and Ten Hompel, Michael},
	month = dec,
	year = {2021},
	pages = {97--105},
	file = {Gouda 等。 - 2021 - Object class-agnostic segmentation for practical C.pdf:/home/red0orange/Zotero/storage/IU9LECIR/Gouda 等。 - 2021 - Object class-agnostic segmentation for practical C.pdf:application/pdf},
}

@misc{xie_unseen_2021,
	series = {Fox},
	title = {Unseen {Object} {Instance} {Segmentation} for {Robotic} {Environments}},
	url = {http://arxiv.org/abs/2007.08073},
	abstract = {In order to function in unstructured environments, robots need the ability to recognize unseen objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. Our proposed method, UOIS-Net, separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. UOIS-Net is comprised of two stages: ﬁrst, it operates only on depth to produce object instance center votes in 2D or 3D and assembles them into rough initial masks. Secondly, these initial masks are reﬁned using RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method can produce sharp and accurate segmentation masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Xie, Christopher and Xiang, Yu and Mousavian, Arsalan and Fox, Dieter},
	month = oct,
	year = {2021},
	note = {arXiv:2007.08073 [cs]},
	file = {Xie 等。 - 2021 - Unseen Object Instance Segmentation for Robotic En.pdf:/home/red0orange/Zotero/storage/VQB5CCSG/Xie 等。 - 2021 - Unseen Object Instance Segmentation for Robotic En.pdf:application/pdf},
}

@misc{irshad_centersnap_2022,
	series = {{CenterSnap}},
	title = {{CenterSnap}: {Single}-{Shot} {Multi}-{Object} {3D} {Shape} {Reconstruction} and {Categorical} {6D} {Pose} and {Size} {Estimation}},
	shorttitle = {{CenterSnap}},
	url = {http://arxiv.org/abs/2203.01929},
	abstract = {This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instancelevel pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which ﬁrst localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple onestage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this perpixel representation, our approach can reconstruct in realtime (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach signiﬁcantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6\% absolute improvement in mAP for 6D pose for novel real-world object instances.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Irshad, Muhammad Zubair and Kollar, Thomas and Laskey, Michael and Stone, Kevin and Kira, Zsolt},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01929 [cs]},
	keywords = {Category},
	file = {Irshad 等。 - 2022 - CenterSnap Single-Shot Multi-Object 3D Shape Reco.pdf:/home/red0orange/Zotero/storage/P68RG5AQ/Irshad 等。 - 2022 - CenterSnap Single-Shot Multi-Object 3D Shape Reco.pdf:application/pdf},
}

@misc{gouda_category-agnostic_2022,
	title = {Category-agnostic {Segmentation} for {Robotic} {Grasping}},
	url = {http://arxiv.org/abs/2204.13613},
	abstract = {Robotic grasping in unstructured and nondeterministic environments needs to depend on smart vision processing that is able to segment unseen and arbitrary number of object categories. The usual case of detecting and segmenting objects from training sets makes the work limited to its own setup. This encourages us to develop generic methods for category-agnostic segmentation.},
	language = {en},
	urldate = {2022-08-01},
	publisher = {arXiv},
	author = {Gouda, Anas and Ghanem, Abraham and Reining, Christopher},
	month = apr,
	year = {2022},
	note = {arXiv:2204.13613 [cs]},
	file = {Gouda 等。 - 2022 - Category-agnostic Segmentation for Robotic Graspin.pdf:/home/red0orange/Zotero/storage/ZNL5TYM5/Gouda 等。 - 2022 - Category-agnostic Segmentation for Robotic Graspin.pdf:application/pdf},
}

@article{tang_show_nodate-1,
	title = {Show and {Tell}: {Learning} {Task}-{Oriented} {Grasping} {Strategy} from {Visual}-{Language} {Human} {Demonstration}},
	abstract = {How can we endow robots with the ability to manipulate objects guided by human language instruction? Previous works have shown that deep neural networks are capable of predicting high quality task-agnostic grasp configurations given only visual inputs. On top of that, it is demanded during human robot interaction that the user instructs robots to perform task-oriented object manipulation with natural language. Inspired by that, we propose GraspCLIP, an endto-end framework that learns task-oriented grasping strategy from visual-language human demonstration. While learning, demonstrated actions along with user preferences are encoded jointly and further conditioned on language inputs. Due to the sparsity of human demonstration, a novel ground truth synthesis pipeline is introduced in accompany for collecting and augmenting real-world demonstration data. Both simulation and real-robot experiments illustrate the superiority of GraspCLIP over established baselines. The code implementation will be publicly available.},
	language = {en},
	author = {Tang, Chao and Liu, Weiyu and Huang, Dehao and Meng, Lingxiao and Zhang, Hong},
	pages = {7},
	file = {Tang 等。 - Show and Tell Learning Task-Oriented Grasping Str.pdf:/home/red0orange/Zotero/storage/MK3V3Y4C/Tang 等。 - Show and Tell Learning Task-Oriented Grasping Str.pdf:application/pdf},
}

@inproceedings{danielczuk_object_2021-1,
	address = {Xi'an, China},
	series = {{SceneCollisionNet}},
	title = {Object {Rearrangement} {Using} {Learned} {Implicit} {Collision} {Functions}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561516/},
	doi = {10.1109/ICRA48506.2021.9561516},
	abstract = {Robotic object rearrangement combines the skills of picking and placing objects. When object models are unavailable, typical collision-checking models may be unable to predict collisions in partial point clouds with occlusions, making generation of collision-free grasping or placement trajectories challenging. We propose a learned collision model that accepts scene and query object point clouds and predicts collisions for 6DOF object poses within the scene. We train the model on a synthetic set of 1 million scene/object point cloud pairs and 2 billion collision queries. We leverage the learned collision model as part of a model predictive path integral (MPPI) policy in a tabletop rearrangement task and show that the policy can plan collision-free grasps and placements for objects unseen in training in both simulated and physical cluttered scenes with a Franka Panda robot. The learned model outperforms both traditional pipelines and learned ablations by 9.8\% in accuracy on a dataset of simulated collision queries and is 75x faster than the best-performing baseline. Videos and supplementary material are available at https://research.nvidia.com/publication/ 2021-03\_Object-Rearrangement-Using.},
	language = {en},
	urldate = {2022-08-08},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Danielczuk, Michael and Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},
	month = may,
	year = {2021},
	pages = {6010--6017},
	file = {Danielczuk 等。 - 2021 - Object Rearrangement Using Learned Implicit Collis.pdf:/home/red0orange/Zotero/storage/AEZ6NGSG/Danielczuk 等。 - 2021 - Object Rearrangement Using Learned Implicit Collis.pdf:application/pdf},
}

@misc{wen_catgrasp_2022,
	title = {{CaTGrasp}: {Learning} {Category}-{Level} {Task}-{Relevant} {Grasping} in {Clutter} from {Simulation}},
	shorttitle = {{CaTGrasp}},
	url = {http://arxiv.org/abs/2109.09163},
	abstract = {Task-relevant grasping is critical for industrial assembly, where downstream manipulation tasks constrain the set of valid grasps. Learning how to perform this task, however, is challenging, since task-relevant grasp labels are hard to deﬁne and annotate. There is also yet no consensus on proper representations for modeling or oﬀ-the-shelf tools for performing task-relevant grasps. This work proposes a framework to learn task-relevant grasping for industrial objects without the need of time-consuming real-world data collection or manual annotation. To achieve this, the entire framework is trained solely in simulation, including supervised training with synthetic label generation and self-supervised, hand-object interaction. In the context of this framework, this paper proposes a novel, object-centric canonical representation at the category level, which allows establishing dense correspondence across object instances and transferring task-relevant grasps to novel instances. Extensive experiments on task-relevant grasping of densely-cluttered industrial objects are conducted in both simulation and real-world setups, demonstrating the eﬀectiveness of the proposed framework. Code and data are available at https://sites.google.com/view/catgrasp.},
	language = {en},
	urldate = {2022-08-15},
	publisher = {arXiv},
	author = {Wen, Bowen and Lian, Wenzhao and Bekris, Kostas and Schaal, Stefan},
	month = feb,
	year = {2022},
	note = {arXiv:2109.09163 [cs, eess]},
	file = {Wen 等。 - 2022 - CaTGrasp Learning Category-Level Task-Relevant Gr.pdf:/home/red0orange/Zotero/storage/FITCAKR6/Wen 等。 - 2022 - CaTGrasp Learning Category-Level Task-Relevant Gr.pdf:application/pdf},
}

@inproceedings{zhu_review_2022,
	address = {Chongqing, China},
	series = {水文，但应该介绍了传统的方法},
	title = {A {Review} of {6D} {Object} {Pose} {Estimation}},
	isbn = {978-1-66542-207-9},
	url = {https://ieeexplore.ieee.org/document/9836663/},
	doi = {10.1109/ITAIC54216.2022.9836663},
	abstract = {The 6D object pose estimation is a forwardlooking technology in the field of computer vision, which has great application potential in metaverse, VR/AR, robot operation, intelligent driving and other fields. It is mainly to get the translation and rotation of rigid object in threedimensional rectangular coordinate system under x, y and z axes. A lot of research pour in this field. This paper briefly describes the 6D object pose estimation technology, introduces various traditional 6D object pose estimation methods, summarizes and analyzes the 6D object pose estimation algorithms based on deep learning and the data sets. Finally, the paper prospects the next research directions.},
	language = {en},
	urldate = {2022-08-15},
	booktitle = {2022 {IEEE} 10th {Joint} {International} {Information} {Technology} and {Artificial} {Intelligence} {Conference} ({ITAIC})},
	publisher = {IEEE},
	author = {Zhu, Yingzhao and Li, Man and Yao, Wensheng and Chen, Chunhua},
	month = jun,
	year = {2022},
	pages = {1647--1655},
	file = {Zhu 等。 - 2022 - A Review of 6D Object Pose Estimation.pdf:/home/red0orange/Zotero/storage/QHABMJNJ/Zhu 等。 - 2022 - A Review of 6D Object Pose Estimation.pdf:application/pdf},
}

@misc{mitash_task-driven_2020,
	series = {Grasp},
	title = {Task-driven {Perception} and {Manipulation} for {Constrained} {Placement} of {Unknown} {Objects}},
	url = {http://arxiv.org/abs/2006.15503},
	abstract = {Recent progress in robotic manipulation has dealt with the case of previously unknown objects in the context of relatively simple tasks, such as bin-picking. Existing methods for more constrained problems, however, such as deliberate placement in a tight region, depend more critically on shape information to achieve safe execution. This work deals with pickand-constrained placement of objects without access to geometric models. The objective is to pick an object and place it safely inside a desired goal region without any collisions, while minimizing the time and the sensing operations required to complete the task. An algorithmic framework is proposed for this purpose, which performs manipulation planning simultaneously over a conservative and an optimistic estimate of the object’s volume. The conservative estimate ensures that the manipulation is safe while the optimistic estimate guides the sensor-based manipulation process when no solution can be found for the conservative estimate. To maintain these estimates and dynamically update them during manipulation, objects are represented by a simple volumetric representation, which stores sets of occupied and unseen voxels. The effectiveness of the proposed approach is demonstrated by developing a robotic system that picks a previously unseen object from a table-top and places it in a constrained space. The system comprises of a dual-arm manipulator with heterogeneous endeffectors and leverages hand-offs as a re-grasping strategy. Realworld experiments show that straightforward pick-sense-andplace alternatives frequently fail to solve pick-and-constrained placement problems. The proposed pipeline, however, achieves more than 95\% success rate and faster execution times as evaluated over multiple physical experiments.},
	language = {en},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Mitash, Chaitanya and Shome, Rahul and Wen, Bowen and Boularias, Abdeslam and Bekris, Kostas},
	month = jun,
	year = {2020},
	note = {arXiv:2006.15503 [cs]},
	keywords = {Grasp},
	file = {Mitash 等。 - 2020 - Task-driven Perception and Manipulation for Constr.pdf:/home/red0orange/Zotero/storage/PN5WMA6X/Mitash 等。 - 2020 - Task-driven Perception and Manipulation for Constr.pdf:application/pdf},
}

@misc{murali_6-dof_2020,
	series = {Grasp},
	title = {6-{DOF} {Grasping} for {Target}-driven {Object} {Manipulation} in {Clutter}},
	url = {http://arxiv.org/abs/1912.03628},
	abstract = {Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3\%, outperforming baseline approaches by 17.6\% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible. Supplementary video can be found at https://youtu.be/w0B5S-gCsJk.},
	language = {en},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Murali, Adithyavairavan and Mousavian, Arsalan and Eppner, Clemens and Paxton, Chris and Fox, Dieter},
	month = may,
	year = {2020},
	note = {arXiv:1912.03628 [cs]},
	keywords = {Grasp},
	file = {Murali 等。 - 2020 - 6-DOF Grasping for Target-driven Object Manipulati.pdf:/home/red0orange/Zotero/storage/XF88L6SN/Murali 等。 - 2020 - 6-DOF Grasping for Target-driven Object Manipulati.pdf:application/pdf},
}

@misc{xu_learning_2022,
	title = {Learning to {Complete} {Object} {Shapes} for {Object}-level {Mapping} in {Dynamic} {Scenes}},
	url = {http://arxiv.org/abs/2208.05067},
	abstract = {In this paper, we propose a novel object-level mapping system that can simultaneously segment, track, and reconstruct objects in dynamic scenes. It can further predict and complete their full geometries by conditioning on reconstructions from depth inputs and a category-level shape prior with the aim that completed object geometry leads to better object reconstruction and tracking accuracy. For each incoming RGB-D frame, we perform instance segmentation to detect objects and build data associations between the detection and the existing object maps. A new object map will be created for each unmatched detection. For each matched object, we jointly optimise its pose and latent geometry representations using geometric residual and differential rendering residual towards its shape prior and completed geometry. Our approach shows better tracking and reconstruction performance compared to methods using traditional volumetric mapping or learned shape prior approaches. We evaluate its effectiveness by quantitatively and qualitatively testing it in both synthetic and real-world sequences.},
	language = {en},
	urldate = {2022-08-16},
	publisher = {arXiv},
	author = {Xu, Binbin and Davison, Andrew J. and Leutenegger, Stefan},
	month = aug,
	year = {2022},
	note = {arXiv:2208.05067 [cs]},
	file = {Xu 等。 - 2022 - Learning to Complete Object Shapes for Object-leve.pdf:/home/red0orange/Zotero/storage/6Q99V86S/Xu 等。 - 2022 - Learning to Complete Object Shapes for Object-leve.pdf:application/pdf},
}

@misc{deng_icaps_2021,
	title = {{iCaps}: {Iterative} {Category}-level {Object} {Pose} and {Shape} {Estimation}},
	shorttitle = {{iCaps}},
	url = {http://arxiv.org/abs/2201.00059},
	abstract = {This paper proposes a category-level 6D object pose and shape estimation approach iCaps1, which allows tracking 6D poses of unseen objects in a category and estimating their 3D shapes. We develop a category-level auto-encoder network using depth images as input, where feature embeddings from the auto-encoder encode poses of objects in a category. The auto-encoder can be used in a particle ﬁlter framework to estimate and track 6D poses of objects in a category. By exploiting an implicit shape representation based on signed distance functions, we build a LatentNet to estimate a latent representation of the 3D shape given the estimated pose of an object. Then the estimated pose and shape can be used to update each other in an iterative way. Our category-level 6D object pose and shape estimation pipeline only requires 2D detection and segmentation for initialization. We evaluate our approach on a publicly available dataset and demonstrate its effectiveness. In particular, our method achieves comparably high accuracy on shape estimation.},
	language = {en},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Deng, Xinke and Geng, Junyi and Bretl, Timothy and Xiang, Yu and Fox, Dieter},
	month = dec,
	year = {2021},
	note = {arXiv:2201.00059 [cs]},
	keywords = {Category},
	file = {Deng 等。 - 2021 - iCaps Iterative Category-level Object Pose and Sh.pdf:/home/red0orange/Zotero/storage/DXKJD2K6/Deng 等。 - 2021 - iCaps Iterative Category-level Object Pose and Sh.pdf:application/pdf},
}

@inproceedings{weng_captra_2021,
	address = {Montreal, QC, Canada},
	title = {{CAPTRA}: {CAtegory}-level {Pose} {Tracking} for {Rigid} and {Articulated} {Objects} from {Point} {Clouds}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{CAPTRA}},
	url = {https://ieeexplore.ieee.org/document/9711093/},
	doi = {10.1109/ICCV48922.2021.01296},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Weng, Yijia and Wang, He and Zhou, Qiang and Qin, Yuzhe and Duan, Yueqi and Fan, Qingnan and Chen, Baoquan and Su, Hao and Guibas, Leonidas J.},
	month = oct,
	year = {2021},
	keywords = {Category},
	pages = {13189--13198},
	file = {Weng 等。 - 2021 - CAPTRA CAtegory-level Pose Tracking for Rigid and.pdf:/home/red0orange/Zotero/storage/S37TFW26/Weng 等。 - 2021 - CAPTRA CAtegory-level Pose Tracking for Rigid and.pdf:application/pdf},
}

@inproceedings{lin_dualposenet_2021,
	address = {Montreal, QC, Canada},
	title = {{DualPoseNet}: {Category}-level {6D} {Object} {Pose} and {Size} {Estimation} {Using} {Dual} {Pose} {Network} with {Refined} {Learning} of {Pose} {Consistency}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{DualPoseNet}},
	url = {https://ieeexplore.ieee.org/document/9711092/},
	doi = {10.1109/ICCV48922.2021.00354},
	abstract = {Category-level 6D object pose and size estimation is to predict full pose configurations of rotation, translation, and size for object instances observed in single, arbitrary views of cluttered scenes. In this paper, we propose a new method of Dual Pose Network with refined learning of pose consistency for this task, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit decoder predicts object poses with a working mechanism different from that of the explicit one; they thus impose complementary supervision on the training of pose encoder. We construct the encoder based on spherical convolutions, and design a module of Spherical Fusion wherein for a better embedding of posesensitive features from the appearance and shape observations. Given no testing CAD models, it is the novel introduction of the implicit decoder that enables the refined pose prediction during testing, by enforcing the predicted pose consistency between the two decoders using a self-adaptive loss term. Thorough experiments on benchmarks of both category- and instance-level object pose datasets confirm efficacy of our designs. DualPoseNet outperforms existing methods with a large margin in the regime of high precision. Our code is released publicly at https://github. com/Gorilla-Lab-SCUT/DualPoseNet.},
	language = {en},
	urldate = {2022-08-17},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Lin, Jiehong and Wei, Zewei and Li, Zhihao and Xu, Songcen and Jia, Kui and Li, Yuanqing},
	month = oct,
	year = {2021},
	keywords = {Category},
	pages = {3540--3549},
	file = {Lin 等。 - 2021 - DualPoseNet Category-level 6D Object Pose and Siz.pdf:/home/red0orange/Zotero/storage/Y3WSGM86/Lin 等。 - 2021 - DualPoseNet Category-level 6D Object Pose and Siz.pdf:application/pdf},
}

@article{lin_sar-net_nodate,
	title = {{SAR}-{Net}: {Shape} {Alignment} and {Recovery} {Network} for {Category}-level {6D} {Object} {Pose} and {Size} {Estimation}},
	abstract = {Given a single scene image, this paper proposes a method of Category-level 6D Object Pose and Size Estimation (COPSE) from the point cloud of the target object, without external real pose-annotated training data. Specifically, beyond the visual cues in RGB images, we rely on the shape information predominately from the depth (D) channel. The key idea is to explore the shape alignment of each instance against its corresponding category-level template shape, and the symmetric correspondence of each object category for estimating a coarse 3D object shape. Our framework deforms the point cloud of the categorylevel template shape to align the observed instance point cloud for implicitly representing its 3D rotation. Then we model the symmetric correspondence by predicting symmetric point cloud from the partially observed point cloud. The concatenation of the observed point cloud and symmetric one reconstructs a coarse object shape, thus facilitating object center (3D translation) and 3D size estimation. Extensive experiments on the category-level NOCS benchmark demonstrate that our lightweight model still competes with state-of-the-art approaches that require labeled real-world images. We also deploy our approach to a physical Baxter robot to perform grasping tasks on unseen but categoryknown instances, and the results further validate the efficacy of our proposed model. Code and pre-trained models are available on the project webpage 1.},
	language = {en},
	author = {Lin, Haitao and Liu, Zichang and Cheang, Chilam and Fu, Yanwei and Guo, Guodong and Xue, Xiangyang},
	keywords = {Category},
	pages = {11},
	file = {Lin 等。 - SAR-Net Shape Alignment and Recovery Network for .pdf:/home/red0orange/Zotero/storage/R4SUCYXH/Lin 等。 - SAR-Net Shape Alignment and Recovery Network for .pdf:application/pdf},
}

@misc{fan_acr-pose_2021,
	title = {{ACR}-{Pose}: {Adversarial} {Canonical} {Representation} {Reconstruction} {Network} for {Category} {Level} {6D} {Object} {Pose} {Estimation}},
	shorttitle = {{ACR}-{Pose}},
	url = {http://arxiv.org/abs/2111.10524},
	abstract = {Recently, category-level 6D object pose estimation has achieved signiﬁcant improvements with the development of reconstructing canonical 3D representations. However, the reconstruction quality of existing methods is still far from excellent. In this paper, we propose a novel Adversarial Canonical Representation Reconstruction Network named ACR-Pose. ACR-Pose consists of a Reconstructor and a Discriminator. The Reconstructor is primarily composed of two novel sub-modules: Pose-Irrelevant Module (PIM) and Relational Reconstruction Module (RRM). PIM tends to learn canonical-related features to make the Reconstructor insensitive to rotation and translation, while RRM explores essential relational information between different input modalities to generate high-quality features. Subsequently, a Discriminator is employed to guide the Reconstructor to generate realistic canonical representations. The Reconstructor and the Discriminator learn to optimize through adversarial training. Experimental results on the prevalent NOCS-CAMERA and NOCS-REAL datasets demonstrate that our method achieves state-of-the-art performance.},
	language = {en},
	urldate = {2022-08-17},
	publisher = {arXiv},
	author = {Fan, Zhaoxin and Song, Zhengbo and Xu, Jian and Wang, Zhicheng and Wu, Kejian and Liu, Hongyan and He, Jun},
	month = nov,
	year = {2021},
	note = {arXiv:2111.10524 [cs]},
	keywords = {Category},
	file = {Fan 等。 - 2021 - ACR-Pose Adversarial Canonical Representation Rec.pdf:/home/red0orange/Zotero/storage/EG43U64G/Fan 等。 - 2021 - ACR-Pose Adversarial Canonical Representation Rec.pdf:application/pdf},
}

@article{tang_learning_nodate,
	title = {Learning {Task}-{Oriented} {Grasping} {Skills} from {Visual}-{Language} {Human} {Demonstration} - 1},
	language = {en},
	author = {Tang, Chao and Liu, Weiyu and Huang, Dehao and Meng, Lingxiao and Zhang, Hong},
	pages = {7},
	file = {Tang 等。 - Learning Task-Oriented Grasping Skills from Visual.pdf:/home/red0orange/Zotero/storage/P3UDBN78/Tang 等。 - Learning Task-Oriented Grasping Skills from Visual.pdf:application/pdf},
}

@article{tang_learning_nodate-1,
	title = {Learning {Task}-{Oriented} {Grasping} {Skills} from {Visual}-{Language} {Human} {Demonstration} - 2},
	language = {en},
	author = {Tang, Chao and Liu, Weiyu and Huang, Dehao and Meng, Lingxiao and Zhang, Hong},
	pages = {7},
	file = {Tang 等。 - Learning Task-Oriented Grasping Skills from Visual.pdf:/home/red0orange/Zotero/storage/2U27LWYA/Tang 等。 - Learning Task-Oriented Grasping Skills from Visual.pdf:application/pdf},
}

@article{tang_learning_nodate-2,
	title = {Learning {Task}-{Oriented} {Grasping} {Skills} from {Visual}-{Language} {Human} {Demonstration} - 3},
	language = {en},
	author = {Tang, Chao and Liu, Weiyu and Huang, Dehao and Meng, Lingxiao and Zhang, Hong},
	pages = {7},
	file = {统计信号处理基础-估计与检测理论.pdf:/home/red0orange/Zotero/storage/XW4RHRBL/统计信号处理基础-估计与检测理论.pdf:application/pdf;Tang 等。 - Learning Task-Oriented Grasping Skills from Visual.pdf:/home/red0orange/Zotero/storage/MI4VAQET/Tang 等。 - Learning Task-Oriented Grasping Skills from Visual.pdf:application/pdf},
}

@inproceedings{chen_fs-net_2021,
	address = {Nashville, TN, USA},
	title = {{FS}-{Net}: {Fast} {Shape}-based {Network} for {Category}-{Level} {6D} {Object} {Pose} {Estimation} with {Decoupled} {Rotation} {Mechanism}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{FS}-{Net}},
	url = {https://ieeexplore.ieee.org/document/9578410/},
	doi = {10.1109/CVPR46437.2021.00163},
	abstract = {In this paper, we focus on category-level 6D pose and size estimation from a monocular RGB-D image. Previous methods suffer from inefﬁcient category-level pose feature extraction, which leads to low accuracy and inference speed. To tackle this problem, we propose a fast shapebased network (FS-Net) with efﬁcient category-level feature extraction for 6D pose estimation. First, we design an orientation aware autoencoder with 3D graph convolution for latent feature extraction. Thanks to the shift and scaleinvariance properties of 3D graph convolution, the learned latent feature is insensitive to point shift and object size. Then, to efﬁciently decode category-level rotation information from the latent feature, we propose a novel decoupled rotation mechanism that employs two decoders to complementarily access the rotation information. For translation and size, we estimate them by two residuals: the difference between the mean of object points and ground truth translation, and the difference between the mean size of the category and ground truth size, respectively. Finally, to increase the generalization ability of the FS-Net, we propose an online box-cage based 3D deformation mechanism to augment the training data. Extensive experiments on two benchmark datasets show that the proposed method achieves state-ofthe-art performance in both category- and instance-level 6D object pose estimation. Especially in category-level pose estimation, without extra synthetic data, our method outperforms existing methods by 6.3\% on the NOCS-REAL dataset 1.},
	language = {en},
	urldate = {2022-09-10},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Wei and Jia, Xi and Chang, Hyung Jin and Duan, Jinming and Shen, Linlin and Leonardis, Ales},
	month = jun,
	year = {2021},
	pages = {1581--1590},
	file = {Chen 等。 - 2021 - FS-Net Fast Shape-based Network for Category-Leve.pdf:/home/red0orange/Zotero/storage/ZI589N58/Chen 等。 - 2021 - FS-Net Fast Shape-based Network for Category-Leve.pdf:application/pdf},
}

@inproceedings{wang_densefusion_2019,
	address = {Long Beach, CA, USA},
	title = {{DenseFusion}: {6D} {Object} {Pose} {Estimation} by {Iterative} {Dense} {Fusion}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{DenseFusion}},
	url = {https://ieeexplore.ieee.org/document/8953386/},
	doi = {10.1109/CVPR.2019.00346},
	abstract = {A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGBD images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose reﬁnement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose. Our code and video are available at https://sites.google.com/view/densefusion/.},
	language = {en},
	urldate = {2022-09-13},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Chen and Xu, Danfei and Zhu, Yuke and Martin-Martin, Roberto and Lu, Cewu and Fei-Fei, Li and Savarese, Silvio},
	month = jun,
	year = {2019},
	pages = {3338--3347},
	file = {Wang 等。 - 2019 - DenseFusion 6D Object Pose Estimation by Iterativ.pdf:/home/red0orange/Zotero/storage/UU6IU74F/Wang 等。 - 2019 - DenseFusion 6D Object Pose Estimation by Iterativ.pdf:application/pdf},
}

@misc{deng_self-supervised_2020,
	title = {Self-supervised {6D} {Object} {Pose} {Estimation} for {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/1909.10159},
	abstract = {To teach robots skills, it is crucial to obtain data with supervision. Since annotating real world data is timeconsuming and expensive, enabling robots to learn in a selfsupervised way is important. In this work, we introduce a robot system for self-supervised 6D object pose estimation. Starting from modules trained in simulation, our system is able to label real world images with accurate 6D object poses for self-supervised learning. In addition, the robot interacts with objects in the environment to change the object conﬁguration by grasping or pushing objects. In this way, our system is able to continuously collect data and improve its pose estimation modules. We show that the self-supervised learning improves object segmentation and 6D pose estimation performance, and consequently enables the system to grasp objects more reliably. A video showing the experiments can be found at https://youtu.be/W1Y0Mmh1Gd8.},
	language = {en},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Deng, Xinke and Xiang, Yu and Mousavian, Arsalan and Eppner, Clemens and Bretl, Timothy and Fox, Dieter},
	month = mar,
	year = {2020},
	note = {arXiv:1909.10159 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Deng 等。 - 2020 - Self-supervised 6D Object Pose Estimation for Robo.pdf:/home/red0orange/Zotero/storage/6J3W936Y/Deng 等。 - 2020 - Self-supervised 6D Object Pose Estimation for Robo.pdf:application/pdf},
}

@misc{mccormac_fusion_2018-1,
	title = {Fusion++: {Volumetric} {Object}-{Level} {SLAM}},
	shorttitle = {Fusion++},
	url = {http://arxiv.org/abs/1808.08378},
	abstract = {We propose an online object-level SLAM system which builds a persistent and accurate 3D graph map of arbitrary reconstructed objects. As an RGB-D camera browses a cluttered indoor scene, Mask-RCNN instance segmentations are used to initialise compact per-object Truncated Signed Distance Function (TSDF) reconstructions with object size-dependent resolutions and a novel 3D foreground mask. Reconstructed objects are stored in an optimisable 6DoF pose graph which is our only persistent map representation. Objects are incrementally reﬁned via depth fusion, and are used for tracking, relocalisation and loop closure detection. Loop closures cause adjustments in the relative pose estimates of object instances, but no intra-object warping. Each object also carries semantic information which is reﬁned over time and an existence probability to account for spurious instance predictions.},
	language = {en},
	urldate = {2022-09-15},
	publisher = {arXiv},
	author = {McCormac, John and Clark, Ronald and Bloesch, Michael and Davison, Andrew J. and Leutenegger, Stefan},
	month = aug,
	year = {2018},
	note = {arXiv:1808.08378 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {McCormac 等。 - 2018 - Fusion++ Volumetric Object-Level SLAM.pdf:/home/red0orange/Zotero/storage/XHA9RPSF/McCormac 等。 - 2018 - Fusion++ Volumetric Object-Level SLAM.pdf:application/pdf},
}

@misc{xu_mid-fusion_2019,
	title = {{MID}-{Fusion}: {Octree}-based {Object}-{Level} {Multi}-{Instance} {Dynamic} {SLAM}},
	shorttitle = {{MID}-{Fusion}},
	url = {http://arxiv.org/abs/1812.07976},
	abstract = {We propose a new multi-instance dynamic RGBD SLAM system using an object-level octree-based volumetric representation. It can provide robust camera tracking in dynamic environments and at the same time, continuously estimate geometric, semantic, and motion properties for arbitrary objects in the scene. For each incoming frame, we perform instance segmentation to detect objects and reﬁne mask boundaries using geometric and motion information. Meanwhile, we estimate the pose of each existing moving object using an object-oriented tracking method and robustly track the camera pose against the static scene. Based on the estimated camera pose and object poses, we associate segmented masks with existing models and incrementally fuse corresponding colour, depth, semantic, and foreground object probabilities into each object model. In contrast to existing approaches, our system is the ﬁrst system to generate an object-level dynamic volumetric map from a single RGB-D camera, which can be used directly for robotic tasks. Our method can run at 2-3 Hz on a CPU, excluding the instance segmentation part. We demonstrate its effectiveness by quantitatively and qualitatively testing it on both synthetic and real-world sequences.},
	language = {en},
	urldate = {2022-09-15},
	publisher = {arXiv},
	author = {Xu, Binbin and Li, Wenbin and Tzoumanikas, Dimos and Bloesch, Michael and Davison, Andrew and Leutenegger, Stefan},
	month = mar,
	year = {2019},
	note = {arXiv:1812.07976 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Xu 等。 - 2019 - MID-Fusion Octree-based Object-Level Multi-Instan.pdf:/home/red0orange/Zotero/storage/S6Y9S2TC/Xu 等。 - 2019 - MID-Fusion Octree-based Object-Level Multi-Instan.pdf:application/pdf},
}

@misc{fu_category-level_2022,
	title = {Category-{Level} {6D} {Object} {Pose} {Estimation} in the {Wild}: {A} {Semi}-{Supervised} {Learning} {Approach} and {A} {New} {Dataset}},
	shorttitle = {Category-{Level} {6D} {Object} {Pose} {Estimation} in the {Wild}},
	url = {http://arxiv.org/abs/2206.15436},
	abstract = {6D object pose estimation is one of the fundamental problems in computer vision and robotics research. While a lot of recent efforts have been made on generalizing pose estimation to novel object instances within the same category, namely category-level 6D pose estimation, it is still restricted in constrained environments given the limited number of annotated data. In this paper, we collect Wild6D, a new unlabeled RGBD object video dataset with diverse instances and backgrounds. We utilize this data to generalize category-level 6D object pose estimation in the wild with semi-supervised learning. We propose a new model, called Rendering for Pose estimation network RePoNet, that is jointly trained using the free ground-truths with the synthetic data, and a silhouette matching objective function on the real-world data. Without using any 3D annotations on real data, our method outperforms state-of-the-art methods on the previous dataset and our Wild6D test set (with manual annotations for evaluation) by a large margin. Project page with Wild6D data: https://oasisyang.github.io/semi-pose .},
	language = {en},
	urldate = {2022-09-17},
	publisher = {arXiv},
	author = {Fu, Yang and Wang, Xiaolong},
	month = jun,
	year = {2022},
	note = {arXiv:2206.15436 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Fu 和 Wang - 2022 - Category-Level 6D Object Pose Estimation in the Wi.pdf:/home/red0orange/Zotero/storage/L27JYMVK/Fu 和 Wang - 2022 - Category-Level 6D Object Pose Estimation in the Wi.pdf:application/pdf},
}

@inproceedings{park_deepsdf_2019,
	address = {Long Beach, CA, USA},
	title = {{DeepSDF}: {Learning} {Continuous} {Signed} {Distance} {Functions} for {Shape} {Representation}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{DeepSDF}},
	url = {https://ieeexplore.ieee.org/document/8954065/},
	doi = {10.1109/CVPR.2019.00025},
	abstract = {Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across ﬁdelity, efﬁciency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape’s surface by a continuous volumetric ﬁeld: the magnitude of a point in the ﬁeld represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape’s boundary as the zero-level-set of the learned function while explicitly representing the classiﬁcation of space as being part of the shapes’ interior or not. While classical SDF’s both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show stateof-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.},
	language = {en},
	urldate = {2022-09-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
	month = jun,
	year = {2019},
	pages = {165--174},
	file = {Park 等。 - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf:/home/red0orange/Zotero/storage/UBSA76PU/Park 等。 - 2019 - DeepSDF Learning Continuous Signed Distance Funct.pdf:application/pdf},
}

@inproceedings{chen_learning_2020,
	address = {Seattle, WA, USA},
	title = {Learning {Canonical} {Shape} {Space} for {Category}-{Level} {6D} {Object} {Pose} and {Size} {Estimation}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9156484/},
	doi = {10.1109/CVPR42600.2020.01199},
	abstract = {We present a novel approach to category-level 6D object pose and size estimation. To tackle intra-class shape variations, we learn canonical shape space (CASS), a uniﬁed representation for a large variety of instances of a certain object category. In particular, CASS is modeled as the latent space of a deep generative model of canonical 3D shapes with normalized pose. We train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated in normalized pose (with actual size), the encoder of the VAE learns view-factorized RGBD embedding. It maps an RGBD image in arbitrary view into a poseindependent 3D shape representation. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks. We integrate the learning of CASS and pose and size estimation into an end-to-end trainable network, achieving the state-of-the-art performance.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Dengsheng and Li, Jun and Wang, Zheng and Xu, Kai},
	month = jun,
	year = {2020},
	pages = {11970--11979},
	file = {Chen 等。 - 2020 - Learning Canonical Shape Space for Category-Level .pdf:/home/red0orange/Zotero/storage/2XIBXNEV/Chen 等。 - 2020 - Learning Canonical Shape Space for Category-Level .pdf:application/pdf},
}

@misc{sundermeyer_contact-graspnet_2021,
	series = {Contact-{GraspNet}},
	title = {Contact-{GraspNet}: {Efficient} 6-{DoF} {Grasp} {Generation} in {Cluttered} {Scenes}},
	shorttitle = {Contact-{GraspNet}},
	url = {http://arxiv.org/abs/2103.14127},
	abstract = {Grasping unseen objects in unconstrained, cluttered environments is an essential skill for autonomous robotic manipulation. Despite recent progress in full 6-DoF grasp learning, existing approaches often consist of complex sequential pipelines that possess several potential failure points and run-times unsuitable for closed-loop grasping. Therefore, we propose an end-to-end network that efﬁciently generates a distribution of 6-DoF parallel-jaw grasps directly from a depth recording of a scene. Our novel grasp representation treats 3D points of the recorded point cloud as potential grasp contacts. By rooting the full 6-DoF grasp pose and width in the observed point cloud, we can reduce the dimensionality of our grasp representation to 4-DoF which greatly facilitates the learning process. Our class-agnostic approach is trained on 17 million simulated grasps and generalizes well to real world sensor data. In a robotic grasping study of unseen objects in structured clutter we achieve over 90\% success rate, cutting the failure rate in half compared to a recent state-of-the-art method. Video of the real world experiments and code are available at https://research.nvidia.com/publication/ 2021-03\_Contact-GraspNet\%3A--Efficient.},
	language = {en},
	urldate = {2022-09-24},
	publisher = {arXiv},
	author = {Sundermeyer, Martin and Mousavian, Arsalan and Triebel, Rudolph and Fox, Dieter},
	month = mar,
	year = {2021},
	note = {arXiv:2103.14127 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Sundermeyer 等。 - 2021 - Contact-GraspNet Efficient 6-DoF Grasp Generation.pdf:/home/red0orange/Zotero/storage/9GDD4CQF/Sundermeyer 等。 - 2021 - Contact-GraspNet Efficient 6-DoF Grasp Generation.pdf:application/pdf},
}

@misc{murali_6-dof_2020-1,
	series = {Grasp},
	title = {6-{DOF} {Grasping} for {Target}-driven {Object} {Manipulation} in {Clutter}},
	url = {http://arxiv.org/abs/1912.03628},
	abstract = {Grasping in cluttered environments is a fundamental but challenging robotic skill. It requires both reasoning about unseen object parts and potential collisions with the manipulator. Most existing data-driven approaches avoid this problem by limiting themselves to top-down planar grasps which is insufficient for many real-world scenarios and greatly limits possible grasps. We present a method that plans 6-DOF grasps for any desired object in a cluttered scene from partial point cloud observations. Our method achieves a grasp success of 80.3\%, outperforming baseline approaches by 17.6\% and clearing 9 cluttered table scenes (which contain 23 unknown objects and 51 picks in total) on a real robotic platform. By using our learned collision checking module, we can even reason about effective grasp sequences to retrieve objects that are not immediately accessible. Supplementary video can be found at https://youtu.be/w0B5S-gCsJk.},
	language = {en},
	urldate = {2022-09-27},
	publisher = {arXiv},
	author = {Murali, Adithyavairavan and Mousavian, Arsalan and Eppner, Clemens and Paxton, Chris and Fox, Dieter},
	month = may,
	year = {2020},
	note = {arXiv:1912.03628 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Murali 等。 - 2020 - 6-DOF Grasping for Target-driven Object Manipulati.pdf:/home/red0orange/Zotero/storage/W3JCNX4G/Murali 等。 - 2020 - 6-DOF Grasping for Target-driven Object Manipulati.pdf:application/pdf},
}

@article{wang_neural_2020,
	title = {Neural {RRT}*: {Learning}-{Based} {Optimal} {Path} {Planning}},
	volume = {17},
	issn = {1545-5955, 1558-3783},
	shorttitle = {Neural {RRT}*},
	url = {https://ieeexplore.ieee.org/document/9037111/},
	doi = {10.1109/TASE.2020.2976560},
	abstract = {Rapidly random-exploring tree (RRT) and its variants are very popular due to their ability to quickly and efﬁciently explore the state space. However, they suffer sensitivity to the initial solution and slow convergence to the optimal solution, which means that they consume a lot of memory and time to ﬁnd the optimal path. It is critical to quickly ﬁnd a short path in many applications such as the autonomous vehicle with limited power/fuel. To overcome these limitations, we propose a novel optimal path planning algorithm based on the convolutional neural network (CNN), namely the neural RRT* (NRRT*). The NRRT* utilizes a nonuniform sampling distribution generated from a CNN model. The model is trained using quantities of successful path planning cases. In this article, we use the A* algorithm to generate the training data set consisting of the map information and the optimal path. For a given task, the proposed CNN model can predict the probability distribution of the optimal path on the map, which is used to guide the sampling process. The time cost and memory usage of the planned path are selected as the metric to demonstrate the effectiveness and efﬁciency of the NRRT*. The simulation results reveal that the NRRT* can achieve convincing performance compared with the state-of-the-art path planning algorithms.},
	language = {en},
	number = {4},
	urldate = {2022-10-10},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Wang, Jiankun and Chi, Wenzheng and Li, Chenming and Wang, Chaoqun and Meng, Max Q.-H.},
	month = oct,
	year = {2020},
	pages = {1748--1758},
	file = {Wang 等。 - 2020 - Neural RRT Learning-Based Optimal Path Planning.pdf:/home/red0orange/Zotero/storage/57BDVK69/Wang 等。 - 2020 - Neural RRT Learning-Based Optimal Path Planning.pdf:application/pdf},
}

@article{elbanhawi_sampling-based_2014,
	title = {Sampling-{Based} {Robot} {Motion} {Planning}: {A} {Review}},
	volume = {2},
	issn = {2169-3536},
	shorttitle = {Sampling-{Based} {Robot} {Motion} {Planning}},
	url = {https://ieeexplore.ieee.org/document/6722915/},
	doi = {10.1109/ACCESS.2014.2302442},
	abstract = {Motion planning is a fundamental research area in robotics. Sampling-based methods offer an efﬁcient solution for what is otherwise a rather challenging dilemma of path planning. Consequently, these methods have been extended further away from basic robot planning into further difﬁcult scenarios and diverse applications. A comprehensive survey of the growing body of work in sampling-based planning is given here. Simulations are executed to evaluate some of the proposed planners and highlight some of the implementation details that are often left unspeciﬁed. An emphasis is placed on contemporary research directions in this ﬁeld. We address planners that tackle current issues in robotics. For instance, real-life kinodynamic planning, optimal planning, replanning in dynamic environments, and planning under uncertainty are discussed. The aim of this paper is to survey the state of the art in motion planning and to assess selected planners, examine implementation details and above all shed a light on the current challenges in motion planning and the promising approaches that will potentially overcome those problems.},
	language = {en},
	urldate = {2022-10-10},
	journal = {IEEE Access},
	author = {Elbanhawi, Mohamed and Simic, Milan},
	year = {2014},
	pages = {56--77},
	file = {Elbanhawi 和 Simic - 2014 - Sampling-Based Robot Motion Planning A Review.pdf:/home/red0orange/Zotero/storage/9J37D82N/Elbanhawi 和 Simic - 2014 - Sampling-Based Robot Motion Planning A Review.pdf:application/pdf},
}

@article{wang_survey_2021,
	title = {A survey of learning‐based robot motion planning},
	volume = {3},
	issn = {2631-6315, 2631-6315},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/csy2.12020},
	doi = {10.1049/csy2.12020},
	abstract = {A fundamental task in robotics is to plan collision‐free motions among a set of obstacles. Recently, learning‐based motion‐planning methods have shown significant advantages in solving different planning problems in high‐dimensional spaces and complex environments. This article serves as a survey of various different learning‐based methods that have been applied to robot motion‐planning problems, including supervised, unsupervised learning, and reinforcement learning. These learning‐based methods either rely on a human‐crafted reward function for specific tasks or learn from successful planning experiences. The classical definition and learning‐related definition of motion‐planning problem are provided in this article. Different learning‐based motion‐planning algorithms are introduced, and the combination of classical motion‐planning and learning techniques is discussed in detail.},
	language = {en},
	number = {4},
	urldate = {2022-10-10},
	journal = {IET Cyber-Systems and Robotics},
	author = {Wang, Jiankun and Zhang, Tianyi and Ma, Nachuan and Li, Zhaoting and Ma, Han and Meng, Fei and Meng, Max Q.‐H.},
	month = dec,
	year = {2021},
	pages = {302--314},
	file = {Wang 等。 - 2021 - A survey of learning‐based robot motion planning.pdf:/home/red0orange/Zotero/storage/6F2WREXI/Wang 等。 - 2021 - A survey of learning‐based robot motion planning.pdf:application/pdf},
}

@misc{kapelyukh_dall-e-bot_2022,
	title = {{DALL}-{E}-{Bot}: {Introducing} {Web}-{Scale} {Diffusion} {Models} to {Robotics}},
	shorttitle = {{DALL}-{E}-{Bot}},
	url = {http://arxiv.org/abs/2210.02438},
	abstract = {We introduce the ﬁrst work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by ﬁrst inferring a text description of those objects, then generating an image representing a natural, human-like arrangement of those objects, and ﬁnally physically arranging the objects according to that image. The signiﬁcance is that we achieve this zero-shot using DALLE, without needing any further data collection or training. Encouraging real-world results with human studies show that this is an exciting direction for the future of web-scale robot learning algorithms. We also propose a list of recommendations to the text-to-image community, to align further developments of these models with applications to robotics. Videos are available at: https://www.robot-learning.uk/dall-e-bot.},
	language = {en},
	urldate = {2022-10-11},
	publisher = {arXiv},
	author = {Kapelyukh, Ivan and Vosylius, Vitalis and Johns, Edward},
	month = oct,
	year = {2022},
	note = {arXiv:2210.02438 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Kapelyukh 等。 - 2022 - DALL-E-Bot Introducing Web-Scale Diffusion Models.pdf:/home/red0orange/Zotero/storage/55K4A7W5/Kapelyukh 等。 - 2022 - DALL-E-Bot Introducing Web-Scale Diffusion Models.pdf:application/pdf},
}

@article{sunderhauf_limits_2018,
	title = {The limits and potentials of deep learning for robotics},
	volume = {37},
	issn = {0278-3649, 1741-3176},
	url = {http://journals.sagepub.com/doi/10.1177/0278364918770733},
	doi = {10.1177/0278364918770733},
	abstract = {The application of deep learning in robotics leads to very speciﬁc problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of roboticsspeciﬁc learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulﬁll the promising potentials of deep learning in robotics.},
	language = {en},
	number = {4-5},
	urldate = {2022-10-11},
	journal = {The International Journal of Robotics Research},
	author = {Sünderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, Jürgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and Corke, Peter},
	month = apr,
	year = {2018},
	pages = {405--420},
	file = {Sünderhauf 等。 - 2018 - The limits and potentials of deep learning for rob.pdf:/home/red0orange/Zotero/storage/Y4RG3PXV/Sünderhauf 等。 - 2018 - The limits and potentials of deep learning for rob.pdf:application/pdf},
}

@article{wada_robotic_nodate,
	title = {Robotic {Manipulation} in {Clutter} with {Object}-{Level} {Semantic} {Mapping}},
	language = {en},
	author = {Wada, Kentaro},
	pages = {166},
	file = {Wada - Robotic Manipulation in Clutter with Object-Level .pdf:/home/red0orange/Zotero/storage/JSHRIID5/Wada - Robotic Manipulation in Clutter with Object-Level .pdf:application/pdf},
}

@inproceedings{zhang_tax-pose_nodate,
	title = {{TAX}-{Pose}: {Task}-{Specific} {Cross}-{Pose} {Estimation} for {Robot} {Manipulation}},
	booktitle = {6th {Annual} {Conference} on {Robot} {Learning}},
	author = {Zhang, Harry and Eisner, Ben and Pan, Chuer and Okorn, Brian and Held, David},
	file = {tax_pose_task_specific_cross_p (1).pdf:/home/red0orange/Zotero/storage/6XZNN3JM/tax_pose_task_specific_cross_p (1).pdf:application/pdf},
}

@misc{breyer_closed-loop_2022,
	series = {实验出论文，仅是用以往工作解决一个工程优雅问题},
	title = {Closed-{Loop} {Next}-{Best}-{View} {Planning} for {Target}-{Driven} {Grasping}},
	url = {http://arxiv.org/abs/2207.10543},
	abstract = {Picking a speciﬁc object from clutter is an essential component of many manipulation tasks. Partial observations often require the robot to collect additional views of the scene before attempting a grasp. This paper proposes a closedloop next-best-view planner that drives exploration based on occluded object parts. By continuously predicting grasps from an up-to-date scene reconstruction, our policy can decide online to ﬁnalize a grasp execution or to adapt the robot’s trajectory for further exploration. We show that our reactive approach decreases execution times without loss of grasp success rates compared to common camera placements and handles situations where the ﬁxed baselines fail. Video and code are available at https://github.com/ethz-asl/active\_grasp.},
	language = {en},
	urldate = {2022-10-25},
	publisher = {arXiv},
	author = {Breyer, Michel and Ott, Lionel and Siegwart, Roland and Chung, Jen Jen},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10543 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Breyer 等。 - 2022 - Closed-Loop Next-Best-View Planning for Target-Dri.pdf:/home/red0orange/Zotero/storage/WU2W26Z3/Breyer 等。 - 2022 - Closed-Loop Next-Best-View Planning for Target-Dri.pdf:application/pdf},
}

@misc{wu_transporters_2022,
	title = {Transporters with {Visual} {Foresight} for {Solving} {Unseen} {Rearrangement} {Tasks}},
	url = {http://arxiv.org/abs/2202.10765},
	abstract = {Rearrangement tasks have been identified as a crucial challenge for intelligent robotic manipulation, but few methods allow for precise construction of unseen structures. We propose a visual foresight model for pick-and-place rearrangement manipulation which is able to learn efficiently. In addition, we develop a multi-modal action proposal module which builds on the Goal-Conditioned Transporter Network, a state-of-the-art imitation learning method. Our image-based task planning method, Transporters with Visual Foresight, is able to learn from only a handful of data and generalize to multiple unseen tasks in a zero-shot manner. TVF is able to improve the performance of a state-of-the-art imitation learning method on unseen tasks in simulation and real robot experiments. In particular, the average success rate on unseen tasks improves from 55.4\% to 78.5\% in simulation experiments and from 30\% to 63.3\% in real robot experiments when given only tens of expert demonstrations. Video and code are available on our project website: https://chirikjianlab.github.io/tvf/},
	language = {en},
	urldate = {2022-10-27},
	publisher = {arXiv},
	author = {Wu, Hongtao and Ye, Jikai and Meng, Xin and Paxton, Chris and Chirikjian, Gregory},
	month = jul,
	year = {2022},
	note = {arXiv:2202.10765 [cs]},
	keywords = {未读},
	file = {Wu 等。 - 2022 - Transporters with Visual Foresight for Solving Uns.pdf:/home/red0orange/Zotero/storage/5B62FPAR/Wu 等。 - 2022 - Transporters with Visual Foresight for Solving Uns.pdf:application/pdf},
}

@inproceedings{wu_put_2022,
	title = {Put the {Bear} on the {Chair}! {Intelligent} {Robot} {Interaction} with {Previously} {Unseen} {Chairs} via {Robot} {Imagination}},
	doi = {10.1109/ICRA46639.2022.9811619},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Wu, Hongtao and Meng, Xin and Ruan, Sipu and Chirikjian, Gregory S.},
	year = {2022},
	keywords = {未读},
	pages = {6276--6282},
	file = {Put_the_Bear_on_the_Chair_Intelligent_Robot_Interaction_with_Previously_Unseen_Chairs_via_Robot_Imagination.pdf:/home/red0orange/Zotero/storage/7VYG4ZIM/Put_the_Bear_on_the_Chair_Intelligent_Robot_Interaction_with_Previously_Unseen_Chairs_via_Robot_Imagination.pdf:application/pdf},
}

@misc{wen_catgrasp_2022-1,
	title = {{CaTGrasp}: {Learning} {Category}-{Level} {Task}-{Relevant} {Grasping} in {Clutter} from {Simulation}},
	shorttitle = {{CaTGrasp}},
	url = {http://arxiv.org/abs/2109.09163},
	abstract = {Task-relevant grasping is critical for industrial assembly, where downstream manipulation tasks constrain the set of valid grasps. Learning how to perform this task, however, is challenging, since task-relevant grasp labels are hard to deﬁne and annotate. There is also yet no consensus on proper representations for modeling or oﬀ-the-shelf tools for performing task-relevant grasps. This work proposes a framework to learn task-relevant grasping for industrial objects without the need of time-consuming real-world data collection or manual annotation. To achieve this, the entire framework is trained solely in simulation, including supervised training with synthetic label generation and self-supervised, hand-object interaction. In the context of this framework, this paper proposes a novel, object-centric canonical representation at the category level, which allows establishing dense correspondence across object instances and transferring task-relevant grasps to novel instances. Extensive experiments on task-relevant grasping of densely-cluttered industrial objects are conducted in both simulation and real-world setups, demonstrating the eﬀectiveness of the proposed framework. Code and data are available at https://sites.google.com/view/catgrasp.},
	language = {en},
	urldate = {2022-10-29},
	publisher = {arXiv},
	author = {Wen, Bowen and Lian, Wenzhao and Bekris, Kostas and Schaal, Stefan},
	month = feb,
	year = {2022},
	note = {arXiv:2109.09163 [cs, eess]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {Wen 等。 - 2022 - CaTGrasp Learning Category-Level Task-Relevant Gr.pdf:/home/red0orange/Zotero/storage/R4K6CCX9/Wen 等。 - 2022 - CaTGrasp Learning Category-Level Task-Relevant Gr.pdf:application/pdf},
}

@misc{rosinol_kimera_2020,
	title = {Kimera: an {Open}-{Source} {Library} for {Real}-{Time} {Metric}-{Semantic} {Localization} and {Mapping}},
	shorttitle = {Kimera},
	url = {http://arxiv.org/abs/1910.02490},
	abstract = {We provide an open-source C++ library for realtime metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINSMono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the ﬂexibility, computational efﬁciency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
	language = {en},
	urldate = {2022-10-31},
	publisher = {arXiv},
	author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
	month = mar,
	year = {2020},
	note = {arXiv:1910.02490 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Rosinol 等。 - 2020 - Kimera an Open-Source Library for Real-Time Metri.pdf:/home/red0orange/Zotero/storage/RDDGETW3/Rosinol 等。 - 2020 - Kimera an Open-Source Library for Real-Time Metri.pdf:application/pdf},
}

@misc{zobeidi_deep_2021,
	series = {{SDDF}},
	title = {A {Deep} {Signed} {Directional} {Distance} {Function} for {Object} {Shape} {Representation}},
	url = {http://arxiv.org/abs/2107.11024},
	abstract = {Neural networks that map 3D coordinates to signed distance function (SDF) or occupancy values have enabled high-ﬁdelity implicit representations of object shape. This paper develops a new shape model that allows synthesizing novel distance views by optimizing a continuous signed directional distance function (SDDF). Similar to deep SDF models, our SDDF formulation can represent whole categories of shapes and complete or interpolate across shapes from partial input data. Unlike an SDF, which measures distance to the nearest surface in any direction, an SDDF measures distance in a given direction. This allows training an SDDF model without 3D shape supervision, using only distance measurements, readily available from depth camera or Lidar sensors. Our model also removes postprocessing steps like surface extraction or rendering by directly predicting distance at arbitrary locations and viewing directions. Unlike deep view-synthesis techniques, such as Neural Radiance Fields, which train high-capacity blackbox models, our model encodes by construction the property that SDDF values decrease linearly along the viewing direction. This structure constraint not only results in dimensionality reduction but also provides analytical conﬁdence about the accuracy of SDDF predictions, regardless of the distance to the object surface.},
	language = {en},
	urldate = {2022-11-01},
	publisher = {arXiv},
	author = {Zobeidi, Ehsan and Atanasov, Nikolay},
	month = dec,
	year = {2021},
	note = {arXiv:2107.11024 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zobeidi 和 Atanasov - 2021 - A Deep Signed Directional Distance Function for Ob.pdf:/home/red0orange/Zotero/storage/43P3FKWU/Zobeidi 和 Atanasov - 2021 - A Deep Signed Directional Distance Function for Ob.pdf:application/pdf},
}

@misc{tremblay_deep_2018,
	series = {{DOPE}},
	title = {Deep {Object} {Pose} {Estimation} for {Semantic} {Robotic} {Grasping} of {Household} {Objects}},
	url = {http://arxiv.org/abs/1809.10790},
	abstract = {Using synthetic data for training deep neural networks for robotic manipulation holds the promise of an almost unlimited amount of pre-labeled training data, generated safely out of harm's way. One of the key challenges of synthetic data, to date, has been to bridge the so-called reality gap, so that networks trained on synthetic data operate correctly when exposed to real-world data. We explore the reality gap in the context of 6-DoF pose estimation of known objects from a single RGB image. We show that for this problem the reality gap can be successfully spanned by a simple combination of domain randomized and photorealistic data. Using synthetic data generated in this manner, we introduce a one-shot deep neural network that is able to perform competitively against a state-of-the-art network trained on a combination of real and synthetic data. To our knowledge, this is the first deep network trained only on synthetic data that is able to achieve state-of-the-art performance on 6-DoF object pose estimation. Our network also generalizes better to novel environments including extreme lighting conditions, for which we show qualitative results. Using this network we demonstrate a real-time system estimating object poses with sufficient accuracy for real-world semantic grasping of known household objects in clutter by a real robot.},
	language = {en},
	urldate = {2022-11-05},
	publisher = {arXiv},
	author = {Tremblay, Jonathan and To, Thang and Sundaralingam, Balakumar and Xiang, Yu and Fox, Dieter and Birchfield, Stan},
	month = sep,
	year = {2018},
	note = {arXiv:1809.10790 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Tremblay 等 - 2018 - Deep Object Pose Estimation for Semantic Robotic G.pdf:/home/red0orange/Zotero/storage/E4Q97WYC/Tremblay 等 - 2018 - Deep Object Pose Estimation for Semantic Robotic G.pdf:application/pdf},
}

@inproceedings{tang_selective_nodate,
	series = {{CoRL2022}},
	title = {Selective {Object} {Rearrangement} in {Clutter}},
	booktitle = {6th {Annual} {Conference} on {Robot} {Learning}},
	author = {Tang, Bingjie and Sukhatme, Gaurav S},
	file = {Selective _lutter.pdf:/home/red0orange/Zotero/storage/MDDVVULA/Selective _lutter.pdf:application/pdf},
}

@inproceedings{fang_graspnet-1billion_2020,
	address = {Seattle, WA, USA},
	series = {{GraspNet}-1billion},
	title = {{GraspNet}-{1Billion}: {A} {Large}-{Scale} {Benchmark} for {General} {Object} {Grasping}},
	volume = {GraspNet-1billion},
	isbn = {978-1-72817-168-5},
	shorttitle = {{GraspNet}-{1Billion}},
	url = {https://ieeexplore.ieee.org/document/9156992/},
	doi = {10.1109/CVPR42600.2020.01146},
	abstract = {Object grasping is critical for many applications, which is also a challenging computer vision problem. However, for cluttered scene, current researches suffer from the problems of insufﬁcient training data and the lacking of evaluation benchmarks. In this work, we contribute a largescale grasp pose detection dataset with a uniﬁed evaluation system. Our dataset contains 97,280 RGB-D image with over one billion grasp poses. Meanwhile, our evaluation system directly reports whether a grasping is successful by analytic computation, which is able to evaluate any kind of grasp poses without exhaustively labeling ground-truth. In addition, we propose an end-to-end grasp pose prediction network given point cloud inputs, where we learn approaching direction and operation parameters in a decoupled manner. A novel grasp afﬁnity ﬁeld is also designed to improve the grasping robustness. We conduct extensive experiments to show that our dataset and evaluation system can align well with real-world experiments and our proposed network achieves the state-of-the-art performance. Our dataset, source code and models are publicly available at www.graspnet.net.},
	language = {en},
	urldate = {2022-11-11},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Fang, Hao-Shu and Wang, Chenxi and Gou, Minghao and Lu, Cewu},
	month = jun,
	year = {2020},
	pages = {11441--11450},
	file = {Fang 等 - 2020 - GraspNet-1Billion A Large-Scale Benchmark for Gen.pdf:/home/red0orange/Zotero/storage/AUQDTBYF/Fang 等 - 2020 - GraspNet-1Billion A Large-Scale Benchmark for Gen.pdf:application/pdf},
}

@article{du_vision-based_2021,
	title = {Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review},
	volume = {54},
	issn = {0269-2821, 1573-7462},
	shorttitle = {Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers},
	url = {https://link.springer.com/10.1007/s10462-020-09888-5},
	doi = {10.1007/s10462-020-09888-5},
	abstract = {This paper presents a comprehensive survey on vision-based robotic grasping. We conclude three key tasks during vision-based robotic grasping, which are object localization, object pose estimation and grasp estimation. In detail, the object localization task contains object localization without classification, object detection and object instance segmentation. This task provides the regions of the target object in the input data. The object pose estimation task mainly refers to estimating the 6D object pose and includes correspondence-based methods, template-based methods and voting-based methods, which affords the generation of grasp poses for known objects. The grasp estimation task includes 2D planar grasp methods and 6DoF grasp methods, where the former is constrained to grasp from one direction. These three tasks could accomplish the robotic grasping with different combinations. Lots of object pose estimation methods need not object localization, and they conduct object localization and object pose estimation jointly. Lots of grasp estimation methods need not object localization and object pose estimation, and they conduct grasp estimation in an end-to-end manner. Both traditional methods and latest deep learning-based methods based on the RGB-D image inputs are reviewed elaborately in this survey. Related datasets and comparisons between state-of-the-art methods are summarized as well. In addition, challenges about vision-based robotic grasping and future directions in addressing these challenges are also pointed out.},
	language = {en},
	number = {3},
	urldate = {2022-11-17},
	journal = {Artificial Intelligence Review},
	author = {Du, Guoguang and Wang, Kai and Lian, Shiguo and Zhao, Kaiyong},
	month = mar,
	year = {2021},
	pages = {1677--1734},
	file = {Du 等 - 2021 - Vision-based robotic grasping from object localiza.pdf:/home/red0orange/Zotero/storage/YPFXRKBG/Du 等 - 2021 - Vision-based robotic grasping from object localiza.pdf:application/pdf},
}

@article{goodwin_you_nodate,
	series = {{CoRL2022}},
	title = {You {Only} {Look} at {One}: {Category}-{Level} {Object} {Representations} for {Pose} {Estimation} {From} a {Single} {Example}},
	abstract = {In order to meaningfully interact with the world, robot manipulators must be able to interpret objects they encounter. A critical aspect of this interpretation is pose estimation: inferring quantities that describe the position and orientation of an object in 3D space. Most existing approaches to pose estimation make limiting assumptions, often working only for specific, known object instances, or at best generalising to an object category using large pose-labelled datasets. In this work, we present a method for achieving category-level pose estimation by inspection of just a single object from a desired category. We show that we can subsequently perform accurate pose estimation for unseen objects from an inspected category, and considerably outperform prior work by exploiting multi-view correspondences. We demonstrate that our method runs in real-time, enabling a robot manipulator equipped with an RGBD sensor to perform online 6D pose estimation for novel objects. Finally, we showcase our method in a continual learning setting, with a robot able to determine whether objects belong to known categories, and if not, use active perception to produce a one-shot category representation for subsequent pose estimation.},
	language = {en},
	author = {Goodwin, Walter and Havoutis, Ioannis and Posner, Ingmar},
	pages = {11},
	file = {Goodwin 等 - You Only Look at One Category-Level Object Repres.pdf:/home/red0orange/Zotero/storage/SHP84BD6/Goodwin 等 - You Only Look at One Category-Level Object Repres.pdf:application/pdf},
}

@article{wen_beyond_nodate,
	title = {{BEYOND} {INSTANCE}-{LEVEL} {REASONING} {IN} {OBJECT} {POSE} {ESTIMATION} {AND} {TRACKING} {FOR} {ROBOTIC} {MANIPULATION}},
	language = {en},
	author = {Wen, Bowen},
	pages = {181},
	file = {Wen - BEYOND INSTANCE-LEVEL REASONING IN OBJECT POSE EST.pdf:/home/red0orange/Zotero/storage/8ZYA2DEQ/Wen - BEYOND INSTANCE-LEVEL REASONING IN OBJECT POSE EST.pdf:application/pdf},
}

@article{pennartz_indicators_2019,
	title = {Indicators and {Criteria} of {Consciousness} in {Animals} and {Intelligent} {Machines}: {An} {Inside}-{Out} {Approach}},
	volume = {13},
	issn = {1662-5137},
	shorttitle = {Indicators and {Criteria} of {Consciousness} in {Animals} and {Intelligent} {Machines}},
	url = {https://www.frontiersin.org/article/10.3389/fnsys.2019.00025/full},
	doi = {10.3389/fnsys.2019.00025},
	language = {en},
	urldate = {2022-11-18},
	journal = {Frontiers in Systems Neuroscience},
	author = {Pennartz, Cyriel M. A. and Farisco, Michele and Evers, Kathinka},
	month = jul,
	year = {2019},
	pages = {25},
	file = {Pennartz 等 - 2019 - Indicators and Criteria of Consciousness in Animal.pdf:/home/red0orange/Zotero/storage/EAT4TTHG/Pennartz 等 - 2019 - Indicators and Criteria of Consciousness in Animal.pdf:application/pdf},
}

@misc{danielczuk_object_2021-2,
	series = {{SceneCollisionNet}},
	title = {Object {Rearrangement} {Using} {Learned} {Implicit} {Collision} {Functions}},
	url = {http://arxiv.org/abs/2011.10726},
	abstract = {Robotic object rearrangement combines the skills of picking and placing objects. When object models are unavailable, typical collision-checking models may be unable to predict collisions in partial point clouds with occlusions, making generation of collision-free grasping or placement trajectories challenging. We propose a learned collision model that accepts scene and query object point clouds and predicts collisions for 6DOF object poses within the scene. We train the model on a synthetic set of 1 million scene/object point cloud pairs and 2 billion collision queries. We leverage the learned collision model as part of a model predictive path integral (MPPI) policy in a tabletop rearrangement task and show that the policy can plan collision-free grasps and placements for objects unseen in training in both simulated and physical cluttered scenes with a Franka Panda robot. The learned model outperforms both traditional pipelines and learned ablations by 9.8\% in accuracy on a dataset of simulated collision queries and is 75x faster than the best-performing baseline. Videos and supplementary material are available at https://research.nvidia.com/publication/ 2021-03\_Object-Rearrangement-Using.},
	language = {en},
	urldate = {2022-11-20},
	publisher = {arXiv},
	author = {Danielczuk, Michael and Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter},
	month = mar,
	year = {2021},
	note = {arXiv:2011.10726 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Danielczuk 等 - 2021 - Object Rearrangement Using Learned Implicit Collis.pdf:/home/red0orange/Zotero/storage/SUAU8KXV/Danielczuk 等 - 2021 - Object Rearrangement Using Learned Implicit Collis.pdf:application/pdf},
}

@misc{lin_multi-view_2021-1,
	series = {Explicit {Scene} {Representation}},
	title = {Multi-{View} {Fusion} for {Multi}-{Level} {Robotic} {Scene} {Understanding}},
	url = {http://arxiv.org/abs/2103.13539},
	abstract = {We present a system for multi-level scene awareness for robotic manipulation. Given a sequence of camera-inhand RGB images, the system calculates three types of information: 1) a point cloud representation of all the surfaces in the scene, for the purpose of obstacle avoidance; 2) the rough pose of unknown objects from categories corresponding to primitive shapes (e.g., cuboids and cylinders); and 3) full 6-DoF pose of known objects. By developing and fusing recent techniques in these domains, we provide a rich scene representation for robot awareness. We demonstrate the importance of each of these modules, their complementary nature, and the potential beneﬁts of the system in the context of robotic manipulation.},
	language = {en},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Lin, Yunzhi and Tremblay, Jonathan and Tyree, Stephen and Vela, Patricio A. and Birchfield, Stan},
	month = oct,
	year = {2021},
	note = {arXiv:2103.13539 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Lin 等 - 2021 - Multi-View Fusion for Multi-Level Robotic Scene Un.pdf:/home/red0orange/Zotero/storage/JFCA3TNA/Lin 等 - 2021 - Multi-View Fusion for Multi-Level Robotic Scene Un.pdf:application/pdf},
}

@misc{lu_online_2022,
	title = {Online {Object} {Model} {Reconstruction} and {Reuse} for {Lifelong} {Improvement} of {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/2109.13910},
	abstract = {This work proposes a robotic pipeline for picking and constrained placement of objects without geometric shape priors. Compared to recent efforts developed for similar tasks, where every object was assumed to be novel, the proposed system recognizes previously manipulated objects and performs online model reconstruction and reuse. Over a lifelong manipulation process, the system keeps learning features of objects it has interacted with and updates their reconstructed models. Whenever an instance of a previously manipulated object reappears, the system aims to ﬁrst recognize it and then register its previously reconstructed model given the current observation. This step greatly reduces object shape uncertainty allowing the system to even reason for parts of objects, which are currently not observable. This also results in better manipulation efﬁciency as it reduces the need for active perception of the target object during manipulation. To get a reusable reconstructed model, the proposed pipeline adopts: i) TSDF for object representation, and ii) a variant of the standard particle ﬁlter algorithm for pose estimation and tracking of the partial object model. Furthermore, an effective way to construct and maintain a dataset of manipulated objects is presented. A sequence of real-world manipulation experiments is performed. They show how future manipulation tasks become more effective and efﬁcient by reusing reconstructed models of previously manipulated objects, which were generated during their prior manipulation, instead of treating objects as novel every time.},
	language = {en},
	urldate = {2022-11-21},
	publisher = {arXiv},
	author = {Lu, Shiyang and Wang, Rui and Miao, Yinglong and Mitash, Chaitanya and Bekris, Kostas},
	month = may,
	year = {2022},
	note = {arXiv:2109.13910 [cs]},
	keywords = {Online, Active, Reconstruction},
	file = {Lu 等 - 2022 - Online Object Model Reconstruction and Reuse for L.pdf:/home/red0orange/Zotero/storage/ZPDZ2MJD/Lu 等 - 2022 - Online Object Model Reconstruction and Reuse for L.pdf:application/pdf},
}

@article{noauthor__nodate-1,
	title = {基于深度学习的机械臂视觉抓取综述},
	language = {zh},
	pages = {24},
	file = {基于深度学习的机械臂视觉抓取综述.pdf:/home/red0orange/Zotero/storage/WIV2ZXGB/基于深度学习的机械臂视觉抓取综述.pdf:application/pdf},
}

@misc{shridhar_perceiver-actor_2022,
	title = {Perceiver-{Actor}: {A} {Multi}-{Task} {Transformer} for {Robotic} {Manipulation}},
	shorttitle = {Perceiver-{Actor}},
	url = {http://arxiv.org/abs/2209.05451},
	abstract = {Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive. Can manipulation still beneﬁt from Transformers with the right problem formulation? We investigate this question with PERACT, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. PERACT encodes language goals and RGB-D voxel observations with a Perceiver Transformer [1], and outputs discretized actions by “detecting the next best voxel action”. Unlike frameworks that operate on 2D images, the voxelized 3D observation and action space provides a strong structural prior for efﬁciently learning 6-DoF actions. With this formulation, we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task. Our results show that PERACT signiﬁcantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.},
	language = {en},
	urldate = {2022-11-25},
	publisher = {arXiv},
	author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
	month = nov,
	year = {2022},
	note = {arXiv:2209.05451 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Shridhar 等 - 2022 - Perceiver-Actor A Multi-Task Transformer for Robo.pdf:/home/red0orange/Zotero/storage/ULDYEW8C/Shridhar 等 - 2022 - Perceiver-Actor A Multi-Task Transformer for Robo.pdf:application/pdf},
}

@misc{zeng_multi-view_2017,
	title = {Multi-view {Self}-supervised {Deep} {Learning} for {6D} {Pose} {Estimation} in the {Amazon} {Picking} {Challenge}},
	url = {http://arxiv.org/abs/1609.09475},
	abstract = {Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC) [1]. A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multiview RGB-D data and self-supervised, data-driven learning to overcome those difﬁculties. The approach was part of the MITPrinceton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016.},
	language = {en},
	urldate = {2022-11-25},
	publisher = {arXiv},
	author = {Zeng, Andy and Yu, Kuan-Ting and Song, Shuran and Suo, Daniel and Walker Jr., Ed and Rodriguez, Alberto and Xiao, Jianxiong},
	month = may,
	year = {2017},
	note = {arXiv:1609.09475 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Zeng 等 - 2017 - Multi-view Self-supervised Deep Learning for 6D Po.pdf:/home/red0orange/Zotero/storage/PLAKVS3Y/Zeng 等 - 2017 - Multi-view Self-supervised Deep Learning for 6D Po.pdf:application/pdf},
}

@article{zeng_tossingbot_2020,
	title = {{TossingBot}: {Learning} to {Throw} {Arbitrary} {Objects} {With} {Residual} {Physics}},
	volume = {36},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{TossingBot}},
	url = {https://ieeexplore.ieee.org/document/9104757/},
	doi = {10.1109/TRO.2020.2988642},
	abstract = {We investigate whether a robot arm can learn to pick and throw arbitrary rigid objects into selected boxes quickly and accurately. Throwing has the potential to increase the physical reachability and picking speed of a robot arm. However, precisely throwing arbitrary objects in unstructured settings presents many challenges: from acquiring objects in grasps suitable for reliable throwing, to handling varying object-centric properties (e.g., mass distribution, friction, shape) and complex aerodynamics. In this work, we propose an end-to-end formulation that jointly learns to infer control parameters for grasping and throwing motion primitives from visual observations (RGB-D images of arbitrary objects in a bin) through trial and error. Within this formulation, we investigate the synergies between grasping and throwing (i.e., learning grasps that enable more accurate throws) and between simulation and deep learning (i.e., using deep networks to predict residuals on top of control parameters predicted by a physics simulator). The resulting system, TossingBot, is able to grasp and successfully throw arbitrary objects into boxes located outside its maximum reach range at 500+ mean picks per hour (600+ grasps per hour with 85\% throwing accuracy); and generalizes to new objects and target locations.},
	language = {en},
	number = {4},
	urldate = {2022-11-25},
	journal = {IEEE Transactions on Robotics},
	author = {Zeng, Andy and Song, Shuran and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
	month = aug,
	year = {2020},
	pages = {1307--1319},
	file = {Zeng 等 - 2020 - TossingBot Learning to Throw Arbitrary Objects Wi.pdf:/home/red0orange/Zotero/storage/TNEH876R/Zeng 等 - 2020 - TossingBot Learning to Throw Arbitrary Objects Wi.pdf:application/pdf},
}

@article{han_scene_2022,
	title = {Scene {Reconstruction} with {Functional} {Objects} for {Robot} {Autonomy}},
	volume = {130},
	issn = {0920-5691, 1573-1405},
	url = {https://link.springer.com/10.1007/s11263-022-01670-0},
	doi = {10.1007/s11263-022-01670-0},
	abstract = {In this paper, we rethink the problem of scene reconstruction from an embodied agent’s perspective: While the classic view focuses on the reconstruction accuracy, our new perspective emphasizes the underlying functions and constraints of the reconstructed scenes that provide actionable information for simulating interactions with agents. Here, we address this challenging problem by reconstructing a functionally equivalent and interactive scene from RGB-D data streams, where the objects within are segmented by a dedicated 3D volumetric panoptic mapping module and subsequently replaced by partbased articulated CAD models to afford ﬁner-grained robot interactions. The object functionality and contextual relations are further organized by a graph-based scene representation that can be readily incorporated into robots’ action speciﬁcations and task deﬁnition, facilitating their long-term task and motion planning in the scenes. In the experiments, we demonstrate that (i) our panoptic mapping module outperforms previous state-of-the-art methods in recognizing and segmenting scene entities, (ii) the geometric and physical reasoning procedure matches, aligns, and replaces object meshes with best-ﬁtted CAD models, and (iii) the reconstructed functionally equivalent and interactive scenes are physically plausible and naturally afford actionable interactions; without any manual labeling, they are seamlessly imported to ROS-based robot simulators and VR environments for simulating complex robot interactions.},
	language = {en},
	number = {12},
	urldate = {2022-11-28},
	journal = {International Journal of Computer Vision},
	author = {Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin},
	month = dec,
	year = {2022},
	pages = {2940--2961},
	file = {Han 等 - 2022 - Scene Reconstruction with Functional Objects for R.pdf:/home/red0orange/Zotero/storage/QU75ZTLA/Han 等 - 2022 - Scene Reconstruction with Functional Objects for R.pdf:application/pdf},
}

@misc{xu_learning_2020,
	title = {Learning {3D} {Dynamic} {Scene} {Representations} for {Robot} {Manipulation}},
	url = {http://arxiv.org/abs/2011.01968},
	abstract = {3D scene representation for robot manipulation should capture three key object properties: permanency -- objects that become occluded over time continue to exist; amodal completeness -- objects have 3D occupancy, even if only partial observations are available; spatiotemporal continuity -- the movement of each object is continuous over space and time. In this paper, we introduce 3D Dynamic Scene Representation (DSR), a 3D volumetric scene representation that simultaneously discovers, tracks, reconstructs objects, and predicts their dynamics while capturing all three properties. We further propose DSR-Net, which learns to aggregate visual observations over multiple interactions to gradually build and refine DSR. Our model achieves state-of-the-art performance in modeling 3D scene dynamics with DSR on both simulated and real data. Combined with model predictive control, DSR-Net enables accurate planning in downstream robotic manipulation tasks such as planar pushing. Video is available at https://youtu.be/GQjYG3nQJ80.},
	language = {en},
	urldate = {2022-11-29},
	publisher = {arXiv},
	author = {Xu, Zhenjia and He, Zhanpeng and Wu, Jiajun and Song, Shuran},
	month = dec,
	year = {2020},
	note = {arXiv:2011.01968 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Xu 等 - 2020 - Learning 3D Dynamic Scene Representations for Robo.pdf:/home/red0orange/Zotero/storage/7EN8F22A/Xu 等 - 2020 - Learning 3D Dynamic Scene Representations for Robo.pdf:application/pdf},
}

@article{ha_deep_2022,
	title = {Deep {Visual} {Constraints}: {Neural} {Implicit} {Models} for {Manipulation} {Planning} {From} {Visual} {Input}},
	volume = {7},
	issn = {2377-3766, 2377-3774},
	shorttitle = {Deep {Visual} {Constraints}},
	url = {https://ieeexplore.ieee.org/document/9844753/},
	doi = {10.1109/LRA.2022.3194955},
	language = {en},
	number = {4},
	urldate = {2022-11-30},
	journal = {IEEE Robotics and Automation Letters},
	author = {Ha, Jung-Su and Driess, Danny and Toussaint, Marc},
	month = oct,
	year = {2022},
	pages = {10857--10864},
	file = {Ha 等 - 2022 - Deep Visual Constraints Neural Implicit Models fo.pdf:/home/red0orange/Zotero/storage/PT5C2ZBQ/Ha 等 - 2022 - Deep Visual Constraints Neural Implicit Models fo.pdf:application/pdf},
}

@inproceedings{zhu_hierarchical_2021-1,
	address = {Xi'an, China},
	title = {Hierarchical {Planning} for {Long}-{Horizon} {Manipulation} with {Geometric} and {Symbolic} {Scene} {Graphs}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561548/},
	doi = {10.1109/ICRA48506.2021.9561548},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Zhu, Yifeng and Tremblay, Jonathan and Birchfield, Stan and Zhu, Yuke},
	month = may,
	year = {2021},
	pages = {6541--6548},
	file = {Zhu 等 - 2021 - Hierarchical Planning for Long-Horizon Manipulatio.pdf:/home/red0orange/Zotero/storage/8J344QNU/Zhu 等 - 2021 - Hierarchical Planning for Long-Horizon Manipulatio.pdf:application/pdf},
}

@inproceedings{curtis_long-horizon_2022,
	address = {Philadelphia, PA, USA},
	title = {Long-{Horizon} {Manipulation} of {Unknown} {Objects} via {Task} and {Motion} {Planning} with {Estimated} {Affordances}},
	isbn = {978-1-72819-681-7},
	url = {https://ieeexplore.ieee.org/document/9812057/},
	doi = {10.1109/ICRA46639.2022.9812057},
	abstract = {We present a strategy for designing and building very general robot manipulation systems using a generalpurpose task-and-motion planner with both engineered and learned modules that estimate properties and affordances of unknown objects. Such systems are closed-loop policies that map from RGB images, depth images, and robot joint encoder measurements to robot joint position commands. We show that this strategy leads to intelligent behaviors even without a priori knowledge regarding the set of objects, their geometries, and their affordances. We show how these modules can be ﬂexibly composed with robot-centric primitives using the PDDLStream task and motion planning framework. Finally, we demonstrate that this strategy can enable a single policy to perform a wide variety of real-world multi-step manipulation tasks, generalizing over a broad class of objects, arrangements, and goals, without prior knowledge of the environment or re-training.},
	language = {en},
	urldate = {2022-11-30},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Curtis, Aidan and Fang, Xiaolin and Kaelbling, Leslie Pack and Lozano-Perez, Tomas and Garrett, Caelan Reed},
	month = may,
	year = {2022},
	pages = {1940--1946},
	file = {Curtis 等 - 2022 - Long-Horizon Manipulation of Unknown Objects via T.pdf:/home/red0orange/Zotero/storage/2GKPEJLH/Curtis 等 - 2022 - Long-Horizon Manipulation of Unknown Objects via T.pdf:application/pdf},
}

@misc{mitash_scene-level_2019,
	title = {Scene-level {Pose} {Estimation} for {Multiple} {Instances} of {Densely} {Packed} {Objects}},
	url = {http://arxiv.org/abs/1910.04953},
	abstract = {This paper introduces key machine learning operations that allow the realization of robust, joint 6D pose estimation of multiple instances of objects either densely packed or in unstructured piles from RGB-D data. The ﬁrst objective is to learn semantic and instance-boundary detectors without manual labeling. An adversarial training framework in conjunction with physics-based simulation is used to achieve detectors that behave similarly in synthetic and real data. Given the stochastic output of such detectors, candidates for object poses are sampled. The second objective is to automatically learn a single score for each pose candidate that represents its quality in terms of explaining the entire scene via a gradient boosted tree. The proposed method uses features derived from surface and boundary alignment between the observed scene and the object model placed at hypothesized poses. Scene-level, multi-instance pose estimation is then achieved by an integer linear programming process that selects hypotheses that maximize the sum of the learned individual scores, while respecting constraints, such as avoiding collisions. To evaluate this method, a dataset of densely packed objects with challenging setups for state-of-the-art approaches is collected. Experiments on this dataset and a public one show that the method signiﬁcantly outperforms alternatives in terms of 6D pose accuracy while trained only with synthetic datasets.},
	language = {en},
	urldate = {2022-11-30},
	publisher = {arXiv},
	author = {Mitash, Chaitanya and Wen, Bowen and Bekris, Kostas and Boularias, Abdeslam},
	month = oct,
	year = {2019},
	note = {arXiv:1910.04953 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Mitash 等 - 2019 - Scene-level Pose Estimation for Multiple Instances.pdf:/home/red0orange/Zotero/storage/RZNJVLA5/Mitash 等 - 2019 - Scene-level Pose Estimation for Multiple Instances.pdf:application/pdf},
}

@article{grinvald_volumetric_2019,
	title = {Volumetric {Instance}-{Aware} {Semantic} {Mapping} and {3D} {Object} {Discovery}},
	volume = {4},
	issn = {2377-3766, 2377-3774},
	url = {http://arxiv.org/abs/1903.00268},
	doi = {10.1109/LRA.2019.2923960},
	abstract = {To autonomously navigate and plan interactions in real-world environments, robots require the ability to robustly perceive and map complex, unstructured surrounding scenes. Besides building an internal representation of the observed scene geometry, the key insight toward a truly functional understanding of the environment is the usage of higher-level entities during mapping, such as individual object instances. This work presents an approach to incrementally build volumetric objectcentric maps during online scanning with a localized RGB-D camera. First, a per-frame segmentation scheme combines an unsupervised geometric approach with instance-aware semantic predictions to detect both recognized scene elements as well as previously unseen objects. Next, a data association step tracks the predicted instances across the different frames. Finally, a map integration strategy fuses information about their 3D shape, location, and, if available, semantic class into a global volume. Evaluation on a publicly available dataset shows that the proposed approach for building instance-level semantic maps is competitive with state-of-the-art methods, while additionally able to discover objects of unseen categories. The system is further evaluated within a real-world robotic mapping setup, for which qualitative results highlight the online nature of the method. Code is available at https://github.com/ethz-asl/voxblox-plusplus.},
	language = {en},
	number = {3},
	urldate = {2022-12-02},
	journal = {IEEE Robotics and Automation Letters},
	author = {Grinvald, Margarita and Furrer, Fadri and Novkovic, Tonci and Chung, Jen Jen and Cadena, Cesar and Siegwart, Roland and Nieto, Juan},
	month = jul,
	year = {2019},
	note = {arXiv:1903.00268 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	pages = {3037--3044},
	file = {Grinvald 等 - 2019 - Volumetric Instance-Aware Semantic Mapping and 3D .pdf:/home/red0orange/Zotero/storage/DB84BYQU/Grinvald 等 - 2019 - Volumetric Instance-Aware Semantic Mapping and 3D .pdf:application/pdf},
}

@inproceedings{oleynikova_voxblox_2017-1,
	title = {Voxblox: {Incremental} {3D} {Euclidean} {Signed} {Distance} {Fields} for {On}-{Board} {MAV} {Planning}},
	shorttitle = {Voxblox},
	url = {http://arxiv.org/abs/1611.03631},
	doi = {10.1109/IROS.2017.8202315},
	abstract = {Micro Aerial Vehicles (MAVs) that operate in unstructured, unexplored environments require fast and ﬂexible local planning, which can replan when new parts of the map are explored. Trajectory optimization methods fulﬁll these needs, but require obstacle distance information, which can be given by Euclidean Signed Distance Fields (ESDFs).},
	language = {en},
	urldate = {2022-12-02},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Oleynikova, Helen and Taylor, Zachary and Fehr, Marius and Nieto, Juan and Siegwart, Roland},
	month = sep,
	year = {2017},
	note = {arXiv:1611.03631 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {1366--1373},
	file = {Oleynikova 等 - 2017 - Voxblox Incremental 3D Euclidean Signed Distance .pdf:/home/red0orange/Zotero/storage/42PG3CFU/Oleynikova 等 - 2017 - Voxblox Incremental 3D Euclidean Signed Distance .pdf:application/pdf},
}

@inproceedings{liu_feddg_2021,
	address = {Nashville, TN, USA},
	title = {{FedDG}: {Federated} {Domain} {Generalization} on {Medical} {Image} {Segmentation} via {Episodic} {Learning} in {Continuous} {Frequency} {Space}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{FedDG}},
	url = {https://ieeexplore.ieee.org/document/9577482/},
	doi = {10.1109/CVPR46437.2021.00107},
	abstract = {Federated learning allows distributed medical institutions to collaboratively learn a shared prediction model with privacy protection. While at clinical deployment, the models trained in federated learning can still suffer from performance drop when applied to completely unseen hospitals outside the federation. In this paper, we point out and solve a novel problem setting of federated domain generalization (FedDG), which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. We present a novel approach, named as Episodic Learning in Continuous Frequency Space (ELCFS), for this problem by enabling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. Our approach transmits the distribution information across clients in a privacy-protecting way through an effective continuous frequency space interpolation mechanism. With the transferred multi-source distributions, we further carefully design a boundary-oriented episodic learning paradigm to expose the local learning to domain distribution shifts and particularly meet the challenges of model generalization in medical image segmentation scenario. The effectiveness of our method is demonstrated with superior performance over state-of-the-arts and in-depth ablation experiments on two medical image segmentation tasks. The code is available at https://github.com/liuquande/FedDG-ELCFS.},
	language = {en},
	urldate = {2022-12-03},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Quande and Chen, Cheng and Qin, Jing and Dou, Qi and Heng, Pheng-Ann},
	month = jun,
	year = {2021},
	pages = {1013--1023},
	file = {Liu 等 - 2021 - FedDG Federated Domain Generalization on Medical .pdf:/home/red0orange/Zotero/storage/KJ6TI42Z/Liu 等 - 2021 - FedDG Federated Domain Generalization on Medical .pdf:application/pdf},
}

@article{ichter_broadly-exploring_nodate,
	title = {Broadly-{Exploring}, {Local}-{Policy} {Trees} for {Long}-{Horizon} {Task} {Planning}},
	abstract = {Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion planning algorithms, such as rapidly-exploring random trees, are capable of efﬁciently exploring large state spaces and computing longhorizon, sequential plans. However, these algorithms are generally challenged with complex, stochastic, and high-dimensional state spaces as well as in the presence of small, topologically complex goal regions, which naturally emerge in tasks that interact with the environment. Machine learning offers a promising solution for its ability to learn general policies that can handle complex interactions and high-dimensional observations. However, these policies are generally limited in horizon length. Our approach, Broadly-Exploring, Local-policy Trees (BELT), merges these two approaches to leverage the strengths of both through a task-conditioned, model-based tree search. BELT uses an RRT-inspired tree search to efﬁciently explore the state space. Locally, the exploration is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. This task space can be quite general and abstract; its only requirements are to be sampleable and to well-cover the space of useful tasks. This search is aided by a task-conditioned model that temporally extends dynamics propagation to allow long-horizon search and sequential reasoning over tasks. BELT is demonstrated experimentally to be able to plan long-horizon, sequential trajectories with a goal conditioned policy and generate plans that are robust.},
	language = {en},
	author = {Ichter, Brian and Sermanet, Pierre and Lynch, Corey},
	pages = {11},
	file = {Ichter 等 - Broadly-Exploring, Local-Policy Trees for Long-Hor.pdf:/home/red0orange/Zotero/storage/HP8LESUV/Ichter 等 - Broadly-Exploring, Local-Policy Trees for Long-Hor.pdf:application/pdf},
}

@misc{simeonov_se3-equivariant_2022,
	title = {{SE}(3)-{Equivariant} {Relational} {Rearrangement} with {Neural} {Descriptor} {Fields}},
	url = {http://arxiv.org/abs/2211.09786},
	abstract = {We present a method for performing tasks involving spatial relations between novel object instances initialized in arbitrary poses directly from point cloud observations. Our framework provides a scalable way for specifying new tasks using only 5-10 demonstrations. Object rearrangement is formalized as the question of finding actions that configure task-relevant parts of the object in a desired alignment. This formalism is implemented in three steps: assigning a consistent local coordinate frame to the task-relevant object parts, determining the location and orientation of this coordinate frame on unseen object instances, and executing an action that brings these frames into the desired alignment. We overcome the key technical challenge of determining task-relevant local coordinate frames from a few demonstrations by developing an optimization method based on Neural Descriptor Fields (NDFs) and a single annotated 3D keypoint. An energy-based learning scheme to model the joint configuration of the objects that satisfies a desired relational task further improves performance. The method is tested on three multi-object rearrangement tasks in simulation and on a real robot. Project website, videos, and code: https://anthonysimeonov.github.io/r-ndf/},
	language = {en},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Simeonov, Anthony and Du, Yilun and Yen-Chen, Lin and Rodriguez, Alberto and Kaelbling, Leslie Pack and Lozano-Perez, Tomas and Agrawal, Pulkit},
	month = nov,
	year = {2022},
	note = {arXiv:2211.09786 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Simeonov 等 - 2022 - SE(3)-Equivariant Relational Rearrangement with Ne.pdf:/home/red0orange/Zotero/storage/ARHW2VWH/Simeonov 等 - 2022 - SE(3)-Equivariant Relational Rearrangement with Ne.pdf:application/pdf},
}

@inproceedings{chen_keyframe_2022,
	title = {Keyframe {Selection} with {Information} {Occupancy} {Grid} {Model} for {Long}-term {Data} {Association}},
	abstract = {As the basic of visual tracking and mapping in VSLAM systems, keyframe selection plays an essential role. In previous works, keyframes are selected according to a series of view change-based judgement that aim to provide the referenced tracking markers and enhance the current visual tracking. However, the enrichment of environment information is always ignored, which results to the lacking of complete environment representation, and the failure of closed-loop detection or submap alignment. In this paper, we propose a keyframe selection judgement based on the deep image global descriptor and information enrichment. The image is expressed by an illumination invariant descriptor for an appropriate statical abstraction; an information enrichment judgement is proposed based on the deep descriptor, by which the environment representation can be completed. The experimental results are conducted showing the advantage of our method in terms of image representation and information enrichment. The proposed information-based keyframe selection judgement can raise the tracking precision and the loop-closed detection rate.},
	booktitle = {2022 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Chen, Weinan and Ye, Hanjing and Zhu, Lei and Tang, Chao and Fu, Changfei and Chen, Yonggang and Zhang, Hong},
	year = {2022},
}

@article{chen_robustness_2021-1,
	title = {Robustness {Improvement} of {Using} {Pre}-{Trained} {Network} in {Visual} {Odometry} for {On}-{Road} {Driving}},
	volume = {70},
	issn = {0018-9545, 1939-9359},
	url = {https://ieeexplore.ieee.org/document/9573476/},
	doi = {10.1109/TVT.2021.3120214},
	abstract = {Robustness in on-road driving Visual Odometry (VO) systems is critical, as it determines the reliable performance in various scenarios and environments. Especially with the development of data-driven technology, the combination of data-driven VO and model-based VO has achieved accurate tracking performance. However, the lack of generalization of pre-trained deep neural networks (DNN) limits the robustness of such a combination in unseen environments. In this study, we introduce a novel framework with appropriate usage of DNN prediction and improve the robustness in the self-driving application. Based on the characteristic of on-road self-driving motion and the DNN output, we propose a two-step optimization strategy with a variable degree of freedom (DoF), i.e., the use of two types of DoF representations during pose estimation. Speciﬁcally, our two-step optimization operates according to the residual of the optimization with the motion label classiﬁcation from the pre-trained DNN, as well as our proposed Motion Evaluation by essential matrix construction. Experimental results show that our framework obtains better tracking accuracy than the existing methods.},
	language = {en},
	number = {12},
	urldate = {2022-12-08},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Chen, Weinan and Zhu, Lei and Loo, Shing Yan and Wang, Jiankun and Wang, Chaoqun and Meng, Max Q.-H. and Zhang, Hong},
	month = dec,
	year = {2021},
	pages = {12415--12426},
	file = {Chen 等 - 2021 - Robustness Improvement of Using Pre-Trained Networ.pdf:/home/red0orange/Zotero/storage/4RS9CTWH/Chen 等 - 2021 - Robustness Improvement of Using Pre-Trained Networ.pdf:application/pdf},
}

@article{chen_dynamic_2022-1,
	title = {Dynamic {Strategy} of {Keyframe} {Selection} {With} {PD} {Controller} for {VSLAM} {Systems}},
	volume = {27},
	issn = {1083-4435, 1941-014X},
	url = {https://ieeexplore.ieee.org/document/9352537/},
	doi = {10.1109/TMECH.2021.3058617},
	abstract = {Keyframe (KF) selection in a KF-based visual simultaneous localization and mapping (VSLAM) system is critical. In previous studies, static thresholds have been used for KF selection decision making; however, suboptimal performance can result from the use of such thresholds. To obtain a better KF setting than that obtained with the existing methods, in this article, we introduce a dynamic KF selection strategy. By considering both the view change between camera observation and KFs in the built map and the rate of this change, we propose to dynamically adjust the threshold for KF selection. A proportion and derivative (PD) controller is designed with the feedback of estimated view change, where the PD controller output is used for KF selection. According to the experimental results, compared with the existing studies, our method can improve the precision of visual tracking by 17.5\% and 16.7\% based on two popular VSLAM systems.},
	language = {en},
	number = {1},
	urldate = {2022-12-08},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Chen, Weinan and Zhu, Lei and Lin, Xubin and He, Li and Guan, Yisheng and Zhang, Hong},
	month = feb,
	year = {2022},
	pages = {115--125},
	file = {Chen 等 - 2022 - Dynamic Strategy of Keyframe Selection With PD Con.pdf:/home/red0orange/Zotero/storage/MS73WLCR/Chen 等 - 2022 - Dynamic Strategy of Keyframe Selection With PD Con.pdf:application/pdf},
}

@article{chen_ceb-map_2021-1,
	title = {{CEB}-{Map}: {Visual} {Localization} {Error} {Prediction} for {Safe} {Navigation}},
	volume = {21},
	issn = {1530-437X, 1558-1748, 2379-9153},
	shorttitle = {{CEB}-{Map}},
	url = {https://ieeexplore.ieee.org/document/9109263/},
	doi = {10.1109/JSEN.2020.2999641},
	abstract = {For safe visual navigation, areas with high localization errors should be concentrated and could be further reﬁned by additional mapping operations. Given an environment map, we propose to predict the visual localization error and hence to either improve the navigation performance or call an additional mapping to reﬁne the built map. Previous work adopts the uncertainty of landmarks for the error prediction. In our work, we take into account both the spatial distribution of visual landmarks and the uncertainty of landmarks. Our main idea is that standing at one place, a good spatial distribution of landmarks means a sufﬁcient enough visible landmarks from all views at that place, i.e., enough landmarks under arbitrary view-direction. Combining the spatial distribution and the uncertainty of landmarks, we propose a new framework to predict the error of visual localization. Furthermore, we show that additional mapping in the area with high predicted error can signiﬁcantly improve the visual localization precision. Experimental results show that there is a strong relationship between the predicted error and the real error, of which the absolute value of correlation coefﬁcient is between 0.707 to 0.915. We apply our method to conduct an optimal reﬁning policy on the built map and the experimental results show the improved localization precision. Applications on navigation test verify the superiority of our proposed method.},
	language = {en},
	number = {10},
	urldate = {2022-12-08},
	journal = {IEEE Sensors Journal},
	author = {Chen, Weinan and Zhu, Lei and Wang, Chaoqun and He, Li and Meng, Max Q.-H.},
	month = may,
	year = {2021},
	pages = {11769--11780},
	file = {Chen 等 - 2021 - CEB-Map Visual Localization Error Prediction for .pdf:/home/red0orange/Zotero/storage/999EDSQI/Chen 等 - 2021 - CEB-Map Visual Localization Error Prediction for .pdf:application/pdf},
}

@inproceedings{adeleye_putting_2022,
	address = {Mexico City, Mexico},
	title = {Putting away the {Groceries} with {Precise} {Semantic} {Placements}},
	isbn = {978-1-66549-042-9},
	url = {https://ieeexplore.ieee.org/document/9926691/},
	doi = {10.1109/CASE49997.2022.9926691},
	abstract = {We present a methodology for a service robot to put away the groceries in a precise and stable configuration that satisfies the desired semantic relationship of grocery and pantry objects. This task is invaluable to the motor impaired and increases the capability of contextualized service robots. We show that augmenting traditional geometric assessment with contextual knowledge of food categorization and a language association model, allows the robot to complete this task efficiently. We quantitatively validate our approach with a data set of 51 common grocery items, of which 11 objects are used for real world experiments. According to our evaluation, our method is able to successfully shelve objects next to semantically related objects 100\% of the time when these relationships exist. We achieve this with an average placement precision of 3.2cm and a standard deviation of 1.1cm. We discuss remaining challenges and needed improvements for robots with these capabilities to be introduced to home environments.},
	language = {en},
	urldate = {2022-12-09},
	booktitle = {2022 {IEEE} 18th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	publisher = {IEEE},
	author = {Adeleye, Akanimoh and Hu, Jiaming and Christensen, Henrik I.},
	month = aug,
	year = {2022},
	pages = {2219--2224},
	file = {Adeleye 等 - 2022 - Putting away the Groceries with Precise Semantic P.pdf:/home/red0orange/Zotero/storage/ERD9XXLK/Adeleye 等 - 2022 - Putting away the Groceries with Precise Semantic P.pdf:application/pdf},
}

@misc{peng_openscene_2022,
	title = {{OpenScene}: {3D} {Scene} {Understanding} with {Open} {Vocabularies}},
	shorttitle = {{OpenScene}},
	url = {http://arxiv.org/abs/2211.15654},
	abstract = {Traditional 3D scene understanding approaches rely on labeled 3D datasets to train a model for a single task with supervision. We propose OpenScene, an alternative approach where a model predicts dense features for 3D scene points that are co-embedded with text and image pixels in CLIP feature space. This zero-shot approach enables taskagnostic training and open-vocabulary queries. For example, to perform SOTA zero-shot 3D semantic segmentation it ﬁrst infers CLIP features for every 3D point and later classiﬁes them based on similarities to embeddings of arbitrary class labels. More interestingly, it enables a suite of open-vocabulary scene understanding applications that have never been done before. For example, it allows a user to enter an arbitrary text query and then see a heat map indicating which parts of a scene match. Our approach is effective at identifying objects, materials, affordances, activities, and room types in complex 3D scenes, all using a single model trained without any labeled 3D data.},
	language = {en},
	urldate = {2022-12-11},
	publisher = {arXiv},
	author = {Peng, Songyou and Genova, Kyle and Jiang, Chiyu "Max" and Tagliasacchi, Andrea and Pollefeys, Marc and Funkhouser, Thomas},
	month = nov,
	year = {2022},
	note = {arXiv:2211.15654 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Peng 等 - 2022 - OpenScene 3D Scene Understanding with Open Vocabu.pdf:/home/red0orange/Zotero/storage/6FJAUMGS/Peng 等 - 2022 - OpenScene 3D Scene Understanding with Open Vocabu.pdf:application/pdf},
}

@misc{ha_semantic_2022,
	title = {Semantic {Abstraction}: {Open}-{World} {3D} {Scene} {Understanding} from {2D} {Vision}-{Language} {Models}},
	shorttitle = {Semantic {Abstraction}},
	url = {http://arxiv.org/abs/2207.11514},
	abstract = {We study open-world 3D scene understanding, a family of tasks that require agents to reason about their 3D environment with an open-set vocabulary and out-of-domain visual inputs – a critical skill for robots to operate in the unstructured 3D world. Towards this end, we propose Semantic Abstraction (SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D spatial capabilities, while maintaining their zero-shot robustness. We achieve this abstraction using relevancy maps extracted from CLIP, and learn 3D spatial and geometric reasoning skills on top of those abstractions in a semantic-agnostic manner. We demonstrate the usefulness of SemAbs on two open-world 3D scene understanding tasks: 1) completing partially observed objects and 2) localizing hidden objects from language descriptions. SemAbs can generalize to novel vocabulary for object attributes and nouns, materials/lighting, classes, and domains (i.e., real-world scans) from training on limited 3D synthetic data.},
	language = {en},
	urldate = {2022-12-11},
	publisher = {arXiv},
	author = {Ha, Huy and Song, Shuran},
	month = dec,
	year = {2022},
	note = {arXiv:2207.11514 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, I.2.10},
	file = {Ha 和 Song - 2022 - Semantic Abstraction Open-World 3D Scene Understa.pdf:/home/red0orange/Zotero/storage/3XY3XSYR/Ha 和 Song - 2022 - Semantic Abstraction Open-World 3D Scene Understa.pdf:application/pdf},
}

@misc{shafiullah_clip-fields_2022,
	title = {{CLIP}-{Fields}: {Weakly} {Supervised} {Semantic} {Fields} for {Robotic} {Memory}},
	shorttitle = {{CLIP}-{Fields}},
	url = {http://arxiv.org/abs/2210.05663},
	abstract = {We propose CLIP-Fields, an implicit scene model that can be trained with no direct human supervision. This model learns a mapping from spatial locations to semantic embedding vectors. The mapping can then be used for a variety of tasks, such as segmentation, instance identification, semantic search over space, and view localization. Most importantly, the mapping can be trained with supervision coming only from web-image and web-text trained models such as CLIP, Detic, and Sentence-BERT. When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or semantic segmentation on the HM3D dataset with only a fraction of the examples. Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in real-world environments. Our code and demonstrations are available here: https://mahis.life/clip-fields},
	language = {en},
	urldate = {2022-12-11},
	publisher = {arXiv},
	author = {Shafiullah, Nur Muhammad Mahi and Paxton, Chris and Pinto, Lerrel and Chintala, Soumith and Szlam, Arthur},
	month = nov,
	year = {2022},
	note = {arXiv:2210.05663 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	file = {Shafiullah 等 - 2022 - CLIP-Fields Weakly Supervised Semantic Fields for.pdf:/home/red0orange/Zotero/storage/U6NXQUF5/Shafiullah 等 - 2022 - CLIP-Fields Weakly Supervised Semantic Fields for.pdf:application/pdf},
}

@misc{zeng_transporter_2022-1,
	title = {Transporter {Networks}: {Rearranging} the {Visual} {World} for {Robotic} {Manipulation}},
	shorttitle = {Transporter {Networks}},
	url = {http://arxiv.org/abs/2010.14406},
	abstract = {Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io},
	language = {en},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Wahid, Ayzaan and Sindhwani, Vikas and Lee, Johnny},
	month = jan,
	year = {2022},
	note = {arXiv:2010.14406 [cs]},
	keywords = {Computer Science - Robotics},
	file = {Zeng 等 - 2022 - Transporter Networks Rearranging the Visual World.pdf:/home/red0orange/Zotero/storage/KTWGU6FU/Zeng 等 - 2022 - Transporter Networks Rearranging the Visual World.pdf:application/pdf},
}

@misc{zeng_learning_2018,
	title = {Learning {Synergies} between {Pushing} and {Grasping} with {Self}-supervised {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1803.09956},
	abstract = {Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors amid challenging cases of clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after only a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu},
	language = {en},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
	month = sep,
	year = {2018},
	note = {arXiv:1803.09956 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zeng 等 - 2018 - Learning Synergies between Pushing and Grasping wi.pdf:/home/red0orange/Zotero/storage/RVKJ822B/Zeng 等 - 2018 - Learning Synergies between Pushing and Grasping wi.pdf:application/pdf},
}

@article{lin_robust_2022,
	title = {Robust {Data} {Association} against {Detection} {Deficiency} for {Semantic} {SLAM}},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Lin, Xubin and Ruan, Jiahao and Yang, Yirui and He, Li and Guan, Yisheng and Zhang, Hong},
	year = {2022},
}

@article{an_deep_2022,
	title = {Deep {Tri}-{Training} for {Semi}-{Supervised} {Image} {Segmentation}},
	volume = {7},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {An, Shan and Zhu, Haogang and Zhang, Jiaao and Ye, Junjie and Wang, Siliang and Yin, Jianqin and Zhang, Hong},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {10097--10104},
}

@article{yang_robust_2022,
	title = {Robust {UWB} {Indoor} {Localization} for {NLOS} {Scenes} via {Learning} {Spatial}-{Temporal} {Features}},
	volume = {22},
	number = {8},
	journal = {IEEE Sensors Journal},
	author = {Yang, Bo and Li, Jun and Shao, Zhanpeng and Zhang, Hong},
	year = {2022},
	note = {Publisher: IEEE},
	pages = {7990--8000},
}

@article{wang_real-time_2020,
	title = {Real-time 3-{D} semantic scene parsing with {LiDAR} sensors},
	journal = {IEEE Transactions on Cybernetics},
	author = {Wang, Fei and Zhuang, Yan and Zhang, Hong and Gu, Hong},
	year = {2020},
	note = {Publisher: IEEE},
}

@article{shao_learning_2020,
	title = {Learning representations from skeletal self-similarities for cross-view action recognition},
	volume = {31},
	number = {1},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Shao, Zhanpeng and Li, Youfu and Zhang, Hong},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {160--174},
}

@article{fu_fastorb-slam_nodate,
	title = {{FastORB}-{SLAM}: {A} fast {ORB}-{SLAM} method with {Coarse}-to-{Fine} descriptor independent keypoint matching. {arXiv} 2020},
	journal = {arXiv preprint arXiv:2008.09870},
	author = {Fu, Q and Yu, H and Wang, X and Yang, Z and Zhang, H and Mian, A},
}

@article{yang_resilient_2021,
	title = {Resilient indoor localization system based on {UWB} and visual–inertial sensors for complex environments},
	volume = {70},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Yang, Bo and Li, Jun and Zhang, Hong},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {1--14},
}

@article{shakeri_polarimetric_2021,
	title = {Polarimetric monocular dense mapping using relative deep depth prior},
	volume = {6},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Shakeri, Moein and Loo, Shing Yang and Zhang, Hong and Hu, Kangkang},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {4512--4519},
}

@article{wen_dense_2021,
	title = {Dense point cloud map construction based on stereo {VINS} for mobile vehicles},
	volume = {178},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Wen, Shuhuan and Liu, Xin and Zhang, Hong and Sun, Fuchun and Sheng, Miao and Fan, Shaokang},
	year = {2021},
	note = {Publisher: Elsevier},
	pages = {328--344},
}

@article{wen_semantic_2021,
	title = {Semantic visual {SLAM} in dynamic environment},
	volume = {45},
	number = {4},
	journal = {Autonomous Robots},
	author = {Wen, Shuhuan and Li, Pengjiang and Zhao, Yongjie and Zhang, Hong and Sun, Fuchun and Wang, Zhe},
	year = {2021},
	note = {Publisher: Springer},
	pages = {493--504},
}

@article{chen_dynamic_2021,
	title = {Dynamic {Strategy} of {Keyframe} {Selection} {With} {PD} {Controller} for {VSLAM} {Systems}},
	volume = {27},
	number = {1},
	journal = {IEEE/ASME Transactions on Mechatronics},
	author = {Chen, Weinan and Zhu, Lei and Lin, Xubin and He, Li and Guan, Yisheng and Zhang, Hong},
	year = {2021},
	note = {Publisher: IEEE},
	pages = {115--125},
}

@inproceedings{chen_perspective_2022,
	title = {Perspective {Phase} {Angle} {Model} for {Polarimetric} {3D} {Reconstruction}},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Chen, Guangcheng and He, Li and Guan, Yisheng and Zhang, Hong},
	year = {2022},
	pages = {398--414},
}

@inproceedings{elkerdawy_fire_2022,
	title = {Fire {Together} {Wire} {Together}: {A} {Dynamic} {Pruning} {Approach} with {Self}-{Supervised} {Mask} {Prediction}},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Elkerdawy, Sara and Elhoushi, Mostafa and Zhang, Hong and Ray, Nilanjan},
	year = {2022},
	pages = {12454--12463},
}

@inproceedings{loo_deeprelativefusion_2021,
	title = {Deeprelativefusion: {Dense} monocular slam using single-image relative depth prediction},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Loo, Shing Yan and Mashohor, Syamsiah and Tang, Sai Hong and Zhang, Hong},
	year = {2021},
	pages = {6641--6648},
}

@inproceedings{yang_uvip_2021,
	title = {{UVIP}: {Robust} {UWB} aided visual-inertial positioning system for complex indoor environments},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Yang, Bo and Li, Jun and Zhang, Hong},
	year = {2021},
	pages = {5454--5460},
}

@inproceedings{lin_robust_2021-1,
	title = {Robust {Improvement} in {3D} {Object} {Landmark} {Inference} for {Semantic} {Mapping}},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lin, Xubin and Yang, Yirui and He, Li and Chen, Weinan and Guan, Yisheng and Zhang, Hong},
	year = {2021},
	pages = {13011--13017},
}

@inproceedings{ting_deep_2021,
	title = {Deep snapshot {HDR} reconstruction based on the polarization camera},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {Ting, Juiwen and Wu, Xuesong and Hu, Kangkang and Zhang, Hong},
	year = {2021},
	pages = {1769--1773},
}

@article{zhou_ndd_2022,
	title = {{NDD}: {A} {3D} {Point} {Cloud} {Descriptor} {Based} on {Normal} {Distribution} for {Loop} {Closure} {Detection}},
	journal = {arXiv preprint arXiv:2209.12513},
	author = {Zhou, Ruihao and He, Li and Zhang, Hong and Lin, Xubin and Guan, Yisheng},
	year = {2022},
}
