@article{hanSceneReconstructionFunctional2022,
  title = {Scene {{Reconstruction}} with {{Functional Objects}} for {{Robot Autonomy}}},
  author = {Han, Muzhi and Zhang, Zeyu and Jiao, Ziyuan and Xie, Xu and Zhu, Yixin and Zhu, Song-Chun and Liu, Hangxin},
  date = {2022-12},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {130},
  number = {12},
  pages = {2940--2961},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-022-01670-0},
  url = {https://link.springer.com/10.1007/s11263-022-01670-0},
  urldate = {2022-11-28},
  abstract = {In this paper, we rethink the problem of scene reconstruction from an embodied agent’s perspective: While the classic view focuses on the reconstruction accuracy, our new perspective emphasizes the underlying functions and constraints of the reconstructed scenes that provide actionable information for simulating interactions with agents. Here, we address this challenging problem by reconstructing a functionally equivalent and interactive scene from RGB-D data streams, where the objects within are segmented by a dedicated 3D volumetric panoptic mapping module and subsequently replaced by partbased articulated CAD models to afford finer-grained robot interactions. The object functionality and contextual relations are further organized by a graph-based scene representation that can be readily incorporated into robots’ action specifications and task definition, facilitating their long-term task and motion planning in the scenes. In the experiments, we demonstrate that (i) our panoptic mapping module outperforms previous state-of-the-art methods in recognizing and segmenting scene entities, (ii) the geometric and physical reasoning procedure matches, aligns, and replaces object meshes with best-fitted CAD models, and (iii) the reconstructed functionally equivalent and interactive scenes are physically plausible and naturally afford actionable interactions; without any manual labeling, they are seamlessly imported to ROS-based robot simulators and VR environments for simulating complex robot interactions.},
  langid = {english},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\QU75ZTLA\\Han 等 - 2022 - Scene Reconstruction with Functional Objects for R.pdf}
}

@misc{kapelyukhDALLEBotIntroducingWebScale2022,
  title = {{{DALL-E-Bot}}: {{Introducing Web-Scale Diffusion Models}} to {{Robotics}}},
  shorttitle = {{{DALL-E-Bot}}},
  author = {Kapelyukh, Ivan and Vosylius, Vitalis and Johns, Edward},
  date = {2022-10-05},
  number = {arXiv:2210.02438},
  eprint = {2210.02438},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.02438},
  urldate = {2022-10-11},
  abstract = {We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those objects, then generating an image representing a natural, human-like arrangement of those objects, and finally physically arranging the objects according to that image. The significance is that we achieve this zero-shot using DALLE, without needing any further data collection or training. Encouraging real-world results with human studies show that this is an exciting direction for the future of web-scale robot learning algorithms. We also propose a list of recommendations to the text-to-image community, to align further developments of these models with applications to robotics. Videos are available at: https://www.robot-learning.uk/dall-e-bot.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\55K4A7W5\\Kapelyukh 等。 - 2022 - DALL-E-Bot Introducing Web-Scale Diffusion Models.pdf}
}

@misc{shridharPerceiverActorMultiTaskTransformer2022,
  title = {Perceiver-{{Actor}}: {{A Multi-Task Transformer}} for {{Robotic Manipulation}}},
  shorttitle = {Perceiver-{{Actor}}},
  author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  date = {2022-11-11},
  number = {arXiv:2209.05451},
  eprint = {2209.05451},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.05451},
  urldate = {2022-11-25},
  abstract = {Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive. Can manipulation still benefit from Transformers with the right problem formulation? We investigate this question with PERACT, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. PERACT encodes language goals and RGB-D voxel observations with a Perceiver Transformer [1], and outputs discretized actions by “detecting the next best voxel action”. Unlike frameworks that operate on 2D images, the voxelized 3D observation and action space provides a strong structural prior for efficiently learning 6-DoF actions. With this formulation, we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task. Our results show that PERACT significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\red0orange\\Documents\\Zotero\\storage\\ULDYEW8C\\Shridhar 等 - 2022 - Perceiver-Actor A Multi-Task Transformer for Robo.pdf}
}
