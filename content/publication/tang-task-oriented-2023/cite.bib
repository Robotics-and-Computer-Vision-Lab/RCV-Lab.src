@misc{tang_task-oriented_2023,
 abstract = {To perform household tasks, assistive robots receive commands in the form of user language instructions for tool manipulation. The initial stage involves selecting the intended tool (i.e., object grounding) and grasping it in a task-oriented manner (i.e., task grounding). Nevertheless, prior researches on visual-language grasping (VLG) focus on object grounding, while disregarding the ﬁne-grained impact of tasks on object grasping. Task-incompatible grasping of a tool will inevitably limit the success of subsequent manipulation steps. Motivated by this problem, this paper proposes GraspCLIP, which addresses the challenge of task grounding in addition to object grounding to enable task-oriented grasp prediction with visuallanguage inputs. Evaluation on a custom dataset demonstrates that GraspCLIP achieves superior performance over established baselines with object grounding only. The effectiveness of the proposed method is further validated on an assistive robotic arm platform for grasping previously unseen kitchen tools given the task speciﬁcation. Our presentation video is available at: https://www.youtube.com/watch?v=e1wfYQPeAXU.},
 author = {Tang, Chao and Huang, Dehao and Meng, Lingxiao and Liu, Weiyu and Zhang, Hong},
 file = {Tang et al. - 2023 - Task-Oriented Grasp Prediction with Visual-Languag.pdf:/home/wlong/Zotero/storage/5WKWLEUG/Tang et al. - 2023 - Task-Oriented Grasp Prediction with Visual-Languag.pdf:application/pdf},
 keywords = {Computer Science - Robotics},
 language = {en},
 month = {February},
 note = {arXiv:2302.14355 [cs]},
 publisher = {arXiv},
 title = {Task-Oriented Grasp Prediction with Visual-Language Inputs},
 url = {http://arxiv.org/abs/2302.14355},
 urldate = {2023-11-23},
 year = {2023}
}

